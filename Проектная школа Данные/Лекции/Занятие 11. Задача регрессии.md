[[L11_Machine_Learning_Regression.pdf]]
## Конспект занятия: Методы машинного обучения. Задача регрессии

Занятие посвящено **задаче регрессии** — одной из фундаментальных задач **машинного обучения с учителем**.

### 1. Задача регрессии и ее характеристики

**Задача регрессии** — это предсказание **непрерывных числовых значений** (в отличие от классификации, которая предсказывает дискретные метки классов).

**Интуитивное понимание:** Если классификация отвечает на вопрос "к какой категории относится объект?" (например, дорогой или дешевый?), то регрессия отвечает на вопрос **"какое точное числовое значение"** (например, сколько стоит?). Для обучения модели необходимы правильные метки в датасете (обучение с учителем).

**Ключевые характеристики:**

1. **Целевая переменная:** Непрерывная числовая величина, которую необходимо предсказать (например, цена, температура, доход).
2. **Входные признаки:** Набор переменных (числовых или категориальных), описывающих объект (например, площадь квартиры, количество комнат).
3. **Функция предсказания:** Модель, которая обучается находить зависимость $f(x) = y$, минимизируя ошибку между предсказанными и реальными значениями.

**Типичные примеры:** Прогнозирование цен на недвижимость, стоимости акций, потребления энергии, температуры воздуха.

### 2. Алгоритмы регрессии

#### 2.1. Линейная регрессия (Linear Regression)

Линейная регрессия — базовый, интерпретируемый метод, предполагающий **линейную зависимость** между признаками и целевой переменной.

- **Формула:** Представляет собой линейную комбинацию признаков и их весов: $y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ$.
- **Обучение:** Модель находит оптимальные веса (коэффициенты $w$). Оптимальные веса находятся путем минимизации **суммы квадратов отклонений (MSE)**, используя **метод наименьших квадратов**. Решение может быть найдено аналитически (через матричные операции) или итеративно (через градиентный спуск).
- **Интерпретируемость:** Коэффициенты $w$ легко интерпретировать, так как они показывают вклад каждого признака в прогноз; положительный вес означает положительное влияние (рост признака увеличивает $y$), отрицательный — отрицательное.

**Интуитивное понимание:** Линейная регрессия **просто ищет прямую линию** (или гиперплоскость в многомерном пространстве), которая проходит максимально близко ко всем точкам данных.

**Ограничения:** Предполагает линейную зависимость, **чувствительна к выбросам** из-за квадратичной функции потерь (MSE), не улавливает сложные нелинейные паттерны.

#### 2.2. Регуляризованные версии линейной регрессии

Эти методы добавляют штрафной член к функции потерь для контроля сложности модели и предотвращения переобучения.

|Метод|Тип регуляризации|Штраф|Особенность|
|:--|:--|:--|:--|
|**Ridge**|L2|Пропорционален квадрату весов ($\alpha \cdot \sum w²$)|Уменьшает веса, но не обнуляет их, сохраняя все признаки.|
|**Lasso**|L1|Пропорционален модулю весов ($\alpha \cdot \sum|w|
|**ElasticNet**|Комбинация L1 и L2|Смесь обоих штрафов|Объединяет преимущества обоих методов, баланс контролируется параметром $\rho$.|

#### 2.3. Деревья решений (Decision Trees)

Дерево решений строит иерархическую структуру, разбивая пространство признаков.

- **Принцип работы:** Начинается с корневого узла. В каждом узле алгоритм выбирает признак и пороговое значение (трешхолд), которые наилучшим образом разделяют данные, максимально снижая суммарную дисперсию в дочерних узлах. Процесс повторяется рекурсивно до критериев остановки.
- **Преимущества:** Не требует нормализации признаков, автоматически учитывает нелинейности, легко визуализируется и интерпретируется.

**Интуитивное понимание:** Модель принимает решения последовательно: "Если площадь больше 100 кв. м, и если комнат меньше 3, то цена такая-то". Предсказание строится на **ступенчатых отрезках** (нелинейная функция), а не на одной прямой линии.

#### 2.4. XGBoost (Extreme Gradient Boosting)

XGBoost — один из наиболее эффективных современных алгоритмов, основанный на технике **градиентного бустинга**.

- **Принцип работы:** Последовательно строит ансамбль слабых моделей (обычно неглубоких деревьев). Каждое новое дерево обучается **предсказывать остаточные ошибки** (градиенты функции потерь) всех предыдущих деревьев, постепенно улучшая качество прогноза.

**Интуитивное понимание:** Вместо того чтобы обучать много независимых деревьев, которые голосуют за финальный ответ, XGBoost обучает их **последовательно**, где каждое следующее дерево пытается исправить ошибки, допущенные всей предыдущей "командой".

### 3. Гиперпараметры (Hyperparameters)

**Гиперпараметры** — это параметры, которые **задаются до начала обучения** и не обновляются в процессе оптимизации. Они контролируют структуру модели, процесс обучения и механизмы регуляризации.

**Интуитивное понимание:** Если веса ($w$) — это то, что модель учит сама, то гиперпараметры — это **ручные настройки** (управленческие решения), которые мы задаем, чтобы контролировать, насколько сложной, быстрой или консервативной будет модель.

**Типы гиперпараметров:**

1. **Структурные:** Определяют архитектуру модели (например, глубина дерева, количество слоев нейросети).
2. **Регуляризационные:** Контролируют сложность, предотвращают переобучение (например, коэффициенты L1/L2 регуляризации).
3. **Оптимизационные:** Управляют процессом обучения (например, **learning rate** — шаг, с которым модель идет к оптимуму, размер батча).

**Ключевые гиперпараметры для деревьев/бустинга:**

- `max_depth`: Ограничивает максимальное количество уровней в дереве. Малые значения предотвращают переобучение. (Типично: 3-10).
- `n_estimators`: Количество деревьев в ансамбле (для XGBoost). Чем больше, тем, как правило, точнее, но медленнее.
- `learning_rate`: Скорость, с которой модель корректирует свои ошибки в бустинге. Малый learning rate требует больше деревьев, но улучшает обобщающую способность.

### 4. Функции потерь (Loss Functions)

**Функция потерь (Loss function)** — это центральный элемент обучения, определяющий **математическую меру ошибки**, которую алгоритм оптимизации стремится минимизировать.

**Интуитивное понимание:** Это внутренний механизм модели, **"шкала боли"**. Чем больше ошибка (разница между предсказанным и реальным), тем выше "боль", и модель корректирует свои параметры, чтобы уменьшить эту "боль".

|Функция потерь|Формула|Особенности|
|:--|:--|:--|
|**MSE** (Mean Squared Error)|$L = (1/n) \cdot \sum(y - \hat{y})²$|**Сильно штрафует большие ошибки** из-за квадрата. Чувствительна к выбросам.|
|**MAE** (Mean Absolute Error)|$L = (1/n) \cdot \sum|y - \hat{y}|

### 5. Валидация и метрики

#### 5.1. Валидация (Validation)

**Валидация** — это оценка обобщающей способности модели на данных, которые она не видела. Это критически важно для предотвращения **переобучения** (overfitting).

**Переобучение** возникает, когда модель отлично работает на обучающей выборке, но плохо предсказывает на новых данных, потому что она запомнила шум, а не общие закономерности.

**Методы валидации:**

1. **Train/Test Split:** Простейший метод. Данные делятся на обучающую (Train) и тестовую (Test) выборки. **Важно:** Тестовая выборка должна быть **неприкосновенной** до финальной оценки, чтобы избежать подглядывания и неосознанного переобучения модели через инженера (Data Leakage).
2. **K-Fold Cross-Validation (Кросс-валидация):** Данные делятся на $K$ частей (обычно 5–10). Модель обучается $K$ раз, при этом каждый сегмент данных по очереди используется как валидационный (тестовый) набор.
    - _Интуитивное понимание (K-Fold):_ Это дает **более надежную и устойчивую оценку** качества модели, усредняя результаты по нескольким тестам. Смотря на среднее значение и стандартное отклонение метрик, можно оценить стабильность работы модели.

#### 5.2. Методы поиска гиперпараметров

- **Grid Search (Поиск по сетке):** Перебирает **все возможные комбинации** из заданного диапазона значений. Гарантирует нахождение оптимума в пределах сетки, но очень вычислительно затратен.
- **Random Search (Случайный поиск):** Случайно сэмплирует комбинации из заданных распределений. Часто более эффективен, чем Grid Search, при ограниченном бюджете вычислений.

#### 5.3. Метрики регрессии (Regression Metrics)

Метрики используются для количественной оценки качества прогнозов.

**Интуитивное понимание:** В случае ошибок (MAE, MSE, RMSE), **чем меньше значение метрики, тем лучше**.

|Метрика|Описание|Применение|
|:--|:--|:--|
|**MAE**|Средняя абсолютная ошибка.|Легко интерпретируется в единицах целевой переменной. Робастна к выбросам.|
|**MSE**|Средняя квадратичная ошибка.|Сильно штрафует большие ошибки.|
|**RMSE**|Корень из средней квадратичной ошибки.|**Возвращает метрику в исходные единицы измерения** (например, рубли, если предсказывалась цена), что делает ее более интерпретируемой, чем MSE.|
|**R²** (Коэффициент детерминации)|Доля дисперсии, объясненная моделью (обычно от 0 до 1).|Легко интерпретируется как процент объясненной вариации.|
|**MAPE**|Средняя абсолютная процентная ошибка.|Полезно для бизнес-анализа, так как показывает ошибку в процентах.|

---

**Аналогия для интуитивного понимания всего процесса:**

Представьте, что вы учите лучника стрелять по мишени (ваши данные).

1. **Алгоритм (Линейная регрессия):** Это сам лук. Простой,10 сантиметров (потому что ошибка возводится в квадрат).
2. **Валидация (Кросс-валидация):** Это 5 разных мишеней, расположенных на разной высоте. Вы даете лучнику стрелять в каждую мишень по очереди и усредняете его результаты. Это гарантирует, что он не просто случайно попал в одну мишень, но стабильно хорошо стреляет в любых условиях.
3. **Метрика (RMSE):** Это финальный отчет, который показывает, насколько в среднем лучник промахивается в сантиметрах (в исходных единицах измерения).