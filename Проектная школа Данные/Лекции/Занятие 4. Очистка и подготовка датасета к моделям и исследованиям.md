Данный конспект составлен на основе предоставленных материалов лекции, посвященной сегментации, подготовке данных (нормализации, стандартизации, кодированию категориальных признаков) и инженерии признаков для моделирования.

---

## Подробный конспект лекции

### 1. Сегментация и группировка данных

#### 1.1. Необходимость сегментации

**Сегментация** — это деление выборки на группы со схожими характеристиками. Это необходимо, поскольку аномалии, наблюдаемые в общей выборке, могут быть связаны с тем, что выборка является **"спутанной"** (состоящей из разных частей).

- **Пример из практики:** При создании модели оценки рейтинга доверия исполнителю, было обнаружено, что очень активные пользователи с огромной историей смешивались с новичками, только что пришедшими на платформу. Заливать их в одну модель неправильно, так как **фичи будут очень спутанными**; например, фича верификации пользователя для "старичков" пройдена по умолчанию, а для новичков — нет.
- **Решение:** Прийти к делению, создав **сегментированную мультимодель**.

#### 1.2. Виды сегментации

Существуют различные виды сегментации:

1. **Категориальная/Демографическая:** На основе таких параметров, как регион, категория товара, пол.
2. **Поведенческая:** Основана на том, как пользователь ведет себя (например, новичок, старичок, активный, оттекающий). Это супер-инструмент для маркетинговых моделей, например, для **предсказания оттока**.
3. **Временная:** Учитывает день недели, сезон (высокий/низкий сезон).
4. **Бизнес-сегменты:** Зависят от специфики бизнеса, например, сегменты по **прайс-группам** (дорогой/дешевый товар).
5. **Алгоритмическая (Кластеризация):** Используется, когда невозможно заранее определить признаки классификации (неизвестно, как отделить одно от другого).

#### 1.3. Алгоритмическая сегментация (Кластеризация)

Алгоритмическая сегментация позволяет делить пользователей, не зная заранее параметров.

- **K-Means (Каминс):** Самый популярный способ. Он определяет оптимальный набор кластеров (сегментов).
    
    - **Логика работы:** Модель ищет центры (центроиды) и притягивает к ним наблюдения.
    - **Результат:** Каждый кластер представляет собой группу объектов со схожими характеристиками.
    - **Требование:** Необходимо выбрать метрику для деления, например, число покупок или среднее число покупок на пользователя. K-Means может помочь найти сегмент, для которого можно построить отдельную модель.
- **RFM (Recency, Frequency, Monetary):** Фреймворк, близкий к поведенческой сегментации. Основан на трех осях:
    
    1. **Recency (R):** Давность последнего действия/покупки.
    2. **Frequency (F):** Частота, сколько всего действий/покупок было.
    3. **Monetary (M):** Сумма, которую человек потратил (денежный фактор).
    
    - **Важность Monetary:** Пользователь, совершающий много действий, но с очень низким чеком, может быть убыточным, так как расходы на ресурсы (доставка, обслуживание) могут превышать прибыль.

#### 1.4. Поиск аномальных сегментов

Аномальные сегменты могут включать, например, фродеров.

- **Методы:**
    - Сравнение метрик по сегментам с глобальными средними, поиск наибольшего разрыва.
    - Расчет **Z-score** для средних по сегментам.
    - Визуализация различий через **Boxplot**.

### 2. Визуализация сегментов и инсайты

**Визуализация** позволяет понять, как сегменты отличаются.

- **Инструменты визуализации:**
    - **Barplot, Boxplot:** Позволяют сравнить распределения и посмотреть выбросы в разных сегментах.
    - **Heatmap (Kitmap):** Позволяет найти тесноту корреляции между разными признаками в разных сегментах.
    - **Subplot (Соплот):** Позволяет классно посмотреть распределения разных сегментов на одном графике.
- **Поиск инсайтов:** Сегментация полезна для поиска инсайтов путем сравнения средних метрик (доход, кликабельность, retention).
- **Ценность анализа:** Само по себе наблюдение не имеет значения для бизнеса; ценность представляют **конкретные гипотезы или предложения**.

### 3. Нормализация и стандартизация признаков

Эти преобразования необходимы, чтобы привести признаки к **единому масштабу**.

#### 3.1. Зачем нужно масштабирование

Масштабирование необходимо для:

1. **Предотвращения перетягивания влияния признаков:** Если признаки имеют очень разный масштаб (например, Цена от 0 до 1 000 000, и Рейтинг от 1 до 5), модель (особенно линейная регрессия) может ошибочно решить, что признак с большим числовым диапазоном (Цена) гораздо важнее, что приводит к искажению.
2. **Корректной работы ML-моделей:** Это критически важно для линейных моделей.
3. **Более понятного сравнения признаков** между собой.
4. **Улучшения качества модели:** Если модель плохо предсказывает (низкие метрики качества, например, `accuracy`), часто нужно вернуться к данным и поработать с параметрами.

#### 3.2. Нормализация (Normalization)

**Определение:** Преобразование значений признаков в единый диапазон, например, от 0 до 1.

- **Методика:** **Min-Max Scaling**.
    - Формула: $\text{X}_{\text{норм}} = (\text{X} - \text{X}_{\text{min}}) / (\text{X}_{\text{max}} - \text{X}_{\text{min}})$.
- **Свойства:** Нормализация **сохраняет форму распределения**, но сжимает размах.
- **Применение:** Очень сильно помогает улучшать алгоритмы типа K-NN и K-Means.

#### 3.3. Стандартизация (Standardization)

**Определение:** Приведение к **нулевому среднему** и **единичному стандартному отклонению**.

- **Методика:** **Standard Scaler** (связан с расчетом Z-score).
    - **Логика:** Центрирование данных относительно среднего и приведение дисперсии к единице.
- **Свойства:** Среднее значение будет 0, а стандартное отклонение — 1. Значения, которые больше 2 или меньше -2, считаются **выбросами**.
- **Применение:** Классный инструмент для логистической регрессии.

#### 3.4. Другие методы масштабирования

К другим методам относятся:

- **Лог-трансформация:** Логарифмирование распределения.
- **Max Abs Scaler:** Шкалирование по максимальному абсолютному значению.
- **Robust Scaler:** Использует медиану и интерквартильный размах, что повышает устойчивость к выбросам.

#### 3.5. Этапы масштабирования

Полный цикл масштабирования может включать:

1. Приведение всех признаков к единому масштабу (диапазон 0-1).
2. Центрирование данных (устранение влияния масштаба).
3. Уменьшение влияния выбросов и сглаживание длинного хвоста (если есть).
4. Проверка качества модели.

### 4. Работа с категориальными признаками и кодирование

#### 4.1. Проблема кодирования

Машина обучения работает на **числах**, а не на тексте. Необходимо перевести категориальные переменные (например, пол, город, тип устройства) в числовой формат, который модель сможет "переварить".

#### 4.2. Виды категориальных признаков

- **Номинативные:** Не имеют логики или порядка (например, тип устройства, город).
- **Порядковые (Ordinal):** Имеют порядок (например, младший, средний, высокий).
- **Бинарные:** Имеют только два варианта (0 или 1, да или нет, мужской/женский).

#### 4.3. Методы кодирования

1. **Label Encoding (Кодирование метками):**
    
    - **Принцип:** Просто заменяет категории цифрами (0, 1, 2, 3...).
    - **Риск:** Создает **ложный порядок** для номинативных данных.
    - **Подходит:** Для бустингов/деревьев.
2. **One-Hot Encoding (OHE):**
    
    - **Принцип:** Столбец с категориями расщепляется на _N_ столбцов (где _N_ — количество вариантов). Для соответствующей категории ставится 1, для остальных 0.
    - **Пример:** Устройство "мобильный" расщепляется на три столбца (`device_mobile`, `device_desktop`, `device_tablet`).
    - **Проблема:** При большом количестве категорий (например, 89 регионов) создается огромный массив столбцов.
3. **Ordinal Encoding (Порядковое кодирование):**
    
    - **Принцип:** **Вручную** задает смысловой порядок категорий (например, низкий=1, средний=2, высокий=3).
    - **Важность:** Используется, когда порядок имеет значение (например, уровень дохода), и модель должна понимать, что "высокий" уровень вносит больший вклад.
4. **Target Encoding (Кодирование целевой метрикой):**
    
    - **Принцип:** Заменяет категорию средним значением целевой метрики (например, средняя конверсия) для этой категории.
    - **Пример:** Замена значения города на среднюю цену объекта недвижимости в этом городе.
5. **Frequency Encoding (Частотное кодирование):**
    
    - **Принцип:** Замена категории ее частотностью (насколько часто она встречается в выборке).

#### 4.4. Лучшие практики кодирования

- **Избегать утечки статистики:** Кодировать категориальные признаки только на обучающей (тренировочной) выборке, а не на всем датасете до его разделения.
- **Аккуратность с количеством категорий:** Слишком большое количество категорий может привести к "бреду".

### 5. Feature Engineering (Инженерия признаков) и подготовка к моделированию

#### 5.1. Суть Feature Engineering

Feature Engineering — это процесс создания **информативных фичей**, которые улучшают качество модели. Это один из ключевых элементов подготовки данных.

#### 5.2. Примеры создания новых признаков

- **Преобразование цены:** Вместо абсолютной цены использовать логнормированную цену или масштабировать ее (Min-Max) для устранения перекоса распределения.
- **Работа с датой:** Вместо полной даты до минуты использовать значение месяца (актуально для сезонного бизнеса).
- **Конверсии:** Вместо двух абсолютных фичей (просмотры и добавление в корзину) использовать одну **фичу конверсии** ((добавление в корзину) / (просмотры)). Это уменьшает шум и является более понятным для бизнеса.
- **Накопленные значения:** Вместо всего накопленного периода (истории) использовать **недавние накопленные значения** (например, просмотры за последние 30, 60, 90 дней), так как их вес может быть разным в зависимости от цикла сделки.

#### 5.3. Чек-лист подготовки данных

Процесс подготовки данных, включающий Feature Engineering, выглядит так:

1. **Очистка данных:** Удаление дубликатов, выбросов, заполнение пропусков.
2. **Кодирование категориальных признаков:** (см. раздел 4).
3. **Нормализация/Стандартизация числовых признаков:** (см. раздел 3).
4. **Генерация признаков (Feature Generation):** Создание новых фичей (конверсии, агрегации по времени).
5. **Отбор признаков (Feature Selection):** Выбор тех фичей, которые действительно влияют на модель.

#### 5.4. Методы отбора признаков

- **Корреляция:** Исключение признаков с высокой корреляцией (например, более 0.95), так как они могут негативно влиять на модель.
- **Feature Importance:** Использование моделей (например, бустингов) для оценки значимости каждого признака.

**Предостережение:** Нельзя просто загружать слишком много фичей в модель, надеясь на лучшее качество; это может привести к сильному искажению.

### 6. Оформление результатов и лучшие практики

#### 6.1. Оформление аналитики (Jupyter Notebook)

Важно обеспечить простоту и чистоту оформления, чтобы любой человек мог прочитать анализ.

- **Структура:** Использование заголовков (Markdown `##` и т.д.), четкие описания, сохранение процесса.
- **Комментарии:** Добавление комментариев к коду и выводам.
- **Визуализация:** Графики должны быть четкими и подписанными (заголовки, оси).
- **Мосты между сущностями:** Необходимо уметь переводить сложные математические результаты на простой язык, понятный бизнесу.

#### 6.2. Визуальные предостережения

- **Шкалирование:** Нельзя искусственно создавать "космическую" разницу, обрезая ось Y. Всегда нужно указывать, что изображено на графике (подписи, легенды).

#### 6.3. Ценность гипотез

Анализ должен заканчиваться **выводами, смыслами и сводным списком потенциальных инсайтов**. Ценность имеют конкретные гипотезы (например, "изменение кнопки заказа повысило метрику конверсии на 10%").

#### 6.4. Z-Score (дополнительно)

**Z-Score** (Z-критерий) — один из способов оценки значимости какого-либо изменения между выборками, как аналог t-критерия. Используется для расчета значимости изменения одного среднего относительно другого.