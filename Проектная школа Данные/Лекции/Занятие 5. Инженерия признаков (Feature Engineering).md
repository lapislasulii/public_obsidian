[[Lecture 5 - Feature Engineering Pt.1 Final.pdf]]

**Подробный конспект занятия по анализу данных: Инженерия признаков (Feature Engineering)**

Занятие, проведенное Ксенией Балабеевой, старшим специалистом по машинному обучению в BIOCAD, было посвящено введению в инженерию признаков, ее значению в машинном обучении и практическому применению на примере медицинских и жилищных данных.

---

### I. Введение и контекст

1. **Спикер и Фон:** Лекцию проводила Ксения Балабеева, старший специалист по машинному обучению в BIOCAD и преподаватель в ИТМО и НИУ ВШЭ.
2. **ML в Биотехнологии:** Компания BIOCAD является ведущей биотехнологической компанией в России, занимающейся полным циклом разработки препаратов для лечения онкологических, аутоиммунных и наследственных заболеваний.
3. **Цель ML:** Команда машинного обучения (ML) стремится **повысить эффективность и производительность процессов** биотехнологической компании на всех этапах создания лекарства, ускоряя выход препаратов на рынок.

---

### II. Инженерия признаков (Feature Engineering)

**Инженерия признаков (Feature Engineering)** — это процесс использования знаний в предметной области (доменных знаний) для создания новых признаков на основе существующих данных, которые повышают эффективность алгоритмов машинного обучения.

> **Интуитивное понимание:** Представьте, что вы даете модели ингредиенты для торта. Инженерия признаков — это не просто взять муку и яйца, а **превратить их в готовое тесто или крем**, сделав данные более "вкусными" и легко усваиваемыми для модели, чтобы она могла дать лучший результат.

Инженерия признаков часто считается одним из **самых важных и самых творческих этапов** в процессе машинного обучения.

**Цели этапа:**

1. Представить сырые данные в лучшем виде для решения конкретной задачи.
2. Повысить качество работы модели.

---

### III. Работа с существующими и создание новых признаков

#### А. Типы признаков в машинном обучении

1. **Числовые признаки**: Представлены числами, которые можно измерить и сравнить количественно.
    - _Непрерывные:_ Могут принимать любое значение в диапазоне (например, BMI, Возраст: 45.5 лет).
    - _Дискретные:_ Принимают целочисленные значения (например, Количество госпитализаций: 3).
2. **Категориальные признаки**: Представлены категориями или метками.
    - _Номинальные:_ Категории без естественного порядка (например, Группа крови, Пол).
    - _Порядковые (Ординальные):_ Категории, имеющие естественный порядок (например, Стадия заболевания I, II, III).
3. **Бинарные признаки:** Имеют два возможных значения (Да/Нет, 0/1).
4. **Временные признаки:** Связаны со временем и могут быть разложены на компоненты (День недели, Час дня).

#### Б. Создание новых признаков (Feature Generation)

1. **Взаимодействие признаков (Interaction Features)**: Комбинирование показателей для создания составных индикаторов.
    
    - _Пример:_ Расчет **Индекса массы тела (BMI)** из веса и роста ($BMI = \text{weight} / \text{height}^2$).
    - _Интуитивное понимание:_ Отдельно рост и вес могут мало о чем сказать, но **их соотношение (BMI) сразу дает ценную информацию** об уровне ожирения, что полезно для прогноза.
    - _Другой пример:_ **Метаболический индекс** путем перемножения глюкозы и кровяного давления.
2. **Полиномиальные признаки (Polynomial Features)**: Создание признаков, возведенных в степень (квадратичные, кубические), для отображения нелинейных взаимосвязей.
    
    - _Пример:_ $age^2$ или $age^3$.
    - _Интуитивное понимание:_ Этот прием **добавляет нелинейности в линейную модель**. Если риск инфаркта растет не равномерно, а резко ускоряется после 50 лет, $age^2$ поможет модели уловить этот резкий "изгиб" зависимости.
3. **Булевы признаки (Boolean Features)**: Создание бинарных индикаторов на основе **пороговых значений (трешхолдов)** из доменной области.
    
    - _Пример:_ is_obese = (BMI > 30) $\to 1$.
    - _Интуитивное понимание:_ Мы преобразуем точное числовое значение в критическую категорию "**наличие/отсутствие**" заболевания или фактора риска, основанную на клинических нормах.
4. **Биннинг (Группировка)**: Группирование непрерывных показателей в значимые категории.
    
    - _Пример:_ Возраст: 18-30 $\to$ 'молодой', 51+ $\to$ 'пожилой'.
    - _Интуитивное понимание:_ Группировка может быть полезна, когда модели сложно уловить разницу между, например, 51 годом и 52 годами, но она должна четко понимать разницу между категорией "**молодой**" и "**пожилой**".

---

### IV. Преобразование признаков

#### А. Кодирование категориальных признаков

Поскольку модели машинного обучения (особенно линейные) работают с числами, категориальные переменные необходимо преобразовать.

1. **Бинарное кодирование (Binary Encoding):** Применяется для признаков, имеющих только два уникальных значения (например, Пол $\to$ 0 или 1).
2. **Прямое кодирование (One-Hot Encoding):** Для категорий с несколькими равнозначными вариантами (например, Диагноз). Создается отдельный бинарный столбец для каждой уникальной категории (0 или 1).
    - _Риск:_ Если уникальных значений очень много, это приводит к сильному разрастанию количества признаков (Проклятие размерности).
3. **Целевое кодирование (Target Encoding):** Каждая категория заменяется **средним значением целевой переменной** для этой категории.
    - _Интуитивное понимание:_ Например, регион "Москва" кодируется как "средний риск инфаркта в Москве".
    - _Риск:_ Может привести к **переобучению** и **утечке данных** (Data Leakage). Для безопасного использования требуется сглаживание и кросс-валидация.

#### Б. Трансформация признаков (Feature Transformation)

Трансформация помогает нормализовать асимметричные распределения данных (например, когда данные смещены вправо, как BMI или уровни биомаркеров), улучшая производительность линейных моделей и снижая влияние выбросов.

1. **Логарифмическое преобразование (Log):** Эффективно для **сильно асимметричных данных** (например, уровни биомаркеров, цены, доходы).
2. **Квадратный корень:** Подходит для **умеренно асимметричных данных**.
3. **Box-Cox / Yeo-Johnson:** Автоматически подбирают оптимальное преобразование для нормализации.

**Преобразование целевой переменной:** Этот прием важен в регрессионных задачах, особенно для линейных моделей, когда целевая переменная сильно скошена.

- **Важно:** После прогнозирования на преобразованной шкале необходимо выполнить **обратное преобразование** (например, экспоненту для логарифма), чтобы получить предсказание в исходной шкале.

#### В. Масштабирование признаков (Feature Scaling)

Масштабирование приводит признаки к общему диапазону, поскольку они часто имеют разные единицы измерения и диапазоны (например, возраст 0-100 и глюкоза 70-200). Это критично для алгоритмов, основанных на расчете расстояний.

> **Интуитивное понимание:** Если не масштабировать, признак с большими числовыми значениями (например, давление 200 мм рт.ст.) будет иметь непропорционально больший "вес" в расчете расстояний, чем признак с меньшими значениями (например, BMI 30), даже если BMI важен для прогноза.

1. **Стандартизация (Z-score):** Преобразует признаки так, чтобы они имели среднее значение **0** и стандартное отклонение **1**.
    - _Применяется для:_ Логистической регрессии, SVM, нейронных сетей.
2. **Min-Max нормализация:** Масштабирует признаки в фиксированный диапазон, обычно ****.
    - _Применяется для:_ Нейронных сетей, алгоритмов на основе расстояний (KNN).

---

### V. Отбор признаков и лучшие практики

#### А. Проклятие размерности (Curse of Dimensionality)

**Проклятие размерности** — это явление, при котором экспоненциальное увеличение количества признаков делает пространство данных разреженным, а расстояния между точками бессмысленными, что затрудняет обучение и приводит к переобучению.

#### Б. Отбор признаков (Feature Selection)

Важно отбирать только **качественные, информативные признаки**.

1. **Отбор на основе корреляции:** Вычисление коэффициентов корреляции между признаками и целевой переменной. Признаки с высокой корреляцией более прогностичны.
    - _Помогает:_ Удалить избыточные признаки и снизить мультиколлинеарность.
    - _Ограничение:_ Корреляция показывает только **линейную связь**.
2. **Важность признаков (Feature Importance):** Некоторые модели ML (Decision Trees, Random Forest, Linear Regression) могут оценивать, какие признаки внесли наибольший вклад в прогноз.

#### В. Лучшие практики

1. **Избегайте утечки данных (Data Leakage)**:
    
    - **Утечка данных** происходит, когда информация о целевой переменной (или тестового набора) неправомерно используется в процессе обучения.
    - _Правило:_ Всегда обучайте преобразования (масштабирование, кодирование) только на **тренировочном наборе**, а затем применяйте их к тестовому.
2. **Доменные знания критичны:** Наиболее эффективные признаки создаются на основе понимания предметной области. Сотрудничество с экспертами необходимо, особенно в таких областях, как медицина.
    
3. **Итеративный процесс:** Конструирование признаков требует экспериментирования, оценки и уточнения. Лучше добавлять признаки по одному, отслеживая их влияние.
    
4. **Мониторинг мультиколлинеарности:** Это высокая корреляция **между самими признаками** (не между признаком и целевой переменной). Мультиколлинеарность делает модели нестабильными. Необходимо сравнивать новый признак с исходным и оставлять только тот, который дает наибольший прирост качества.
    
5. **Баланс сложности и интерпретируемости:** В медицине и других критически важных областях интерпретируемость (возможность объяснить, почему модель дала такой прогноз) часто критична для доверия.
    

---

### VI. Практическая часть: Предсказание стоимости жилья

Практическая часть занятия была посвящена ознакомлению с процессом **Разведочного анализа данных (EDA)** на основе датасета конкурса Kaggle по прогнозированию цен домов в Айове.

1. **Инструменты:** Использовались библиотеки Pandas (для работы с таблицами), NumPy (для вычислений), Matplotlib и Seaborn (для визуализации).
2. **Целевая переменная:** Стоимость жилья (Price).
3. **Беглый анализ:**
    - Использование метода `describe()` для получения описательных статистик (среднее, минимум, максимум, квантили) по **числовым** признакам.
    - Проверка пропусков: Использование `is_null().sum()` показало, что пропуски в данном датасете отсутствуют (все значения равны 0).
4. **Автоматизированный EDA:** Была использована библиотека `ydata-profiling` (ранее `pandas-profiling`) для генерации быстрого интерактивного отчета о данных в одну строку кода.
    - Отчет позволяет быстро увидеть распределения, базовые статистики, а также **Alerts** (сигналы) о проблемах в данных, таких как высокая корреляция или несбалансированность.
    - Например, было обнаружено, что распределение целевой переменной (Цены) является **несимметричным** и имеет длинный "хвост" (скошенность), что указывает на необходимость **логарифмического преобразования** для линейных моделей.