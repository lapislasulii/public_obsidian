
[[L12_Text_Classification.pdf]]
Данный конспект основан на материалах занятия, посвященного **Классификации текстовых данных**. Занятие охватывает переход от работы с табличными данными к **текстовым данным** (новой модальности) и рассматривает полный цикл решения задачи классификации текстов, от предобработки до использования современных моделей-трансформеров.

---

## 1. Постановка задачи и основные понятия

### 1.1. Классификация и ее метрики

**Классификация** (Classification) — это задача машинного обучения, цель которой — построить модель $f: X \rightarrow Y$ для предсказания категориальной переменной $y_i$, где $y_i$ может принимать ограниченное количество категорий (классов).

**Классификация текстов** — это фундаментальная задача обработки естественного языка (NLP), заключающаяся в автоматическом присвоении текстовым документам предопределенных категорий или меток.

|Задача|Примеры|
|:--|:--|
|**Анализ тональности**|Определение позитивных/негативных мнений клиентов.|
|**Категоризация новостей**|Распределение статей по темам (спорт, политика).|
|**Обнаружение спама**|Фильтрация нежелательных сообщений.|
|**Классификация обращений**|Сортировка запросов в службу поддержки.|

**Метрики оценки качества** Метрики обычно вычисляются с помощью **Confusion Matrix** (матрица несоответствий или матрица ошибок).

| Метрика                                 | Формула                                                                                 | Интуитивное объяснение                                                                                                                                                                                 |
| :-------------------------------------- | :-------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Accuracy (Доля верных предсказаний)** | $(TP + TN) / (TP + TN + FP + FN)$                                                       | Доля правильных предсказаний среди всех объектов. _Насколько часто модель угадывает правильно в целом?_.                                                                                               |
| **F1-Score**                            | $2 \times (\text{Precision} \times \text{Recall}) / (\text{Precision} + \text{Recall})$ | Гармоническое среднее между точностью (Precision) и полнотой (Recall). _Насколько хорошо модель находит баланс между тем, чтобы не пропустить важные случаи и не делать слишком много лишних ошибок?_. |

---

## 2. Text Data Preprocessing (Предобработка текстовых данных)

Предобработка текста является критически важным этапом, поскольку текст является **неструктурированным данным**, в котором может быть много артефактов и "мусора". Исследования показывают, что до 80% времени в ML-проектах тратится на подготовку данных.

| Этап                         | Интуитивное объяснение                                                                                                                                                                                                                                | Особенности для русского языка                       |
| :--------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------- |
| **1. Очистка текста**        | Удаление всего "мусора", который не несёт смысловой нагрузки: HTML-тегов, гиперссылок, специальных символов, лишних пробелов, пунктуации (хотя пунктуация иногда может нести информацию, например, много восклицательных знаков в негативном отзыве). |                                                      |
| **2. Нормализация регистра** | Приведение всех слов к единому регистру, обычно к нижнему (например, "Бежали" $\rightarrow$ "бежали"). _Для компьютера "Кот" и "кот" — разные слова; нормализация делает их одинаковыми_.                                                             |                                                      |
| **3. Токенизация**           | Разбиение текста на отдельные слова или **токены** (более мелкие единицы, чем слова, например, подстроки). _Разбиваем текст на "кирпичики", которые будем кодировать_.                                                                                |                                                      |
| **4. Лемматизация**          | Приведение слов к их словарной (нормальной) форме (например, "бежали" $\rightarrow$ "бежать"). _Учитывая богатую морфологию русского языка, это важно, чтобы модель понимала, что все формы слова (бегу, бежать, бежал) — это одна и та же идея_.     | Важна из-за богатой морфологии и флективности языка. |
| **5. Удаление стоп-слов**    | Исключение высокочастотных слов ("и", "в", "на", "с"), которые не несут большой смысловой нагрузки. _Убираем "шум", чтобы он не мешал статистическим методам векторизации_.                                                                           |                                                      |

---

## 3. EDA (Исследовательский анализ данных) для текстовых данных

**Исследовательский анализ данных (EDA)** для текстов позволяет понять структуру датасета и выявить потенциальные проблемы, например, дисбаланс классов.

| Метрика EDA                       | Интуитивное объяснение                                                                                                                                                                         |
| :-------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Распределение длин текстов** | Анализ количества слов/символов в документах. _Помогает понять, какой длины тексты (например, спам может быть коротким, а развернутые отзывы — длинными). Длина может стать важным признаком_. |
| **2. Частотный анализ**           | Топ-20 наиболее частых слов по классам. _Позволяет выявить ключевые слова для каждой категории_.                                                                                               |
| **3. Баланс классов**             | Соотношение количества примеров в каждом классе. _Проверяем, нет ли сильного перекоса (дисбаланса), который может ухудшить обучение модели_.                                                   |
| **4. Облако слов**                | Визуализация наиболее частых терминов. _Быстрый способ увидеть, о чем чаще всего говорится в тексте_.                                                                                          |
| **5. Уникальность словаря**       | Размер словаря (список всех уникальных слов в корпусе) и покрытие редких слов.                                                                                                                 |

---

## 4. Кодирование текстов (Векторизация)

Чтобы компьютер мог обработать текст, его необходимо перевести в **числовой формат** (векторы).

### 4.1. Bag of Words (Мешок Слов)

**Bag of Words (BoW)** — классический базовый подход, который представляет тексты в виде векторов, фокусируясь только на частоте встречаемости каждого слова в документе.

- **Концепция:** Метод игнорирует порядок слов и грамматику.
- **Этапы:** Токенизация $\rightarrow$ Создание словаря (список уникальных слов) $\rightarrow$ Векторизация (подсчет частоты слов в каждом документе).
- **Интуитивное объяснение:** _Мы просто кладем все слова из текста в "мешок" и считаем, сколько раз каждое слово встретилось. Предложения "кот ловит мышь" и "мышь ловит кот" будут иметь одинаковое векторное представление_.
- **Недостаток:** Создает **разреженные** векторы (большинство элементов равны нулю) очень большой размерности, равной размеру словаря.

### 4.2. TF-IDF

**TF-IDF (Term Frequency-Inverse Document Frequency)** — статистическая мера, оценивающая важность слова в документе. Является значительным улучшением по сравнению с BoW.

- **Формула:** TF-IDF = TF(t, d) $\times$ IDF(t).
    - **TF (Term Frequency):** Частота слова $t$ в документе $d$.
    - **IDF (Inverse Document Frequency):** Мера редкости термина во всем корпусе.
- **Интуитивное объяснение:** _Слово получает наивысший вес, если оно **часто встречается** в конкретном документе (высокий TF), но **редко встречается** во всем корпусе (высокий IDF). Это позволяет выделить уникальные ключевые термины и автоматически снизить вес общих слов (стоп-слов)_.
- **Применение:** Использование векторов TF-IDF для классификации и информационного поиска.

### 4.3. N-граммы

**N-граммы** — это последовательности из $N$ соседних слов или символов.

- **Цель:** Сохраняют локальный контекст и улавливают устойчивые выражения, что является ключевым улучшением по сравнению с BoW.
- **Типы:** Униграммы (N=1, отдельные слова), Биграммы (N=2, пары слов), Триграммы (N=3, тройки слов).
- **Пример:** Для фразы "машинное обучение текстов":
    - Униграммы: ['машинное', 'обучение', 'текстов'].
    - Биграммы: ['машинное обучение', 'обучение текстов'].
- **Недостаток:** При росте N экспоненциально растет размерность словаря и разреженность данных.

---

## 5. Эмбеддинги и измерение сходства

### 5.1. Эмбеддинги (Embeddings)

**Эмбеддинги** — это ключевая технология современного NLP, которая преобразует текст в числовые векторы фиксированной размерности, сохраняя **семантическую близость** слов.

- **Концепция:** Похожие по смыслу слова имеют близкие векторные представления.
- **Интуитивное объяснение:** _Каждое слово получает свой числовой "отпечаток" в пространстве. Если слова "король" и "королева" близки по смыслу, их векторы будут находиться рядом. Более того, с этими векторами можно выполнять арифметические операции: вектор("король") - вектор("мужчина") + вектор("женщина") $\approx$ вектор("королева")_.
- **Типы:**
    - **Word Embeddings (Word2Vec, GloVe):** Статические векторы для каждого слова, **не учитывают контекст** предложения.
    - **Contextual Embeddings (BERT, ELMo):** **Динамические векторы**, зависящие от контекста. Одно и то же слово может иметь разные векторы в разных предложениях.

### 5.2. Косинусное сходство (Cosine Similarity)

**Косинусное расстояние** — одна из самых популярных метрик для измерения семантической близости текстов в векторном пространстве.

- **Принцип:** Учитывает только **направление** векторов, игнорируя их длину, что идеально подходит для сравнения текстов разной длины.
- **Интуитивное объяснение:** _Мы смотрим на угол между векторами двух текстов. Чем меньше угол, то есть чем больше векторы "смотрят" в одном направлении, тем выше сходство по смыслу_.

---

## 6. Современные модели классификации текстов

### 6.1. Механизм Attention (Внимание)

Архитектура **Transformer**, представленная в 2017 году, произвела революцию в NLP благодаря механизму **Attention**.

- **Self-Attention (Механизм самовнимания):** Ключевой механизм, позволяющий модели определить, какие слова в предложении наиболее важны для понимания каждого конкретного слова.
- **Интуитивное объяснение:** _Это способность модели динамически "фокусироваться" на нужных частях предложения. Например, в предложении "Банк реки был крутым" механизм Self-attention определяет, что слово "банк" связано со словом "реки" и означает "берег", а не финансовое учреждение_.
- **Преимущества Трансформеров:**
    1. **Параллельная обработка:** Обрабатывают текст одновременно, а не последовательно (как RNN), что ускоряет обучение.
    2. **Работа с длинными последовательностями:** Эффективно работают с длинными текстами, решая проблему затухающего градиента.
    3. **Прямые связи:** Каждое слово может напрямую взаимодействовать с любым другим словом в тексте, независимо от расстояния.

### 6.2. Архитектура Transformer и BERT

Архитектура Трансформера состоит из двух основных блоков: **Encoder (Кодировщик)** и **Decoder (Декодировщик)**.

- **Encoder (Кодировщик):** Обрабатывает входной текст и извлекает признаки (создает эмбеддинги).
- **Decoder (Декодировщик):** Генерирует выходную последовательность или классифицирует ее.

**Positional Encoding (Позиционное кодирование):** Специальный слой, который кодирует позицию слов, поскольку трансформер не имеет встроенного понимания порядка.

**BERT (Bidirectional Encoder Representations from Transformers)** — модель, предобученная на огромных объемах текста.

- **Ключевое отличие:** **Двунаправленность**. Модель анализирует контекст слова как слева, так и справа одновременно.
- **Предобучение:** Использует **Masked Language Modeling (MLM)** (предсказание замаскированных токенов) и **Next Sentence Prediction (NSP)** (предсказание, следует ли второе предложение за первым).
- **Применение для классификации:** Используется специальный токен **[CLS]** в начале текста, выходное представление которого содержит агрегированную информацию обо всем тексте и подается на классификационный слой.

### 6.3. Выбор современных моделей

Выбор модели зависит от требуемой точности, скорости работы и задачи.

|Модель|Архитектура|Особенности|Применение|
|:--|:--|:--|:--|
|**BERT, RoBERTa, ALBERT**|Encoder-only|Двунаправленный контекст, улучшенное предобучение (RoBERTa).|**Высокая точность, Классификация**.|
|**DistilBERT**|Encoder-only|Облегченная версия BERT (на 40% меньше, 97% качества).|**Скорость работы**, быстрая классификация в продакшене.|
|**GPT-3**|Decoder-only|Огромное количество параметров.|**Генерация текста**, диалоги.|
|**T5**|Encoder-Decoder|Text-to-Text подход.|**Универсальные задачи NLP** (классификация, суммаризация, перевод).|
|**ruBERT, rubert-tiny**|Спец. модели|Модели, адаптированные для русского языка.|Русский язык.|

---

## 7. Инструментарий NLP в Python

Для решения задач обработки естественного языка используются следующие библиотеки:

|Библиотека|Назначение|Особенности|Источник|
|:--|:--|:--|:--|
|**Hugging Face Transformers**|Современные модели|10,000+ предобученных моделей (BERT, GPT), простой API.||
|**scikit-learn**|Классическое ML|Методы TF-IDF, CountVectorizer, классификаторы (Logistic Regression, SVM).||
|**spaCy**|Высокопроизводительность|Быстрая предобработка, NER, поддержка русского языка.||
|**NLTK**|Базовая библиотека|Токенизация, стемминг, лемматизация, работа с корпусами.||
|**PyMorphy3**|Морфологический анализ|Точная лемматизация русских слов, определение частей речи.||
|**Gensim**|Эмбеддинги|Word2Vec, FastText, Doc2Vec, тематическое моделирование.||

**Практическая рекомендация:** Для продакшена часто используется связка: **spaCy** (для предобработки) + **Transformers** (для моделей). Для русского языка полезны **PyMorphy3** и **ruBERT**.