[[1 - вводная.pdf]]
### 1. Организационные моменты и «Правила игры»

Курс 2026 года ориентирован на практическое освоение инструментов обработки данных. Основной упор сделан на регулярную работу:

- **Лабораторные работы:** 6 работ по 12 баллов (итого **72 балла**). Дедлайны сокращены до 2–3 недель, чтобы избежать «штурмовщины» в конце семестра.
- **Рубежный контроль:** оценивается в **8 баллов**.
- **Экзамен:** 20 баллов (10 — теория, 10 — практика). Для получения оценки «отлично» экзамен обязателен.
- **Инструментарий:** В этом семестре вместо GitHub используется **Google Classroom и Google Colab**. Это обосновано тем, что GitHub плохо отображает формат Jupyter-тетрадок (.ipynb), а Colab является индустриальным стандартом для быстрых экспериментов в Data Science.

---

### 2. Философия Python в науке о данных

#### Определение Data Science

**Data Science (наука о данных)** — это область, объединяющая методики обработки колоссальных объемов данных, где основной фокус смещается с простых алгоритмов на **размерность данных** и **экспериментальный путь** решения задач.

> **Интуитивное соображение:** Если классические алгоритмы изучают, _как_ пройти по списку из $N$ элементов, то Data Science изучает, что делать, когда $N$ — это 10 миллиардов, и универсального алгоритма не существует.

#### Проблема «двух языков»

Исторически в науке существовал разрыв:

1. **Языки для экспериментов (MATLAB, R):** Позволяют быстро писать формулы и проверять гипотезы, но слишком медленны для реальной нагрузки.
2. **Языки для продакшена (C++, Fortran):** Высокая производительность, но крайне низкая скорость написания кода и проведения опытов.

#### Решение: Python как «Пульт управления»

Python решил эту проблему, став **связующим языком (glue language)**.

- **Концепция:** Мы используем Python для описания логики и управления потоками данных (как «пульт»), а все тяжелые вычисления делегируем библиотекам, написанным на C или Rust.
- **Парадокс производительности:** Python сам по себе — динамический и медленный язык, но в связке с C-библиотеками он работает со скоростью компилируемых языков.

---

### 3. SciPy Stack: Архитектура инструментов

Весь инструментарий Data Science (Scientific Python) представляет собой пятиуровневую пирамиду:

1. **Уровень 1: CPython.** Основа — интерпретатор, управление памятью и API для связи с C.
2. **Уровень 2: NumPy (Numerical Python).** Вводит объект **ndarray** (многомерный массив). Это фундамент для всех быстрых вычислений за счет непрерывного хранения данных в памяти.
3. **Уровень 3: SciPy (Scientific Python).** Библиотека математических методов (интегрирование, оптимизация, линтех). В отличие от стандартного модуля `math`, алгоритмы SciPy реализованы на C и работают с массивами NumPy.
4. **Уровень 4: Pandas.** Переход от математических матриц к **таблицам**. Основные абстракции: `DataFrame` (таблица с метками) и `Series` (проиндексированный список).
5. **Уровень 5: Высокоуровневый инструментарий.** Визуализация (Matplotlib, Seaborn) и машинное обучение (Scikit-Learn). Здесь начинается моделирование.

---

### 4. Интерактивные вычисления: IPython и Jupyter

#### Проблема цикла «edit-compile-run»

Традиционная разработка неэффективна для анализа данных. Если загрузка данных занимает 5 минут, нельзя перезапускать весь скрипт ради изменения одного графика.

> **Интуитивное соображение:** Jupyter позволяет нам «замораживать» состояние. Мы один раз загрузили миллиард строк в память и дальше работаем с ними интерактивно в разных ячейках, не теряя прогресс.

#### Архитектура Jupyter

Jupyter разделяет место написания кода и место его выполнения:

- **Frontend:** Интерфейс в браузере.
- **Kernel (Ядро):** Отдельный процесс (например, на Python), который выполняет код и хранит состояние переменных.
- **ZeroMQ:** «Брокер сообщений», который связывает их. Поскольку Python однопоточен, ZeroMQ берет на себя очередь запросов от пользователя.

**Пять каналов (сокетов) связи:**

- **Shell:** Основной канал. Принимает запросы на выполнение кода. Если ячейка долго считается, сокет блокируется (символ `[*]`).
- **IOPub:** Канал публикации вывода (текст `print`, графики, ошибки). Работает по модели «издатель-подписчик».
- **Stdin:** Для интерактивного ввода (функция `input()`). Блокирует всё ядро до ввода данных.
- **Control:** Канал с высоким приоритетом для команд управления (кнопка «Stop», перезапуск).
- **Heartbeat:** Проверка «жив» ли сервер. Пинг-понг между фронтендом и ядром.

---

### 5. Инструментарий IPython: Магия и Интроспекция

IPython расширяет возможности «ванильного» Python специальным синтаксисом:

1. **Интроспекция (изучение объектов):**
    - `?object` — показать **docstring** (описание, параметры).
    - `??object` — показать **исходный код** (не работает для скомпилированных C-расширений, таких как NumPy).
    - `*pattern*?` — поиск в пространстве имен (например, `np.*Array*?` найдет всё, что содержит «Array»).
2. **Магические команды (`%` — строчные, `%%` — ячеечные):**
    - `%time` / `%%time` — замер времени выполнения.
    - `%debug` — **пост-мортем отладка**. Позволяет зайти внутрь стека ошибки сразу после её возникновения без расстановки `print()`.
    - `%xmode` — настройка детализации ошибок (Plain, Context, Verbose).
3. **История:** Объекты `In` (список команд) и `Out` (словарь результатов). Позволяют получить результат вычисления ячейки, даже если он не был сохранен в переменную.
4. **Системная оболочка:** Команды через `!` (например, `!ls` или `!pip install`). Это взаимодействие не с ядром Python, а с операционной системой (виртуальной машиной).

---

### 6. NumPy: Почему это быстро?

#### Проблема списков Python

Стандартный список — это **гетерогенная** коллекция. В него можно положить и число, и строку, потому что список хранит не сами данные, а **указатели** на объекты `PyObject`.

- **Накладные расходы:** Чтобы хранить 1 байт данных (например, число), Python тратит 28 байт на метаданные (счетчик ссылок, код типа, размер).
- **Промахи кэша:** Объекты разбросаны по всей оперативной памяти («куче»), и процессору приходится «прыгать» за каждым элементом.

#### Решение NumPy (ndarray)

NumPy осознанно ограничивает гибкость ради скорости (принцип **Unboxed Data**):

- **Гомогенность:** Все элементы должны быть одного системного типа (например, `int32` или `float64`).
- **Непрерывность:** Данные лежат в памяти одним сплошным блоком, как в языке C.
- **Эффективность:** 1 млн чисел в NumPy займет ~4 МБ, а в списке Python — ~28 МБ (экономия в 7 раз!).

> **Интуитивное соображение (Параллель):** Список Python похож на сумку с разными предметами, разбросанными внутри — чтобы найти следующий, нужно порыться. Массив NumPy — это плотно уложенная пачка одинаковых кирпичей: зная, где лежит первый, процессор мгновенно находит все остальные, загружая их в кэш целыми блоками (**Locality of Reference**).

Это позволяет использовать **векторизацию (SIMD)**: одна инструкция процессора может обработать 4–8 чисел одновременно, так как они лежат подряд.