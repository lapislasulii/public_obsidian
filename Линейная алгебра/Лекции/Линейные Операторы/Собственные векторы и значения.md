
## 1. Обзор лекции

Лекция открывается мотивирующей проблемой: когда линейный оператор на конечномерном пространстве допускает представление диагональной матрицей? Для этого вводится понятие инвариантного подпространства, и объясняется, что «предел мечтаний» — разложение всего пространства в прямую сумму одномерных инвариантных подпространств. Это приводит к понятиям собственного вектора и собственного значения (числа). Разбираются конкретные примеры: нулевой оператор, тождественный оператор, гомотетия, проектор, отражение, оператор транспонирования, поворот вокруг оси, и оператор дифференцирования. Затем строится алгоритм нахождения собственных значений через характеристический многочлен — определитель матрицы $tE - A$. Доказывается независимость характеристического многочлена от выбора базиса, его связь со следом, определителем и суммами главных миноров. Завершается лекция теоремой о линейной независимости собственных подпространств и достаточным условием диагонализируемости: наличие $n$ различных собственных значений в исходном поле гарантирует диагонализируемость.

---

## 2. Предварительные сведения

**Линейный оператор** — отображение $A: V \to V$, сохраняющее операции сложения и умножения на скаляр.

**Инвариантное подпространство** оператора $A$ — подпространство $U \subseteq V$ такое, что $A(U) \subseteq U$.

**Прямая сумма подпространств** $V = U_1 \oplus \cdots \oplus U_k$ — разложение, при котором каждый вектор единственным образом представляется суммой из слагаемых.

**Матрица оператора** в базисе $e_1, \ldots, e_n$ — матрица $A = [a_{ij}]$, где $A e_j = \sum_i a_{ij} e_i$.

**Смена базиса** — при переходе к новому базису матрица оператора преобразуется по формуле $\tilde{A} = C^{-1} A C$, где $C$ — матрица перехода.

**Определитель оператора** — определитель любой его матрицы (не зависит от базиса).

**Ядро оператора** $\ker A = {x \in V : Ax = 0}$ — подпространство.

**Теорема о ядре и образе**: $\dim \ker A + \dim \operatorname{Im} A = \dim V$.

**Однородная система линейных уравнений** имеет ненулевое решение тогда и только тогда, когда определитель матрицы системы равен нулю (обращение принципа Крамера).

**Определитель Вандермонда** $\det[(\lambda_i^{j-1})] = \prod_{i > j}(\lambda_i - \lambda_j) \neq 0$ при попарно различных $\lambda_i$ — ключевой факт для линейной независимости экспонент.

---

## 3. Основное содержание

---

### 3.1. Мотивация: диагонализируемость и инвариантные подпространства

**МОТИВАЦИЯ.** Если матрица оператора диагональна, то вычисления с ним предельно просты: степени, экспоненты, системы дифференциальных уравнений — всё сводится к работе с отдельными числами. Возникает вопрос: при каких условиях оператор диагонализируем?

**ИНТУИЦИЯ.** Если пространство $V$ разложено в прямую сумму одномерных инвариантных подпространств $V = L_1 \oplus \cdots \oplus L_n$, то оператор $A$ действует на каждом $L_i$ как умножение на некоторое число $\lambda_i$. Выбрав в каждом $L_i$ один ненулевой вектор $e_i$, мы получаем базис ${e_1,\ldots,e_n}$, в котором матрица оператора диагональна:

$$[A]_{e} = \operatorname{diag}(\lambda_1, \ldots, \lambda_n).$$

**СВЯЗИ.** Одномерное инвариантное подпространство порождается ровно одним (с точностью до скаляра) вектором, который под действием оператора переходит в себя с точностью до масштабирования — это и есть понятие собственного вектора.

---

### 3.2. Определение собственного вектора и собственного значения

**Определение 3.1** (Собственный вектор и собственное значение). Пусть $A: V \to V$ — линейный оператор над полем $\mathbb{F}$. Ненулевой вектор $x \in V$, $x \neq 0$, называется **собственным вектором** оператора $A$, если существует $\lambda \in \mathbb{F}$ такое, что $$A x = \lambda x.$$ Скаляр $\lambda$ называется **собственным значением** (собственным числом) оператора $A$, соответствующим собственному вектору $x$.

**МОТИВАЦИЯ.** Собственный вектор — это направление, которое оператор не поворачивает, а лишь растягивает или сжимает (в том числе с отрицательным коэффициентом). Такие направления — «скелет» оператора.

**ИНТУИЦИЯ.** Геометрически: если $A$ — матрица линейного преобразования плоскости, то собственные векторы — это те стрелки, которые остаются на своих прямых после применения преобразования.

**ТИПИЧНЫЕ ОШИБКИ.**

- Нулевой вектор не является собственным по определению, даже если $A \cdot 0 = \lambda \cdot 0$ выполнено тривиально.
- Собственное значение $\lambda = 0$ вполне допустимо (оно означает, что $x \in \ker A$, $x \neq 0$).
- Собственный вектор не единственен: любой ненулевой кратный собственного вектора тоже является собственным вектором с тем же собственным значением.

---

### 3.3. Примеры: простые операторы

**Пример 3.1** (Нулевой оператор). Пусть $\mathbf{0}: V \to V$ — нулевой оператор, $\mathbf{0}(x) = 0$ для всех $x$.

Тогда $\mathbf{0}(x) = 0 = 0 \cdot x$ для любого $x \neq 0$.

Следовательно, **все** ненулевые векторы из $V$ являются собственными, единственное собственное значение $\lambda = 0$.

---

**Пример 3.2** (Тождественный оператор). Пусть $E: V \to V$, $E(x) = x$.

Тогда $E(x) = x = 1 \cdot x$, и снова **все** ненулевые векторы собственные, $\lambda = 1$.

---

**Пример 3.3** (Гомотетия с коэффициентом $\mu$). Пусть $H_\mu(x) = \mu x$.

Тогда $H_\mu(x) = \mu x$, все ненулевые векторы собственные, $\lambda = \mu$.

---

### 3.4. Пример: проектор

**Пример 3.4** (Собственные значения проектора). Пусть $P: V \to V$ — проектор, то есть $P^2 = P$ (идемпотент). Пусть $V = U \oplus W$, $P$ — проектор на $U$ вдоль $W$.

Найдём все возможные $\lambda$.

**Шаг 1.** Пусть $v$ — собственный вектор: $Pv = \lambda v$, $v \neq 0$.

**Шаг 2.** Подействуем проектором ещё раз на обе части: $$P(Pv) = P(\lambda v) = \lambda (Pv) = \lambda \cdot \lambda v = \lambda^2 v.$$

**Шаг 3.** Так как $P^2 = P$: $$P^2 v = Pv = \lambda v.$$

**Шаг 4.** Следовательно, $\lambda^2 v = \lambda v$, то есть $(\lambda^2 - \lambda)v = 0$.

**Шаг 5.** Так как $v \neq 0$, получаем $\lambda^2 - \lambda = 0$, откуда $\lambda(\lambda - 1) = 0$.

Таким образом, $\lambda = 0$ или $\lambda = 1$.

**Описание собственных пространств:**

- Собственное пространство с $\lambda = 1$: $Pv = v$ — это в точности $U$ (векторы, которые $P$ переводит в себя).
- Собственное пространство с $\lambda = 0$: $Pv = 0$ — это в точности $W$ (векторы, которые $P$ обнуляет).

[восстановлено] Матрица проектора в базисе, согласованном с разложением $V = U \oplus W$, имеет диагональный вид: единицы на позициях, соответствующих $\dim U$, и нули на остальных. Это показывает, что проектор диагонализируем всегда, несмотря на то что различных собственных значений может быть меньше $n$. [/восстановлено]

---

### 3.5. Пример: отражение

**Пример 3.5** (Собственные значения отражения). Пусть $R: V \to V$ — отражение, то есть $R^2 = E$ (инволюция). Пусть $V = U \oplus W$, $R$ меняет знак компоненты из $W$: $$R(u + w) = u - w, \quad u \in U, ; w \in W.$$

**Шаг 1.** Пусть $v$ — собственный вектор: $Rv = \lambda v$.

**Шаг 2.** Подействуем $R$ ещё раз: $$R^2 v = R(\lambda v) = \lambda R v = \lambda^2 v.$$

**Шаг 3.** Так как $R^2 = E$: $$R^2 v = v.$$

**Шаг 4.** Следовательно, $\lambda^2 v = v$, то есть $(\lambda^2 - 1)v = 0$.

**Шаг 5.** Так как $v \neq 0$: $\lambda^2 = 1$, откуда $\lambda = \pm 1$ (при условии, что $\operatorname{char} \mathbb{F} \neq 2$).

**Описание собственных пространств:**

- $\lambda = 1$: $Rv = v$, то есть $v \in U$ (симметричная часть).
- $\lambda = -1$: $Rv = -v$, то есть $v \in W$ (кососимметричная часть).

**Пример 3.5а** (Оператор транспонирования). Рассмотрим пространство $M_{n}(\mathbb{F})$ всех квадратных матриц и оператор $T: A \mapsto A^{\top}$.

Так как $T^2(A) = (A^\top)^\top = A$, это отражение (инволюция).

Разложение: $M_n = \mathrm{Sym}_n \oplus \mathrm{Skew}_n$, где $\mathrm{Sym}_n$ — симметрические, $\mathrm{Skew}_n$ — кососимметрические матрицы.

- Симметрические матрицы ($A^\top = A$) — собственные с $\lambda = 1$.
- Кососимметрические ($A^\top = -A$) — собственные с $\lambda = -1$.

[восстановлено] Чтобы разложение $M_n = \mathrm{Sym}_n \oplus \mathrm{Skew}_n$ работало, необходимо $\operatorname{char} \mathbb{F} \neq 2$, так как в характеристике 2 симметрические и кососимметрические матрицы совпадают. [/восстановлено]

---

### 3.6. Пример: поворот вокруг оси в $\mathbb{R}^3$

**Пример 3.6** (Поворот вокруг оси). Пусть $V = \mathbb{R}^3$, $\ell$ — некоторая ось (одномерное подпространство), $R_\varphi$ — поворот вокруг $\ell$ на угол $\varphi$.

**Случай 1.** Любой вектор $v \in \ell \setminus {0}$ переходит в себя: $R_\varphi(v) = v$, значит $\lambda = 1$ — всегда собственное значение, а $\ell$ — собственное подпространство.

**Случай 2.** Если $\varphi = 2\pi k$, $k \in \mathbb{Z}$ (полный оборот), то $R_\varphi = E$, и все ненулевые векторы собственные с $\lambda = 1$.

**Случай 3.** Если $\varphi = (2k+1)\pi$ (поворот на $\pi$), то $R_\varphi$ действует как $-E$ на плоскость, перпендикулярную $\ell$: векторы из этой плоскости — собственные с $\lambda = -1$.

**Случай 4.** При произвольном $\varphi \neq 0, \pi \pmod{2\pi}$ над $\mathbb{R}$ нет других вещественных собственных значений: никакой вектор, не лежащий на $\ell$, не переходит в пропорциональный.

[восстановлено] Над $\mathbb{C}$ характеристический многочлен матрицы поворота на угол $\varphi$ в плоскости имеет корни $e^{\pm i\varphi}$, которые вещественны тогда и только тогда, когда $\varphi = 0$ или $\varphi = \pi$. [/восстановлено]

---

### 3.7. Пример: оператор дифференцирования

**Пример 3.7** (Дифференцирование многочленов). Пусть $V = \mathbb{R}[x]_{\leq n}$ — пространство вещественных многочленов степени не выше $n$, $D$ — оператор дифференцирования.

$D(f) = \lambda f$ означает, что $f' = \lambda f$. Для многочленов это возможно только при $\lambda = 0$, так как производная многочлена степени $k \geq 1$ — многочлен степени $k - 1$, не пропорциональный исходному (в пространстве многочленов над $\mathbb{R}$). При $\lambda = 0$: $f' = 0$, значит $f = c \neq 0$ — ненулевые константы.

Таким образом, единственное собственное значение $\lambda = 0$, собственные векторы — ненулевые константы.

---

**Пример 3.8** (Дифференцирование на пространстве функций). Пусть $V$ — пространство функций вида $\sum_{i} \alpha_i e^{\lambda_i x}$, $D$ — дифференцирование.

Тогда $D(e^{\lambda x}) = \lambda e^{\lambda x}$, то есть $e^{\lambda x}$ — собственный вектор с собственным значением $\lambda$.

**Утверждение.** Никакая нетривиальная линейная комбинация $\alpha e^{\lambda_1 x} + \beta e^{\lambda_2 x}$, $\lambda_1 \neq \lambda_2$, не является собственным вектором.

**Доказательство.** Пусть $D(\alpha e^{\lambda_1 x} + \beta e^{\lambda_2 x}) = \mu (\alpha e^{\lambda_1 x} + \beta e^{\lambda_2 x})$.

Тогда $\alpha \lambda_1 e^{\lambda_1 x} + \beta \lambda_2 e^{\lambda_2 x} = \mu \alpha e^{\lambda_1 x} + \mu \beta e^{\lambda_2 x}$.

Перенося всё в одну сторону: $$\alpha(\lambda_1 - \mu) e^{\lambda_1 x} + \beta(\lambda_2 - \mu)e^{\lambda_2 x} = 0.$$

Функции $e^{\lambda_1 x}$ и $e^{\lambda_2 x}$ при $\lambda_1 \neq \lambda_2$ линейно независимы (определитель Вандермонда ненулевой). Следовательно, $\alpha(\lambda_1 - \mu) = 0$ и $\beta(\lambda_2 - \mu) = 0$. Поскольку нельзя одновременно $\lambda_1 = \mu$ и $\lambda_2 = \mu$ при $\lambda_1 \neq \lambda_2$, то $\alpha = 0$ или $\beta = 0$, то есть комбинация тривиальна. $\square$

---

### 3.8. Собственное подпространство

**Определение 3.2** (Собственное подпространство). Пусть $\lambda \in \mathbb{F}$ — собственное значение оператора $A$. **Собственным подпространством** оператора $A$, соответствующим $\lambda$, называется $$V_\lambda = \ker(A - \lambda E) = {x \in V : Ax = \lambda x}.$$

**МОТИВАЦИЯ.** Объединение всех собственных векторов с данным $\lambda$ вместе с нулём образует подпространство (проверка линейности тривиальна). Это ядро оператора $A - \lambda E$.

**ИНТУИЦИЯ.** $V_\lambda$ — это «eigenspace»: целое подпространство, на котором $A$ действует как гомотетия с коэффициентом $\lambda$.

**Замечание 3.1.** $V_\lambda$ всегда содержит нулевой вектор, который, однако, не является собственным по определению. Собственные векторы с $\lambda$ — это в точности $V_\lambda \setminus {0}$.

---

### 3.9. Как искать собственные значения: условие нетривиальности ядра

Собственное значение $\lambda$ существует тогда и только тогда, когда $\ker(A - \lambda E) \neq {0}$, то есть оператор $A - \lambda E$ необратим.

**Критерий необратимости:** оператор необратим $\Leftrightarrow$ он не является автоморфизмом $\Leftrightarrow$ его определитель равен нулю.

Следовательно, $\lambda$ — собственное значение $\Leftrightarrow$ $$\det(A - \lambda E) = 0.$$

На уровне матриц: зафиксируем базис и пусть $A$ — матрица оператора. Условие $Ax = \lambda x$ равносильно $(A - \lambda E)x = 0$ — однородная система. У неё есть ненулевое решение $\Leftrightarrow$ $\det(A - \lambda E) = 0$.

---

### 3.10. Характеристический многочлен

**Определение 3.3** (Характеристический многочлен). Пусть $A$ — матрица $n \times n$ (матрица некоторого оператора в фиксированном базисе). **Характеристическим многочленом** оператора называется $$\chi_A(t) = \det(tE - A) = \begin{vmatrix} t - a_{11} & -a_{12} & \cdots & -a_{1n} \\ -a_{21} & t - a_{22} & \cdots & -a_{2n} \\ \vdots & & \ddots & \vdots \\ -a_{n1} & -a_{n2} & \cdots & t - a_{nn} \end{vmatrix}.$$

[восстановлено] Некоторые авторы определяют характеристический многочлен как $\det(A - tE)$. Мы используем $\det(tE - A)$, умноженный при необходимости на $(-1)^n$, чтобы старший коэффициент был равен $+1$. [/восстановлено]

**Замечание 3.2.** Собственные значения оператора в поле $\mathbb{F}$ — это в точности корни $\chi_A(t)$ в $\mathbb{F}$.

---

### 3.11. Независимость характеристического многочлена от базиса

**Теорема 3.1.** Характеристический многочлен оператора не зависит от выбора базиса.

**Доказательство.**

**Идея.** Матрицы одного оператора в разных базисах связаны соотношением подобия $\tilde{A} = C^{-1}AC$. Надо показать, что $\det(tE - \tilde{A}) = \det(tE - A)$.

**Шаг 1.** Пусть $A$ и $\tilde{A} = C^{-1}AC$ — матрицы оператора в двух базисах.

**Шаг 2.** Вычислим характеристический многочлен $\tilde{A}$: $$\det(tE - \tilde{A}) = \det(tE - C^{-1}AC).$$

**Шаг 3.** Заметим, что $tE = C^{-1}(tE)C$ (единичная матрица коммутирует со всем): $$tE - C^{-1}AC = C^{-1}(tE)C - C^{-1}AC = C^{-1}(tE - A)C.$$

**Шаг 4.** Применим мультипликативность определителя: $$\det(C^{-1}(tE - A)C) = \det(C^{-1}) \cdot \det(tE - A) \cdot \det(C).$$

**Шаг 5.** Так как $\det(C^{-1}) \cdot \det(C) = 1$: $$\det(tE - \tilde{A}) = \det(tE - A). \quad \square$$

**Следствие 3.1.** Корректно говорить о характеристическом многочлене **оператора**, а не отдельной матрицы.

---

### 3.12. Структура характеристического многочлена: коэффициенты

Разложим $\chi_A(t) = \det(tE - A)$ и найдём коэффициенты.

**Теорема 3.2** (Коэффициенты характеристического многочлена). $$\chi_A(t) = t^n - \operatorname{tr}(A), t^{n-1} + M_2, t^{n-2} - \cdots + (-1)^n \det(A),$$ где $M_k = \sum \text{(все главные миноры порядка } k\text{)}$.

В частности:

- Старший коэффициент при $t^n$: равен $1$.
- Коэффициент при $t^{n-1}$: равен $-\operatorname{tr}(A) = -(a_{11} + \cdots + a_{nn})$.
- Свободный член: $\chi_A(0) = \det(-A) = (-1)^n \det(A)$, то есть свободный коэффициент равен $(-1)^n \det A$.

**Определение 3.4** (Главный минор). **Главным минором** порядка $k$ матрицы $A$ называется определитель подматрицы, получаемой вычёркиванием одних и тех же $n - k$ строк и $n - k$ столбцов, симметричных относительно главной диагонали.

[доказательство добавлено]

**Доказательство общей формулы.**

Обозначим $B(t) = tE - A$. Тогда $\chi_A(t) = \det B(t)$.

Раскладываем по формуле Лейбница: $$\det B(t) = \sum_{\sigma \in S_n} \operatorname{sgn}(\sigma) \prod_{i=1}^n B_{i,\sigma(i)}(t),$$ где $B_{i,j}(t) = t\delta_{ij} - a_{ij}$.

При перемножении $n$ элементов $B_{i,\sigma(i)}(t)$ степень по $t$ получается равной числу индексов $i$, для которых $\sigma(i) = i$ (то есть фиксированных точек перестановки $\sigma$), ибо только диагональные элементы содержат $t$.

Коэффициент при $t^{n-k}$ получается из всех перестановок, имеющих ровно $n - k$ фиксированных точек. Фиксированные точки образуют набор индексов $I = {i_1, \ldots, i_{n-k}}$; произведение соответствующих диагональных элементов даёт $(-1)^{n-k} t^{n-k}$ (с учётом знака минус в $t\delta_{ij} - a_{ij}$). Оставшиеся $k$ индексов $J = {1,\ldots,n} \setminus I$ пробегают перестановки без фиксированных точек (беспорядки); их вклад суммируется в определитель подматрицы $A_J$ (главный минор порядка $k$, отвечающий индексам $J$), взятый с соответствующим знаком.

Суммируя по всем наборам $J$ мощности $k$ и соответствующим главным минорам, получаем: $$[\text{коэффициент при } t^{n-k}] = (-1)^k M_k,$$ где $M_k$ — сумма всех главных миноров порядка $k$. $\square$

**Частные случаи:**

При $k = 1$: $M_1 = a_{11} + \cdots + a_{nn} = \operatorname{tr}(A)$.

При $k = n$: $M_n = \det(A)$.

---

**Пример 3.9** (Матрица поворота на угол $\varphi$). $$A = \begin{pmatrix} \cos\varphi & -\sin\varphi \ \sin\varphi & \cos\varphi \end{pmatrix}.$$

$$\chi_A(t) = t^2 - \operatorname{tr}(A) \cdot t + \det(A) = t^2 - 2\cos\varphi \cdot t + 1.$$

Дискриминант: $D = 4\cos^2\varphi - 4 = 4(\cos^2\varphi - 1) = -4\sin^2\varphi \leq 0$.

Над $\mathbb{R}$: вещественных корней нет при $\varphi \neq 0, \pi$.

Над $\mathbb{C}$: корни $e^{\pm i\varphi}$, всегда два комплексных корня.

---

### 3.13. Характеристические числа vs. собственные значения

**Определение 3.5.** Корни характеристического многочлена $\chi_A(t)$ в **алгебраическом замыкании** $\overline{\mathbb{F}}$ поля $\mathbb{F}$ называются **характеристическими числами** оператора.

Корни характеристического многочлена в **исходном поле** $\mathbb{F}$ — это в точности **собственные значения**.

**Следствие 3.2.** Число различных характеристических чисел (с учётом кратности) равно ровно $n$ в алгебраически замкнутом поле. Таким образом, **количество различных собственных значений не превосходит $n$**.

---

### 3.14. Теорема о линейной независимости собственных подпространств

**Теорема 3.3.** Собственные подпространства $V_{\lambda_1}, V_{\lambda_2}, \ldots, V_{\lambda_k}$, соответствующие **различным** собственным значениям $\lambda_1 \neq \lambda_2 \neq \cdots \neq \lambda_k$, **линейно независимы**, то есть: если

$$x_1 + x_2 + \cdots + x_k = 0, \quad x_i \in V_{\lambda_i},$$

то $x_i = 0$ для всех $i$.

**МОТИВАЦИЯ.** Это ключевое утверждение для диагонализации: собственные векторы с различными собственными значениями не могут «мешать» друг другу, что позволяет взять их одновременно в базис.

**ИНТУИЦИЯ.** Оператор «видит» собственные вектора разных собственных значений по-разному: он умножает каждый на своё число. Зависимость означала бы, что разные «масштабирования» дают одинаковый результат, что противоречит различности $\lambda_i$.

**Доказательство** (индукция по $k$).

**База** ($k = 1$). Если $x_1 = 0$, то доказывать нечего. А из $x_1 \in V_{\lambda_1}$, $x_1 \neq 0$ нарушения нет.

Более точно: из $x_1 = 0$ следует $x_1 = 0$. $\square$ для $k = 1$.

**Индукционный переход.** Пусть утверждение верно для $k - 1$ подпространств. Докажем для $k$.

Пусть $x_1 + x_2 + \cdots + x_k = 0$ с $x_i \in V_{\lambda_i}$.

**Шаг 1.** Подействуем оператором $A$ на обе части: $$A(x_1 + \cdots + x_k) = A(0) = 0.$$

Так как $A$ линеен и $x_i \in V_{\lambda_i}$: $$\lambda_1 x_1 + \lambda_2 x_2 + \cdots + \lambda_k x_k = 0. \tag{I}$$

**Шаг 2.** Умножим исходное равенство $x_1 + \cdots + x_k = 0$ на $\lambda_k$: $$\lambda_k x_1 + \lambda_k x_2 + \cdots + \lambda_k x_k = 0. \tag{II}$$

**Шаг 3.** Вычтем (II) из (I): $$(\lambda_1 - \lambda_k)x_1 + (\lambda_2 - \lambda_k)x_2 + \cdots + (\lambda_{k-1} - \lambda_k)x_{k-1} = 0.$$

Это равенство имеет $k - 1$ слагаемое, причём $(\lambda_i - \lambda_k)x_i \in V_{\lambda_i}$ (подпространство замкнуто умножением на скаляр).

**Шаг 4.** По индукционному предположению (линейная независимость первых $k - 1$ подпространств): $$(\lambda_i - \lambda_k)x_i = 0 \quad \text{для } i = 1, \ldots, k-1.$$

**Шаг 5.** Так как $\lambda_i \neq \lambda_k$ при $i < k$, коэффициенты $\lambda_i - \lambda_k \neq 0$. Следовательно, $x_i = 0$ для $i = 1, \ldots, k-1$.

**Шаг 6.** Подставляем в исходное равенство: $$0 + \cdots + 0 + x_k = 0 \implies x_k = 0.$$

Таким образом, все $x_i = 0$. $\square$

---

### 3.15. Достаточное условие диагонализируемости

**Определение 3.6** (Диагонализируемый оператор). Оператор $A: V \to V$ называется **диагонализируемым**, если существует базис пространства $V$, в котором матрица оператора $A$ диагональна.

**Теорема 3.4** (Достаточное условие диагонализируемости). Если характеристический многочлен оператора $A$ имеет в поле $\mathbb{F}$ ровно $n$ **различных** корней (собственных значений) $\lambda_1, \lambda_2, \ldots, \lambda_n \in \mathbb{F}$, то оператор $A$ диагонализируем. При этом в базисе из собственных векторов матрица оператора принимает вид

$$[A]_e = \begin{pmatrix} \lambda_1 & & 0 \\ & \ddots & \\ 0 & & \lambda_n \end{pmatrix}.$$

**Доказательство.**

**Шаг 1.** По условию, $\lambda_1, \ldots, \lambda_n$ — различные собственные значения. Каждому $\lambda_i$ соответствует хотя бы один ненулевой собственный вектор $e_i \in V_{\lambda_i}$.

**Шаг 2.** По теореме 3.3, подпространства $V_{\lambda_1}, \ldots, V_{\lambda_n}$ линейно независимы. Поскольку размерность каждого $V_{\lambda_i} \geq 1$, совокупность ${e_1, \ldots, e_n}$ линейно независима.

**Шаг 3.** Так как $\dim V = n$ и мы нашли $n$ линейно независимых векторов, они образуют базис $V$.

**Шаг 4.** В этом базисе $Ae_i = \lambda_i e_i$, поэтому матрица оператора — диагональная с $\lambda_i$ на диагонали. $\square$

**МОТИВАЦИЯ.** Если у оператора $n$ различных собственных значений, то у него есть $n$ «привилегированных» направлений, в которых он действует просто. Выбрав эти направления в качестве базиса, мы диагонализируем матрицу.

**ТИПИЧНЫЕ ОШИБКИ.**

- Условие является **достаточным, но не необходимым**. Есть операторы (например, проектор, тождественный оператор) с меньшим числом различных собственных значений, которые тем не менее диагонализируемы.
- «Ровно $n$ различных корней» означает: все корни $\chi_A$ в $\mathbb{F}$ различны. Если характеристический многочлен не раскладывается полностью на линейные множители в $\mathbb{F}$ (например, поворот над $\mathbb{R}$), то собственных значений меньше $n$, и метод не применим.

**Пример 3.10** (Проектор — пример недостаточности условия). Проектор $P$ в $\mathbb{R}^n$ имеет только два различных собственных значения ${0, 1}$ (меньше $n$ при $n > 2$), но он диагонализируем — в базисе из $\dim U$ единичных и $\dim W$ нулевых его матрица диагональна.

---

## 4. Справочный лист

|Понятие|Формула|
|---|---|
|Собственный вектор|$Ax = \lambda x$, $x \neq 0$|
|Собственное подпространство|$V_\lambda = \ker(A - \lambda E)$|
|Условие нахождения $\lambda$|$\det(A - \lambda E) = 0$|
|Характеристический многочлен|$\chi_A(t) = \det(tE - A)$|
|Структура $\chi_A$|$t^n - \operatorname{tr}(A),t^{n-1} + M_2,t^{n-2} - \cdots + (-1)^n\det(A)$|
|Независимость $\chi_A$ от базиса|$\det(tE - C^{-1}AC) = \det(tE - A)$|
|$n$ различных $\lambda$ $\Rightarrow$|диагонализируемость|
|Собственные значения проектора|${0, 1}$|
|Собственные значения отражения|${+1, -1}$ (при $\operatorname{char}\mathbb{F} \neq 2$)|
|Собственные функции $D$|$e^{\lambda x}$ с собственным значением $\lambda$|

---

## 5. Концептуальное резюме

Центральная идея лекции: линейный оператор устроен наиболее просто, если пространство раскладывается в прямую сумму инвариантных подпространств, причём одномерных — тогда матрица диагональна. Носителями такой структуры являются собственные векторы: ненулевые векторы, которые оператор лишь масштабирует. Собственные значения находятся из характеристического уравнения $\det(A - \lambda E) = 0$, а характеристический многочлен $\chi_A(t) = \det(tE - A)$ является инвариантом оператора (не зависит от базиса) и кодирует важнейшие скалярные характеристики: след ($M_1$), суммы главных миноров ($M_k$), и определитель ($M_n$). Фундаментальная теорема об линейной независимости собственных подпространств обеспечивает: при наличии $n$ различных собственных значений оператор диагонализируем, а базис из собственных векторов переводит его матрицу в диагональную форму с собственными значениями на диагонали.

---

## 6. Связи с более широкой математикой

**Теория нормальных форм.** Когда характеристический многочлен имеет кратные корни, диагонализация невозможна в общем случае. Вместо неё строится жорданова нормальная форма — блочно-диагональная матрица с жордановыми клетками. Это следующий шаг после данной лекции.

**Алгебра:** Теорема Гамильтона–Кэли утверждает, что $\chi_A(A) = 0$ — оператор является корнем собственного характеристического многочлена. Этот результат органично связан с понятием минимального многочлена оператора.

**Функциональный анализ.** В бесконечномерных пространствах понятие «собственного значения» расширяется до понятия спектра оператора. Для самосопряжённых операторов в гильбертовых пространствах — спектральная теорема (обобщение диагонализации).

**Дифференциальные уравнения.** Система $\dot{x} = Ax$ решается через собственные значения и векторы: $x(t) = e^{At}x_0$. Если $A$ диагонализируема, решение принимает явный вид через экспоненты $e^{\lambda_i t}$.

**Квантовая механика.** Операторы физических наблюдаемых — самосопряжённые операторы. Их собственные значения — возможные результаты измерений, собственные векторы — соответствующие стационарные состояния.

**Геометрия.** Собственные вектора матрицы ковариации — главные оси эллипсоида инерции. Это лежит в основе метода главных компонент (PCA) в машинном обучении.