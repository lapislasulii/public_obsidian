
### Билет 1. Векторные пространства: определение и примеры. Внешняя прямая сумма пространств. Подпространства. Факторпространство.

#### 1.1. Векторное пространство: определение и примеры

**Определение:**
Векторное пространство $V$ над полем $F$ — это непустое множество элементов, называемых **векторами**, для которых определены две операции:
1.  **Внутренняя операция сложения:** Сложение двух векторов $u, v \in V$ дает вектор $u+v \in V$.
2.  **Внешняя операция умножения на скаляр:** Умножение элемента $\lambda \in F$ (скаляра) на вектор $u \in V$ дает вектор $\lambda u \in V$.

Эти операции должны удовлетворять следующим аксиомам:

**Аксиомы сложения векторов (V, +) образует абелеву группу:**
1.  **Ассоциативность:** $(u+v)+w = u+(v+w)$ для всех $u, v, w \in V$.
2.  **Коммутативность:** $u+v = v+u$ для всех $u, v \in V$.
3.  **Существование нулевого вектора:** Существует элемент $\mathbf{0} \in V$ такой, что $u+\mathbf{0} = u$ для всех $u \in V$.
4.  **Существование противоположного вектора:** Для каждого $u \in V$ существует элемент $-u \in V$ такой, что $u+(-u) = \mathbf{0}$.

**Аксиомы умножения на скаляр:**
5.  **Внешняя дистрибутивность (по векторам):** $\lambda(u+v) = \lambda u + \lambda v$ для всех $\lambda \in F$ и $u, v \in V$.
6.  **Внешняя дистрибутивность (по скалярам):** $(\lambda+\mu)u = \lambda u + \mu u$ для всех $\lambda, \mu \in F$ и $u \in V$.
7.  **Внешняя ассоциативность:** $\lambda(\mu u) = (\lambda\mu)u$ для всех $\lambda, \mu \in F$ и $u \in V$.
8.  **Единичный скаляр:** $1 \cdot u = u$ для всех $u \in V$, где $1$ — единичный элемент поля $F$.

**Примеры векторных пространств:**
*   **Нулевое пространство:** Множество, состоящее только из нулевого вектора $\{\mathbf{0}\}$.
*   **Координатное (арифметическое) пространство $F^N$:** Множество всех столбцов (или строк) высоты $N$ с элементами из поля $F$. Операции сложения и умножения на скаляр определяются покомпонентно. Например, $\mathbb{R}^3$ — привычное трехмерное пространство.
*   **Пространство отображений (функций):** Множество всех функций $f: X \to F$ из некоторого множества $X$ в поле $F$. Операции сложения функций и умножения на скаляр определяются поточечно: $(f+g)(x) = f(x)+g(x)$ и $(\lambda f)(x) = \lambda f(x)$.
*   **Поле над своим подполем:** Любое поле $F$ является векторным пространством над своим подполем $K$. Например, комплексные числа $\mathbb{C}$ являются векторным пространством над полем вещественных чисел $\mathbb{R}$.
*   **Матрицы $M_{m \times n}(F)$:** Множество всех прямоугольных матриц размера $m \times n$ с элементами из поля $F$. Операции сложения матриц и умножения матрицы на скаляр определяются стандартным образом.
*   **Многочлены $P_n[x]$:** Множество всех многочленов степени не выше $n$ с коэффициентами из поля $F$. Операции сложения многочленов и умножения многочлена на скаляр определяются стандартным образом. (Важно: многочлены, степень которых *в точности* равна $n$, не образуют В.П., так как при сложении или вычитании степень может уменьшиться).
*   **Пространство решений однородной СЛАУ $A\mathbf{x} = \mathbf{0}$:** Множество всех решений однородной системы линейных алгебраических уравнений является подпространством $F^N$.

#### 1.2. Внешняя прямая сумма пространств

**Определение:**
Пусть $U$ и $V$ — два векторных пространства над одним и тем же полем $F$. **Внешняя прямая сумма** $U \oplus V$ — это новое векторное пространство, элементами которого являются упорядоченные пары $(u, v)$, где $u \in U$ и $v \in V$. Операции сложения и умножения на скаляр определяются покомпонентно:
*   Сложение: $(u_1, v_1) + (u_2, v_2) = (u_1+u_2, v_1+v_2)$
*   Умножение на скаляр: $\lambda(u, v) = (\lambda u, \lambda v)$

**Свойства:**
*   Внешняя прямая сумма $U \oplus V$ сама является векторным пространством.
*   **Размерность внешней прямой суммы:** Если $U$ и $V$ конечномерны, то $\dim(U \oplus V) = \dim(U) + \dim(V)$.
    *   **Доказательство:** Пусть $E_U = \{e_1, \dots, e_n\}$ — базис $U$, и $E_V = \{f_1, \dots, f_m\}$ — базис $V$. Тогда система векторов $B = \{(e_1, \mathbf{0}_V), \dots, (e_n, \mathbf{0}_V), (\mathbf{0}_U, f_1), \dots, (\mathbf{0}_U, f_m)\}$ образует базис $U \oplus V$. Эта система состоит из $n+m$ векторов, которые линейно независимы и порождают $U \oplus V$.

#### 1.3. Подпространства

**Определение:**
Подмножество $U$ векторного пространства $V$ над полем $F$ называется **подпространством** $V$, если $U$ само является векторным пространством над $F$ относительно тех же операций сложения и умножения на скаляр, что и в $V$.

**Критерий подпространства:**
Непустое подмножество $U \subseteq V$ является подпространством $V$ тогда и только тогда, когда оно **замкнуто** относительно операций сложения и умножения на скаляр:
1.  Для любых $u_1, u_2 \in U$, $u_1+u_2 \in U$.
2.  Для любого $\lambda \in F$ и $u \in U$, $\lambda u \in U$.

**Доказательство критерия:**
*   **Необходимость ($\implies$):** Если $U$ — подпространство, то по определению оно является векторным пространством, а значит, операции сложения и умножения на скаляр определены и замкнуты внутри $U$.
*   **Достаточность ($\impliedby$):** Предположим, $U$ замкнуто относительно сложения и умножения на скаляр.
    *   Аксиомы ассоциативности и коммутативности сложения, а также аксиомы умножения на скаляр (5-8) выполняются в $U$, так как они выполняются во всем $V$, а $U$ является подмножеством $V$.
    *   Нулевой вектор: Так как $U$ непусто, выберем любой $u \in U$. Тогда $0 \cdot u = \mathbf{0} \in U$ (по аксиоме 7 и замкнутости по умножению на скаляр).
    *   Противоположный вектор: Для любого $u \in U$, $(-1) \cdot u = -u \in U$ (по аксиоме 8 и замкнутости по умножению на скаляр).
    Таким образом, все аксиомы векторного пространства выполняются для $U$.

**Примеры подпространств:**
*   **Тривиальные подпространства:** Нулевое пространство $\{\mathbf{0}\}$ и само пространство $V$.
*   **Пространство непрерывных функций $C(\mathbb{R})$** является подпространством пространства всех функций $\text{Map}(\mathbb{R}, \mathbb{R})$.
*   **Пространство диагональных матриц** является подпространством пространства всех квадратных матриц $M_{n \times n}(F)$.
*   **Пространство решений однородной СЛАУ $A\mathbf{x} = \mathbf{0}$** является подпространством $F^N$.

#### 1.4. Факторпространство

**Определение:**
Пусть $V$ — векторное пространство над полем $F$, и $U$ — его подпространство. Введем на $V$ отношение эквивалентности: $v_1 \sim v_2 \pmod U$ (читается "$v_1$ эквивалентен $v_2$ по модулю $U$"), если $v_1 - v_2 \in U$.

**Свойства отношения эквивалентности:**
1.  **Рефлексивность:** $v \sim v$, так как $v-v = \mathbf{0} \in U$.
2.  **Симметричность:** Если $v_1 \sim v_2$, то $v_1-v_2 \in U$. Тогда $v_2-v_1 = -(v_1-v_2) \in U$, следовательно $v_2 \sim v_1$.
3.  **Транзитивность:** Если $v_1 \sim v_2$ и $v_2 \sim v_3$, то $v_1-v_2 \in U$ и $v_2-v_3 \in U$. Тогда $(v_1-v_2) + (v_2-v_3) = v_1-v_3 \in U$, следовательно $v_1 \sim v_3$.

Отношение эквивалентности разбивает $V$ на непересекающиеся классы эквивалентности. Класс эквивалентности вектора $v \in V$ обозначается как $v+U = \{v+u \mid u \in U\}$. Эти классы также называются **линейными многообразиями, параллельными $U$**.

**Факторпространство $V/U$** — это множество всех классов эквивалентности по модулю $U$. На $V/U$ вводятся операции сложения классов и умножения класса на скаляр:
*   Сложение: $(v_1+U) + (v_2+U) = (v_1+v_2)+U$
*   Умножение на скаляр: $\lambda(v+U) = (\lambda v)+U$

**Корректность операций:**
Операции определены корректно, то есть результат не зависит от выбора представителей классов. Например, если $v_1 \sim v_1'$ и $v_2 \sim v_2'$, то $v_1-v_1' \in U$ и $v_2-v_2' \in U$. Тогда $(v_1+v_2)-(v_1'+v_2') = (v_1-v_1')+(v_2-v_2') \in U$, что означает $(v_1+v_2)+U = (v_1'+v_2')+U$. Аналогично для умножения на скаляр.

**Свойства:**
*   Факторпространство $V/U$ само является векторным пространством над $F$.
*   **Размерность факторпространства:** Если $V$ конечномерно, то $\dim(V/U) = \dim(V) - \dim(U)$.
    *   **Доказательство:** Пусть $E_U = \{e_1, \dots, e_k\}$ — базис $U$. Дополним его до базиса $E_V = \{e_1, \dots, e_k, e_{k+1}, \dots, e_n\}$ пространства $V$. Тогда классы $\{e_{k+1}+U, \dots, e_n+U\}$ образуют базис факторпространства $V/U$. Их количество равно $n-k = \dim(V) - \dim(U)$.

---

### Билет 2. Гомоморфизмы векторных пространств: определение и примеры. Образ и ядро гомоморфизма. Теорема о полном прообразе гомоморфизма и следствие из неё для СЛАУ. Основная теорема о гомоморфизме векторных пространств. Теорема фон Дика.

#### 2.1. Гомоморфизмы векторных пространств: определение и примеры

**Определение:**
Пусть $U$ и $V$ — векторные пространства над одним и тем же полем $F$. Отображение $A: U \to V$ называется **гомоморфизмом векторных пространств** (или **линейным отображением**), если оно сохраняет операции сложения векторов и умножения на скаляр:
1.  **Аддитивность:** $A(u_1 + u_2) = A(u_1) + A(u_2)$ для всех $u_1, u_2 \in U$.
2.  **Однородность:** $A(\lambda u) = \lambda A(u)$ для всех $\lambda \in F$ и $u \in U$.

Эти два условия эквивалентны одному: $A(\lambda_1 u_1 + \lambda_2 u_2) = \lambda_1 A(u_1) + \lambda_2 A(u_2)$ для всех $\lambda_1, \lambda_2 \in F$ и $u_1, u_2 \in U$.

**Примеры гомоморфизмов:**
*   **Нулевое отображение:** $A(u) = \mathbf{0}_V$ для всех $u \in U$.
*   **Тождественное отображение:** $A(u) = u$ для всех $u \in U$ (если $U=V$).
*   **Умножение на скаляр:** $A(u) = \lambda u$ для фиксированного $\lambda \in F$.
*   **Матричное умножение:** Для матрицы $M_{m \times n}(F)$, отображение $A: F^n \to F^m$, заданное как $A(\mathbf{x}) = M\mathbf{x}$, является гомоморфизмом.
*   **Дифференцирование:** Отображение $D: P_n[x] \to P_{n-1}[x]$, заданное как $D(P(x)) = P'(x)$, является гомоморфизмом.
*   **Интегрирование:** Отображение $I: P_n[x] \to P_{n+1}[x]$, заданное как $I(P(x)) = \int_0^x P(t) dt$, является гомоморфизмом.
*   **Проекция:** Отображение $P: \mathbb{R}^3 \to \mathbb{R}^2$, заданное как $P(x,y,z) = (x,y)$, является гомоморфизмом.

#### 2.2. Образ и ядро гомоморфизма

**Определение ядра (Ker $A$):**
**Ядро гомоморфизма** $A: U \to V$ — это множество всех векторов из $U$, которые отображаются в нулевой вектор пространства $V$:
$\text{Ker}(A) = \{u \in U \mid A(u) = \mathbf{0}_V\}$.

**Свойства ядра:**
*   $\text{Ker}(A)$ является подпространством $U$.
    *   **Доказательство:**
        1.  $\mathbf{0}_U \in \text{Ker}(A)$, так как $A(\mathbf{0}_U) = A(0 \cdot u) = 0 \cdot A(u) = \mathbf{0}_V$. Значит, $\text{Ker}(A)$ непусто.
        2.  Если $u_1, u_2 \in \text{Ker}(A)$, то $A(u_1)=\mathbf{0}_V$ и $A(u_2)=\mathbf{0}_V$. Тогда $A(u_1+u_2) = A(u_1)+A(u_2) = \mathbf{0}_V+\mathbf{0}_V = \mathbf{0}_V$. Значит, $u_1+u_2 \in \text{Ker}(A)$.
        3.  Если $u \in \text{Ker}(A)$ и $\lambda \in F$, то $A(u)=\mathbf{0}_V$. Тогда $A(\lambda u) = \lambda A(u) = \lambda \mathbf{0}_V = \mathbf{0}_V$. Значит, $\lambda u \in \text{Ker}(A)$.
    По критерию подпространства, $\text{Ker}(A)$ является подпространством $U$.
*   Гомоморфизм $A$ является **инъективным** (мономорфизмом) тогда и только тогда, когда $\text{Ker}(A) = \{\mathbf{0}_U\}$.

**Определение образа (Im $A$):**
**Образ гомоморфизма** $A: U \to V$ — это множество всех векторов из $V$, которые являются образами каких-либо векторов из $U$:
$\text{Im}(A) = \{v \in V \mid \exists u \in U \text{ такой, что } A(u) = v\}$.

**Свойства образа:**
*   $\text{Im}(A)$ является подпространством $V$.
    *   **Доказательство:**
        1.  $\mathbf{0}_V \in \text{Im}(A)$, так как $A(\mathbf{0}_U) = \mathbf{0}_V$. Значит, $\text{Im}(A)$ непусто.
        2.  Если $v_1, v_2 \in \text{Im}(A)$, то существуют $u_1, u_2 \in U$ такие, что $A(u_1)=v_1$ и $A(u_2)=v_2$. Тогда $v_1+v_2 = A(u_1)+A(u_2) = A(u_1+u_2)$. Так как $u_1+u_2 \in U$, то $v_1+v_2 \in \text{Im}(A)$.
        3.  Если $v \in \text{Im}(A)$ и $\lambda \in F$, то существует $u \in U$ такой, что $A(u)=v$. Тогда $\lambda v = \lambda A(u) = A(\lambda u)$. Так как $\lambda u \in U$, то $\lambda v \in \text{Im}(A)$.
    По критерию подпространства, $\text{Im}(A)$ является подпространством $V$.
*   Гомоморфизм $A$ является **сюръективным** (эпиморфизмом) тогда и только тогда, когда $\text{Im}(A) = V$.

#### 2.3. Теорема о полном прообразе гомоморфизма и следствие из неё для СЛАУ

**Теорема о полном прообразе:**
Пусть $A: U \to V$ — гомоморфизм векторных пространств. Если $v_0 \in \text{Im}(A)$ — некоторый фиксированный вектор, то полный прообраз $A^{-1}(v_0) = \{u \in U \mid A(u) = v_0\}$ является линейным многообразием вида $u_0 + \text{Ker}(A)$, где $u_0$ — любое частное решение уравнения $A(u) = v_0$.

**Доказательство:**
1.  **Покажем, что $u_0 + \text{Ker}(A) \subseteq A^{-1}(v_0)$:**
    Пусть $u \in u_0 + \text{Ker}(A)$. Тогда $u = u_0 + w$ для некоторого $w \in \text{Ker}(A)$.
    Применим $A$ к $u$: $A(u) = A(u_0+w) = A(u_0) + A(w)$.
    Так как $u_0$ — частное решение, $A(u_0) = v_0$. Так как $w \in \text{Ker}(A)$, $A(w) = \mathbf{0}_V$.
    Следовательно, $A(u) = v_0 + \mathbf{0}_V = v_0$. Значит, $u \in A^{-1}(v_0)$.
2.  **Покажем, что $A^{-1}(v_0) \subseteq u_0 + \text{Ker}(A)$:**
    Пусть $u \in A^{-1}(v_0)$. Тогда $A(u) = v_0$.
    Рассмотрим вектор $u - u_0$. Применим $A$ к нему: $A(u-u_0) = A(u) - A(u_0) = v_0 - v_0 = \mathbf{0}_V$.
    Следовательно, $u-u_0 \in \text{Ker}(A)$. Обозначим $w = u-u_0$. Тогда $u = u_0 + w$, где $w \in \text{Ker}(A)$.
    Значит, $u \in u_0 + \text{Ker}(A)$.

**Следствие для СЛАУ:**
Рассмотрим систему линейных алгебраических уравнений (СЛАУ) $A\mathbf{x} = \mathbf{b}$, где $A$ — матрица коэффициентов, $\mathbf{x}$ — столбец неизвестных, $\mathbf{b}$ — столбец свободных членов. Эту систему можно рассматривать как уравнение $A(\mathbf{x}) = \mathbf{b}$, где $A$ — гомоморфизм $F^n \to F^m$, заданный умножением на матрицу $A$.
*   Если СЛАУ совместна (т.е. $\mathbf{b} \in \text{Im}(A)$), и $\mathbf{x}_0$ — некоторое частное решение, то **общее решение СЛАУ** имеет вид $\mathbf{x} = \mathbf{x}_0 + \mathbf{x}_h$, где $\mathbf{x}_h$ — произвольное решение соответствующей однородной СЛАУ $A\mathbf{x}_h = \mathbf{0}$.
*   Множество решений однородной СЛАУ $A\mathbf{x}_h = \mathbf{0}$ является в точности ядром гомоморфизма $A$, то есть $\text{Ker}(A)$.
*   Таким образом, множество всех решений неоднородной СЛАУ $A\mathbf{x} = \mathbf{b}$ является линейным многообразием $\mathbf{x}_0 + \text{Ker}(A)$.

#### 2.4. Основная теорема о гомоморфизме векторных пространств

**Формулировка:**
Пусть $A: U \to V$ — гомоморфизм векторных пространств. Тогда факторпространство $U/\text{Ker}(A)$ изоморфно образу гомоморфизма $\text{Im}(A)$.
$$U/\text{Ker}(A) \cong \text{Im}(A)$$

**Доказательство:**
Построим отображение $\bar{A}: U/\text{Ker}(A) \to \text{Im}(A)$ следующим образом:
Для любого класса эквивалентности $u+\text{Ker}(A) \in U/\text{Ker}(A)$, определим $\bar{A}(u+\text{Ker}(A)) = A(u)$.

1.  **Корректность определения $\bar{A}$:**
    Пусть $u_1+\text{Ker}(A) = u_2+\text{Ker}(A)$. Это означает, что $u_1-u_2 \in \text{Ker}(A)$.
    Тогда $A(u_1-u_2) = \mathbf{0}_V$, что влечет $A(u_1) - A(u_2) = \mathbf{0}_V$, или $A(u_1) = A(u_2)$.
    Следовательно, $\bar{A}(u_1+\text{Ker}(A)) = \bar{A}(u_2+\text{Ker}(A))$, и отображение $\bar{A}$ определено корректно.

2.  **$\bar{A}$ является гомоморфизмом:**
    *   Аддитивность: $\bar{A}((u_1+\text{Ker}(A)) + (u_2+\text{Ker}(A))) = \bar{A}((u_1+u_2)+\text{Ker}(A)) = A(u_1+u_2) = A(u_1)+A(u_2) = \bar{A}(u_1+\text{Ker}(A)) + \bar{A}(u_2+\text{Ker}(A))$.
    *   Однородность: $\bar{A}(\lambda(u+\text{Ker}(A))) = \bar{A}((\lambda u)+\text{Ker}(A)) = A(\lambda u) = \lambda A(u) = \lambda \bar{A}(u+\text{Ker}(A))$.

3.  **$\bar{A}$ является сюръективным:**
    Для любого $v \in \text{Im}(A)$ существует $u \in U$ такой, что $A(u)=v$. Тогда $\bar{A}(u+\text{Ker}(A)) = A(u) = v$. Значит, $\bar{A}$ сюръективно на $\text{Im}(A)$.

4.  **$\bar{A}$ является инъективным:**
    Пусть $\bar{A}(u+\text{Ker}(A)) = \mathbf{0}_V$. Это означает $A(u) = \mathbf{0}_V$.
    По определению ядра, $u \in \text{Ker}(A)$.
    Следовательно, класс $u+\text{Ker}(A)$ является нулевым классом в $U/\text{Ker}(A)$ (т.е. $\text{Ker}(A)$).
    Значит, $\text{Ker}(\bar{A}) = \{\text{Ker}(A)\}$, что является нулевым элементом факторпространства. Таким образом, $\bar{A}$ инъективно.

Поскольку $\bar{A}$ является биективным гомоморфизмом, оно является изоморфизмом.

**Следствие (Теорема о ранге и дефекте):**
Для конечномерного пространства $U$, из основной теоремы о гомоморфизме и формулы для размерности факторпространства следует:
$\dim(U/\text{Ker}(A)) = \dim(\text{Im}(A))$
$\dim(U) - \dim(\text{Ker}(A)) = \dim(\text{Im}(A))$
$\dim(U) = \dim(\text{Ker}(A)) + \dim(\text{Im}(A))$
Это утверждение известно как **теорема о размерности ядра и образа** (или теорема о ранге и дефекте), где $\dim(\text{Im}(A))$ называется рангом гомоморфизма, а $\dim(\text{Ker}(A))$ — его дефектом.

#### 2.5. Теорема фон Дика

**Теорема (Третья теорема об изоморфизме для векторных пространств):**
Пусть $U, V, W$ — векторные пространства над одним и тем же полем $F$, такие что $U$ является подпространством $V$, а $V$ является подпространством $W$ (т.е. $U \subseteq V \subseteq W$). Тогда факторпространство $(W/U)/(V/U)$ изоморфно факторпространству $W/V$.
$$(W/U)/(V/U) \cong W/V$$

**Доказательство:**

1.  **Убедимся, что $V/U$ является подпространством $W/U$.**
    *   Элементы $W/U$ — это смежные классы вида $w+U$, где $w \in W$.
    *   Элементы $V/U$ — это смежные классы вида $v+U$, где $v \in V$.
    *   Поскольку $V \subseteq W$, любой вектор $v \in V$ также является вектором из $W$. Следовательно, любой класс $v+U \in V/U$ также является классом $w+U \in W/U$ (где $w=v$). Таким образом, $V/U \subseteq W/U$.
    *   $V/U$ непусто, так как содержит нулевой класс $U$.
    *   Замкнутость относительно сложения: $(v_1+U) + (v_2+U) = (v_1+v_2)+U$. Поскольку $v_1, v_2 \in V$, то $v_1+v_2 \in V$, и, следовательно, $(v_1+v_2)+U \in V/U$.
    *   Замкнутость относительно умножения на скаляр: $\lambda(v+U) = (\lambda v)+U$. Поскольку $v \in V$, то $\lambda v \in V$, и, следовательно, $(\lambda v)+U \in V/U$.
    Таким образом, $V/U$ является подпространством $W/U$, и мы можем рассмотреть факторпространство $(W/U)/(V/U)$.

2.  **Построим гомоморфизм $\phi: W/U \to W/V$.**
    Определим отображение $\phi$ следующим образом:
    Для любого класса $w+U \in W/U$, пусть $\phi(w+U) = w+V$.

3.  **Проверим корректность определения $\phi$.**
    Предположим, что $w_1+U = w_2+U$ для некоторых $w_1, w_2 \in W$.
    Это означает, что $w_1-w_2 \in U$.
    Поскольку $U \subseteq V$, то $w_1-w_2 \in V$.
    Из этого следует, что $w_1+V = w_2+V$.
    Таким образом, $\phi(w_1+U) = \phi(w_2+U)$, и отображение $\phi$ определено корректно (не зависит от выбора представителя класса).

4.  **Проверим, что $\phi$ является гомоморфизмом векторных пространств.**
    *   **Аддитивность:**
        $\phi((w_1+U) + (w_2+U)) = \phi((w_1+w_2)+U)$
        $= (w_1+w_2)+V$
        $= (w_1+V) + (w_2+V)$
        $= \phi(w_1+U) + \phi(w_2+U)$.
    *   **Однородность:**
        $\phi(\lambda(w+U)) = \phi((\lambda w)+U)$
        $= (\lambda w)+V$
        $= \lambda(w+V)$
        $= \lambda \phi(w+U)$.
    Следовательно, $\phi$ является гомоморфизмом.

5.  **Проверим, что $\phi$ является сюръективным.**
    Пусть $x \in W/V$ — произвольный элемент. По определению факторпространства, $x = w+V$ для некоторого $w \in W$.
    Рассмотрим класс $w+U \in W/U$. Тогда $\phi(w+U) = w+V = x$.
    Таким образом, для любого элемента в $W/V$ существует его прообраз в $W/U$, что означает сюръективность $\phi$.
    Следовательно, $\text{Im}(\phi) = W/V$.

6.  **Найдем ядро гомоморфизма $\phi$.**
    Ядро $\text{Ker}(\phi)$ — это множество всех классов $w+U \in W/U$, которые отображаются в нулевой элемент $W/V$ (которым является класс $V$).
    $\text{Ker}(\phi) = \{w+U \in W/U \mid \phi(w+U) = V\}$
    $\phi(w+U) = V \iff w+V = V \iff w \in V$.
    Таким образом, $\text{Ker}(\phi) = \{w+U \in W/U \mid w \in V\}$.
    Это множество в точности совпадает с подпространством $V/U$.

7.  **Применим Основную теорему о гомоморфизме векторных пространств.**
    Основная теорема о гомоморфизме утверждает, что для любого гомоморфизма $\phi: A \to B$, факторпространство $A/\text{Ker}(\phi)$ изоморфно образу $\text{Im}(\phi)$.
    В нашем случае:
    *   $A = W/U$
    *   $B = W/V$
    *   $\text{Ker}(\phi) = V/U$
    *   $\text{Im}(\phi) = W/V$
    Подставляя эти значения, получаем:
    $$(W/U) / (V/U) \cong W/V$$

---

### Билет 3. Линейные комбинации и оболочки. Линейная (не)зависимость системы векторов. Свойства линейной (не)зависимости. Порождающая система векторов. Основная лемма о линейной зависимости. Связь между линейной (не)зависимостью системы строк/столбцов квадратной матрицы и её (не)вырожденностью.

#### 3.1. Линейные комбинации и оболочки

**Определение линейной комбинации:**
Пусть $V$ — векторное пространство над полем $F$, и $v_1, v_2, \dots, v_k$ — векторы из $V$. **Линейной комбинацией** этих векторов называется любой вектор вида:
$v = \lambda_1 v_1 + \lambda_2 v_2 + \dots + \lambda_k v_k$,
где $\lambda_1, \lambda_2, \dots, \lambda_k$ — скаляры из поля $F$.

**Определение линейной оболочки (Span):**
**Линейной оболочкой** множества векторов $S = \{v_1, v_2, \dots, v_k\}$ (или любого подмножества $S \subseteq V$) называется множество всех возможных линейных комбинаций векторов из $S$. Обозначается $\text{Span}(S)$ или $\langle S \rangle$.
$\text{Span}(S) = \{\lambda_1 v_1 + \dots + \lambda_k v_k \mid \lambda_i \in F\}$.

**Свойства линейной оболочки:**
*   Линейная оболочка любого непустого множества векторов $S \subseteq V$ является подпространством $V$.
*   $\text{Span}(S)$ является наименьшим подпространством $V$, содержащим $S$.

#### 3.2. Линейная (не)зависимость системы векторов

**Определение линейной зависимости:**
Система векторов $v_1, v_2, \dots, v_k$ из векторного пространства $V$ называется **линейно зависимой**, если существует нетривиальная линейная комбинация этих векторов, равная нулевому вектору. То есть, существуют скаляры $\lambda_1, \lambda_2, \dots, \lambda_k \in F$, не все равные нулю, такие что:
$\lambda_1 v_1 + \lambda_2 v_2 + \dots + \lambda_k v_k = \mathbf{0}$.

**Определение линейной независимости:**
Система векторов $v_1, v_2, \dots, v_k$ называется **линейно независимой**, если единственная линейная комбинация этих векторов, равная нулевому вектору, является тривиальной. То есть, если $\lambda_1 v_1 + \lambda_2 v_2 + \dots + \lambda_k v_k = \mathbf{0}$, то обязательно $\lambda_1 = \lambda_2 = \dots = \lambda_k = 0$.

**Свойства линейной (не)зависимости:**
*   Если система векторов содержит нулевой вектор, то она линейно зависима. (Например, $1 \cdot \mathbf{0} = \mathbf{0}$).
*   Если система векторов содержит два одинаковых вектора, то она линейно зависима. (Например, $1 \cdot v_i + (-1) \cdot v_j = \mathbf{0}$ при $v_i=v_j$).
*   Если часть линейно независимой системы линейно зависима, то вся система линейно зависима. (Контрапозиция: любая подсистема линейно независимой системы также линейно независима).
*   Система векторов линейно зависима тогда и только тогда, когда хотя бы один из векторов может быть выражен как линейная комбинация остальных.
    *   **Доказательство:**
        *   **($\implies$)** Пусть $\lambda_1 v_1 + \dots + \lambda_k v_k = \mathbf{0}$ — нетривиальная линейная комбинация, где, например, $\lambda_j \neq 0$. Тогда $v_j = -\frac{\lambda_1}{\lambda_j} v_1 - \dots - \frac{\lambda_{j-1}}{\lambda_j} v_{j-1} - \frac{\lambda_{j+1}}{\lambda_j} v_{j+1} - \dots - \frac{\lambda_k}{\lambda_j} v_k$.
        *   **($\impliedby$)** Пусть $v_j = \sum_{i \neq j} \alpha_i v_i$. Тогда $\sum_{i \neq j} \alpha_i v_i - 1 \cdot v_j = \mathbf{0}$. Это нетривиальная линейная комбинация, так как коэффициент при $v_j$ равен $-1 \neq 0$.

#### 3.3. Порождающая система векторов

**Определение:**
Система векторов $S = \{v_1, v_2, \dots, v_k\}$ называется **порождающей системой** (или остовом) векторного пространства $V$, если $\text{Span}(S) = V$. То есть, любой вектор из $V$ может быть представлен как линейная комбинация векторов из $S$.

**Конечномерное пространство:**
Векторное пространство $V$ называется **конечномерным**, если оно обладает конечной порождающей системой. В противном случае оно называется бесконечномерным.

#### 3.4. Основная лемма о линейной зависимости

**Формулировка:**
Пусть $V$ — векторное пространство. Если система векторов $A = \{a_1, \dots, a_n\}$ порождает $V$, то любая система векторов $B = \{b_1, \dots, b_m\}$ из $V$ с $m > n$ является линейно зависимой.

**Доказательство:**
Поскольку система $A$ порождает $V$, каждый вектор $b_j \in B$ может быть выражен как линейная комбинация векторов из $A$:
$b_j = \sum_{i=1}^n \alpha_{ij} a_i$ для $j=1, \dots, m$.

Рассмотрим произвольную линейную комбинацию векторов из $B$, равную нулю:
$\sum_{j=1}^m \lambda_j b_j = \mathbf{0}$.

Подставим выражения для $b_j$:
$\sum_{j=1}^m \lambda_j \left( \sum_{i=1}^n \alpha_{ij} a_i \right) = \mathbf{0}$.

Изменим порядок суммирования:
$\sum_{i=1}^n \left( \sum_{j=1}^m \alpha_{ij} \lambda_j \right) a_i = \mathbf{0}$.

Чтобы эта линейная комбинация была нулевой, коэффициенты при $a_i$ должны быть равны нулю (если $A$ линейно независима, что не обязательно, но мы можем использовать это для построения системы уравнений).
Пусть $c_i = \sum_{j=1}^m \alpha_{ij} \lambda_j$. Мы ищем нетривиальное решение $\lambda_j$ для системы $c_i=0$.
Это приводит к однородной системе линейных алгебраических уравнений относительно $\lambda_1, \dots, \lambda_m$:
$\begin{cases}
\alpha_{11}\lambda_1 + \alpha_{12}\lambda_2 + \dots + \alpha_{1m}\lambda_m = 0 \\
\alpha_{21}\lambda_1 + \alpha_{22}\lambda_2 + \dots + \alpha_{2m}\lambda_m = 0 \\
\dots \\
\alpha_{n1}\lambda_1 + \alpha_{n2}\lambda_2 + \dots + \alpha_{nm}\lambda_m = 0
\end{cases}$

Эта система имеет $n$ уравнений и $m$ неизвестных. Поскольку $m > n$ (количество неизвестных больше количества уравнений), эта однородная СЛАУ **гарантированно имеет ненулевое решение** для $\lambda_1, \dots, \lambda_m$.
Это ненулевое решение $(\lambda_1, \dots, \lambda_m)$ дает нетривиальную линейную комбинацию векторов $b_j$, равную нулю, что по определению означает, что система $B$ линейно зависима.

#### 3.5. Связь между линейной (не)зависимостью системы строк/столбцов квадратной матрицы и её (не)вырожденностью

Пусть $A$ — квадратная матрица размера $n \times n$ над полем $F$.

**Теорема:** Следующие утверждения эквивалентны:
1.  Матрица $A$ **невырождена** (т.е. $\det(A) \neq 0$).
2.  Система столбцов матрицы $A$ **линейно независима**.
3.  Система строк матрицы $A$ **линейно независима**.
4.  Матрица $A$ **обратима** (т.е. существует $A^{-1}$).
5.  Ранг матрицы $A$ равен $n$ (т.е. $\text{rank}(A) = n$).
6.  Однородная СЛАУ $A\mathbf{x} = \mathbf{0}$ имеет только тривиальное решение $\mathbf{x} = \mathbf{0}$.
7.  Для любой правой части $\mathbf{b}$, СЛАУ $A\mathbf{x} = \mathbf{b}$ имеет единственное решение.

**Доказательство (ключевые связи):**
*   **($1 \iff 2$):** Определитель матрицы равен нулю тогда и только тогда, когда её столбцы линейно зависимы. Если столбцы линейно независимы, то $\det(A) \neq 0$.
    *   **Доказательство:** Столбцы матрицы $A$ линейно независимы тогда и только тогда, когда гомоморфизм $A: F^n \to F^n$ (умножение на $A$) инъективен. Для конечномерных пространств одинаковой размерности инъективность эквивалентна сюръективности, что эквивалентно биективности (изоморфизму). Изоморфизм эквивалентен невырожденности матрицы.
*   **($1 \iff 3$):** Аналогично для строк. Это следует из того, что $\det(A) = \det(A^T)$, и столбцы $A^T$ — это строки $A$.
*   **($1 \iff 4$):** Матрица обратима тогда и только тогда, когда её определитель не равен нулю.
*   **($2 \iff 5$):** Ранг матрицы определяется как размерность линейной оболочки её столбцов (столбцовый ранг). Если столбцы линейно независимы, то их $n$ штук, и они образуют базис в $F^n$, поэтому размерность их линейной оболочки равна $n$.
*   **($2 \iff 6$):** Однородная СЛАУ $A\mathbf{x} = \mathbf{0}$ имеет только тривиальное решение тогда и только тогда, когда столбцы матрицы $A$ линейно независимы.
    *   **Доказательство:** Уравнение $A\mathbf{x} = \mathbf{0}$ можно записать как $x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \dots + x_n \mathbf{a}_n = \mathbf{0}$, где $\mathbf{a}_i$ — столбцы матрицы $A$. Если столбцы линейно независимы, то единственное решение этой линейной комбинации — $x_1=\dots=x_n=0$. Если столбцы линейно зависимы, то существует нетривиальное решение.

Таким образом, для квадратной матрицы линейная независимость её строк (или столбцов) является прямым индикатором её невырожденности и обратимости.

---

### Билет 4. Базис и размерность векторного пространства. Существование базиса в любом конечномерном пространстве. Теоремы о дополнении линейно независимой системы до базиса и о монотонности размерности.

#### 4.1. Базис и размерность векторного пространства

**Определение базиса (Определение 1):**
Система векторов $E = \{e_1, e_2, \dots, e_n\}$ из векторного пространства $V$ называется **базисом** $V$, если любой вектор $v \in V$ может быть **единственным образом** представлен в виде линейной комбинации векторов из $E$:
$v = \lambda_1 e_1 + \lambda_2 e_2 + \dots + \lambda_n e_n$, где $\lambda_i \in F$.
Коэффициенты $\lambda_1, \dots, \lambda_n$ называются **координатами** вектора $v$ в базисе $E$.

**Определение базиса (Определение 2, эквивалентное):**
Система векторов $E = \{e_1, e_2, \dots, e_n\}$ называется **базисом** векторного пространства $V$, если она одновременно:
1.  **Линейно независима**.
2.  **Порождает пространство $V$** (т.е. $\text{Span}(E) = V$).

**Доказательство эквивалентности определений:**
*   **($\text{Определение 1} \implies \text{Определение 2}$):**
    1.  **Порождает:** По определению 1, любой вектор $v \in V$ выражается через $E$, значит $E$ порождает $V$.
    2.  **Линейно независима:** Предположим, $\lambda_1 e_1 + \dots + \lambda_n e_n = \mathbf{0}$. Мы также знаем, что $\mathbf{0} = 0 \cdot e_1 + \dots + 0 \cdot e_n$. Из единственности разложения (по определению 1) следует, что $\lambda_1 = \dots = \lambda_n = 0$. Значит, $E$ линейно независима.
*   **($\text{Определение 2} \implies \text{Определение 1}$):**
    1.  **Существование разложения:** Так как $E$ порождает $V$, любой вектор $v \in V$ может быть представлен как линейная комбинация векторов из $E$.
    2.  **Единственность разложения:** Предположим, что вектор $v$ имеет два разложения:
        $v = \lambda_1 e_1 + \dots + \lambda_n e_n$
        $v = \mu_1 e_1 + \dots + \mu_n e_n$
        Вычтем одно из другого: $\mathbf{0} = (\lambda_1-\mu_1)e_1 + \dots + (\lambda_n-\mu_n)e_n$.
        Так как $E$ линейно независима, все коэффициенты должны быть равны нулю: $\lambda_i-\mu_i = 0 \implies \lambda_i = \mu_i$ для всех $i$. Значит, разложение единственно.

**Теорема о числе векторов в базисе:**
Все базисы любого конечномерного векторного пространства содержат одно и то же число векторов.

**Доказательство:**
Пусть $E = \{e_1, \dots, e_n\}$ и $F = \{f_1, \dots, f_m\}$ — два базиса пространства $V$.
1.  Так как $E$ является базисом, оно порождает $V$.
2.  Так как $F$ является базисом, оно линейно независимо.
3.  По основной лемме о линейной зависимости, если порождающая система $E$ имеет $n$ векторов, а линейно независимая система $F$ имеет $m$ векторов, то $m \le n$.

Аналогично, поменяв ролями $E$ и $F$:
1.  Так как $F$ является базисом, оно порождает $V$.
2.  Так как $E$ является базисом, оно линейно независимо.
3.  По основной лемме о линейной зависимости, если порождающая система $F$ имеет $m$ векторов, а линейно независимая система $E$ имеет $n$ векторов, то $n \le m$.

Из $m \le n$ и $n \le m$ следует, что $n=m$. Таким образом, все базисы содержат одинаковое число векторов.

**Определение размерности:**
**Размерностью** конечномерного векторного пространства $V$ (обозначается $\dim(V)$) называется число векторов в любом его базисе.

#### 4.2. Существование базиса в любом конечномерном пространстве

**Теорема:** Любое конечномерное векторное пространство обладает базисом.

**Доказательство:**
Пусть $V$ — конечномерное векторное пространство. По определению, оно порождается конечной системой векторов $S = \{v_1, \dots, v_k\}$.
Если $S$ линейно независимо, то $S$ само является базисом.
Если $S$ линейно зависимо, то по свойству линейной зависимости, хотя бы один вектор из $S$ может быть выражен как линейная комбинация остальных. Пусть это будет $v_j$. Тогда система $S' = S \setminus \{v_j\}$ также порождает $V$ (поскольку $v_j$ "лишний").
Мы можем повторять этот процесс: если $S'$ линейно зависима, удаляем из неё вектор, который является линейной комбинацией остальных, получая $S''$. Поскольку исходная система $S$ конечна, этот процесс удаления векторов должен завершиться. Он завершится, когда оставшаяся система станет линейно независимой. Эта конечная линейно независимая система, которая по-прежнему порождает $V$, и будет базисом $V$.

#### 4.3. Теоремы о дополнении линейно независимой системы до базиса и о монотонности размерности

**Теорема о дополнении линейно независимой системы до базиса:**
Пусть $V$ — конечномерное векторное пространство размерности $n$. Любая линейно независимая система векторов $S = \{v_1, \dots, v_k\}$ в $V$ может быть дополнена до базиса $V$. То есть, существуют векторы $v_{k+1}, \dots, v_n \in V$ такие, что система $\{v_1, \dots, v_k, v_{k+1}, \dots, v_n\}$ является базисом $V$.

**Доказательство:**
Пусть $S = \{v_1, \dots, v_k\}$ — линейно независимая система в $V$.
Если $\text{Span}(S) = V$, то $S$ уже является базисом (поскольку оно линейно независимо и порождает $V$). В этом случае $k=n$, и дополнение не требуется.
Если $\text{Span}(S) \neq V$, то существует вектор $v_{k+1} \in V$ такой, что $v_{k+1} \notin \text{Span}(S)$.
Тогда система $S_1 = \{v_1, \dots, v_k, v_{k+1}\}$ будет линейно независимой.
*   **Доказательство линейной независимости $S_1$:** Предположим, $\lambda_1 v_1 + \dots + \lambda_k v_k + \lambda_{k+1} v_{k+1} = \mathbf{0}$.
    Если $\lambda_{k+1} \neq 0$, то $v_{k+1} = -\frac{\lambda_1}{\lambda_{k+1}} v_1 - \dots - \frac{\lambda_k}{\lambda_{k+1}} v_k$, что означает $v_{k+1} \in \text{Span}(S)$, противоречие.
    Следовательно, $\lambda_{k+1}$ должно быть равно $0$.
    Тогда $\lambda_1 v_1 + \dots + \lambda_k v_k = \mathbf{0}$. Поскольку $S$ линейно независима, все $\lambda_1, \dots, \lambda_k$ также должны быть равны $0$.
    Таким образом, все коэффициенты в линейной комбинации равны нулю, и $S_1$ линейно независима.

Мы можем повторять этот процесс, добавляя по одному вектору, который не лежит в линейной оболочке предыдущих, до тех пор, пока полученная система не будет порождать $V$. Этот процесс должен завершиться, так как по основной лемме о линейной зависимости, любая линейно независимая система в $V$ не может содержать более $n = \dim(V)$ векторов. Когда система достигнет $n$ векторов, она будет линейно независимой и будет порождать $V$, то есть станет базисом.

**Теорема о монотонности размерности:**
Пусть $V$ — конечномерное векторное пространство, и $U$ — его подпространство. Тогда:
1.  $\dim(U) \le \dim(V)$.
2.  Если $U \neq V$, то $\dim(U) < \dim(V)$.

**Доказательство:**
1.  Пусть $B_U = \{u_1, \dots, u_k\}$ — базис подпространства $U$. По определению, $B_U$ является линейно независимой системой векторов в $U$, а значит, и в $V$. По теореме о дополнении линейно независимой системы до базиса, $B_U$ может быть дополнена до базиса $V$. Число векторов в базисе $V$ равно $\dim(V)$. Следовательно, $k \le \dim(V)$, то есть $\dim(U) \le \dim(V)$.
2.  Если $U = V$, то $\dim(U) = \dim(V)$. Если $U \neq V$, то существует хотя бы один вектор $v \in V$ такой, что $v \notin U$. Тогда система $B_U \cup \{v\}$ будет линейно независимой в $V$ и будет содержать $k+1$ векторов. Следовательно, $k+1 \le \dim(V)$, что означает $k < \dim(V)$, то есть $\dim(U) < \dim(V)$.

---

### Билет 5. Переход к новому базису. Матрица перехода и её свойства. Изменение координат вектора при изменении базиса.

#### 5.1. Матрица перехода и её свойства

Пусть $V$ — конечномерное векторное пространство над полем $F$ размерности $n$.
Пусть $E = \{e_1, e_2, \dots, e_n\}$ — "старый" базис $V$.
Пусть $\tilde{E} = \{\tilde{e}_1, \tilde{e}_2, \dots, \tilde{e}_n\}$ — "новый" базис $V$.

Каждый вектор нового базиса $\tilde{e}_j$ может быть единственным образом выражен как линейная комбинация векторов старого базиса $E$:
$\tilde{e}_j = c_{1j} e_1 + c_{2j} e_2 + \dots + c_{nj} e_n = \sum_{i=1}^n c_{ij} e_i$.

**Определение матрицы перехода:**
**Матрица перехода** $C_{E \to \tilde{E}}$ (или просто $C$) от базиса $E$ к базису $\tilde{E}$ — это матрица, столбцами которой являются координаты векторов нового базиса $\tilde{E}$ в старом базисе $E$.
$$C = \begin{pmatrix}
c_{11} & c_{12} & \dots & c_{1n} \\
c_{21} & c_{22} & \dots & c_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
c_{n1} & c_{n2} & \dots & c_{nn}
\end{pmatrix}$$
Векторно это можно записать как: $\tilde{E} = E \cdot C$, где $E$ и $\tilde{E}$ здесь рассматриваются как строки векторов.

**Свойства матрицы перехода:**
1.  **Квадратность:** Матрица перехода $C$ всегда является квадратной матрицей размера $n \times n$, где $n = \dim(V)$.
2.  **Невырожденность (обратимость):** Матрица перехода $C$ всегда невырождена (т.е. $\det(C) \neq 0$) и, следовательно, обратима. Это следует из того, что столбцы $C$ являются координатами линейно независимых векторов (векторов нового базиса) в старом базисе.
3.  **Переход от базиса к себе:** Матрица перехода от базиса $E$ к самому себе $E$ является единичной матрицей: $C_{E \to E} = I$.
4.  **Произведение матриц перехода:** Если есть три базиса $E, \tilde{E}, \tilde{\tilde{E}}$, то матрица перехода от $E$ к $\tilde{\tilde{E}}$ равна произведению матриц перехода от $E$ к $\tilde{E}$ и от $\tilde{E}$ к $\tilde{\tilde{E}}$:
    $C_{E \to \tilde{\tilde{E}}} = C_{E \to \tilde{E}} \cdot C_{\tilde{E} \to \tilde{\tilde{E}}}$.
    *   **Доказательство:** Пусть $\tilde{E} = E C_{E \to \tilde{E}}$ и $\tilde{\tilde{E}} = \tilde{E} C_{\tilde{E} \to \tilde{\tilde{E}}}$.
        Тогда $\tilde{\tilde{E}} = (E C_{E \to \tilde{E}}) C_{\tilde{E} \to \tilde{\tilde{E}}} = E (C_{E \to \tilde{E}} C_{\tilde{E} \to \tilde{\tilde{E}}})$.
        По определению, $C_{E \to \tilde{\tilde{E}}}$ — это матрица, которая связывает $E$ и $\tilde{\tilde{E}}$.
5.  **Обратная матрица перехода:** Обратная матрица к матрице перехода $C_{E \to \tilde{E}}$ является матрицей перехода в обратном направлении:
    $(C_{E \to \tilde{E}})^{-1} = C_{\tilde{E} \to E}$.
    *   **Доказательство:** Из $\tilde{E} = E C_{E \to \tilde{E}}$ умножим справа на $(C_{E \to \tilde{E}})^{-1}$:
        $\tilde{E} (C_{E \to \tilde{E}})^{-1} = E$.
        По определению, матрица, которая связывает $\tilde{E}$ и $E$, это $C_{\tilde{E} \to E}$.

#### 5.2. Изменение координат вектора при изменении базиса

Пусть $v \in V$ — произвольный вектор.
Пусть $X_E = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ — столбец координат вектора $v$ в старом базисе $E$.
Пусть $X_{\tilde{E}} = \begin{pmatrix} \tilde{x}_1 \\ \vdots \\ \tilde{x}_n \end{pmatrix}$ — столбец координат вектора $v$ в новом базисе $\tilde{E}$.

По определению координат:
$v = x_1 e_1 + \dots + x_n e_n = E \cdot X_E$ (где $E$ — строка векторов).
$v = \tilde{x}_1 \tilde{e}_1 + \dots + \tilde{x}_n \tilde{e}_n = \tilde{E} \cdot X_{\tilde{E}}$ (где $\tilde{E}$ — строка векторов).

Мы знаем, что $\tilde{E} = E \cdot C_{E \to \tilde{E}}$. Подставим это во второе выражение для $v$:
$v = (E \cdot C_{E \to \tilde{E}}) \cdot X_{\tilde{E}} = E \cdot (C_{E \to \tilde{E}} \cdot X_{\tilde{E}})$.

Сравнивая это с $v = E \cdot X_E$, и учитывая единственность разложения вектора по базису $E$, получаем:
$X_E = C_{E \to \tilde{E}} \cdot X_{\tilde{E}}$.

Эта формула показывает, как получить старые координаты, зная новые и матрицу перехода от старого к новому базису.

Чтобы выразить новые координаты через старые, умножим обе части на $(C_{E \to \tilde{E}})^{-1}$ слева:
$X_{\tilde{E}} = (C_{E \to \tilde{E}})^{-1} \cdot X_E$.
Используя свойство обратной матрицы перехода:
$X_{\tilde{E}} = C_{\tilde{E} \to E} \cdot X_E$.

**Вывод:** Координаты вектора преобразуются с помощью **обратной** матрицы перехода (или матрицы перехода в обратном направлении). Это свойство называется **контравариантностью** координат.

---

### Билет 6. Теоремы о размерностях внешней прямой суммы и факторпространства. Теорема о размерности ядра и образа.

#### 6.1. Теорема о размерности внешней прямой суммы

**Формулировка:**
Пусть $U$ и $V$ — конечномерные векторные пространства над одним и тем же полем $F$. Тогда размерность их внешней прямой суммы $U \oplus V$ равна сумме их размерностей:
$\dim(U \oplus V) = \dim(U) + \dim(V)$.

**Доказательство:**
Пусть $\dim(U) = n$ и $\dim(V) = m$.
Пусть $E_U = \{e_1, \dots, e_n\}$ — базис пространства $U$.
Пусть $E_V = \{f_1, \dots, f_m\}$ — базис пространства $V$.

Рассмотрим следующую систему векторов в $U \oplus V$:
$B = \{(e_1, \mathbf{0}_V), \dots, (e_n, \mathbf{0}_V), (\mathbf{0}_U, f_1), \dots, (\mathbf{0}_U, f_m)\}$.
Эта система состоит из $n+m$ векторов. Докажем, что она является базисом $U \oplus V$.

1.  **Линейная независимость:**
    Предположим, что существует линейная комбинация этих векторов, равная нулевому вектору $(\mathbf{0}_U, \mathbf{0}_V)$:
    $\sum_{i=1}^n \lambda_i (e_i, \mathbf{0}_V) + \sum_{j=1}^m \mu_j (\mathbf{0}_U, f_j) = (\mathbf{0}_U, \mathbf{0}_V)$.
    Используя определения операций в прямой сумме, получаем:
    $(\sum_{i=1}^n \lambda_i e_i, \sum_{j=1}^m \mu_j f_j) = (\mathbf{0}_U, \mathbf{0}_V)$.
    Это означает, что $\sum_{i=1}^n \lambda_i e_i = \mathbf{0}_U$ и $\sum_{j=1}^m \mu_j f_j = \mathbf{0}_V$.
    Поскольку $E_U$ — базис $U$, он линейно независим, следовательно, все $\lambda_i = 0$.
    Поскольку $E_V$ — базис $V$, он линейно независим, следовательно, все $\mu_j = 0$.
    Таким образом, все коэффициенты в линейной комбинации равны нулю, и система $B$ линейно независима.

2.  **Порождает $U \oplus V$:**
    Пусть $(u, v)$ — произвольный вектор из $U \oplus V$.
    Так как $E_U$ — базис $U$, $u = \sum_{i=1}^n \lambda_i e_i$ для некоторых $\lambda_i \in F$.
    Так как $E_V$ — базис $V$, $v = \sum_{j=1}^m \mu_j f_j$ для некоторых $\mu_j \in F$.
    Тогда $(u, v) = (\sum_{i=1}^n \lambda_i e_i, \sum_{j=1}^m \mu_j f_j) = (\sum_{i=1}^n \lambda_i e_i, \mathbf{0}_V) + (\mathbf{0}_U, \sum_{j=1}^m \mu_j f_j)$
    $= \sum_{i=1}^n \lambda_i (e_i, \mathbf{0}_V) + \sum_{j=1}^m \mu_j (\mathbf{0}_U, f_j)$.
    Таким образом, любой вектор из $U \oplus V$ может быть выражен как линейная комбинация векторов из $B$.

Поскольку $B$ является линейно независимой системой, порождающей $U \oplus V$, она является базисом. Число векторов в $B$ равно $n+m$. Следовательно, $\dim(U \oplus V) = n+m = \dim(U) + \dim(V)$.

#### 6.2. Теорема о размерности факторпространства

**Формулировка:**
Пусть $V$ — конечномерное векторное пространство, и $U$ — его подпространство. Тогда размерность факторпространства $V/U$ равна разности размерностей $V$ и $U$:
$\dim(V/U) = \dim(V) - \dim(U)$.

**Доказательство:**
Пусть $\dim(V) = n$ и $\dim(U) = k$.
Поскольку $U$ является подпространством $V$, мы можем выбрать базис $U$ и дополнить его до базиса $V$.
Пусть $E_U = \{e_1, \dots, e_k\}$ — базис подпространства $U$.
Дополним $E_U$ до базиса $E_V = \{e_1, \dots, e_k, e_{k+1}, \dots, e_n\}$ пространства $V$.

Рассмотрим следующую систему классов эквивалентности в $V/U$:
$B_{V/U} = \{e_{k+1}+U, \dots, e_n+U\}$.
Эта система состоит из $n-k$ элементов. Докажем, что она является базисом $V/U$.

1.  **Линейная независимость:**
    Предположим, что существует линейная комбинация этих классов, равная нулевому классу $\mathbf{0}_V+U = U$:
    $\sum_{i=k+1}^n \lambda_i (e_i+U) = U$.
    Это означает $(\sum_{i=k+1}^n \lambda_i e_i) + U = U$.
    По определению классов эквивалентности, это равносильно тому, что $\sum_{i=k+1}^n \lambda_i e_i \in U$.
    Поскольку $E_U = \{e_1, \dots, e_k\}$ — базис $U$, вектор $\sum_{i=k+1}^n \lambda_i e_i$ должен быть линейной комбинацией векторов из $E_U$:
    $\sum_{i=k+1}^n \lambda_i e_i = \sum_{j=1}^k \mu_j e_j$ для некоторых $\mu_j \in F$.
    Перенесем все в одну сторону:
    $(-\mu_1)e_1 + \dots + (-\mu_k)e_k + \lambda_{k+1}e_{k+1} + \dots + \lambda_n e_n = \mathbf{0}$.
    Поскольку $E_V = \{e_1, \dots, e_n\}$ — базис $V$, он линейно независим. Следовательно, все коэффициенты в этой линейной комбинации должны быть равны нулю:
    $-\mu_1=\dots=-\mu_k=0$ и $\lambda_{k+1}=\dots=\lambda_n=0$.
    Таким образом, все коэффициенты $\lambda_i$ в исходной линейной комбинации классов равны нулю, и система $B_{V/U}$ линейно независима.

2.  **Порождает $V/U$:**
    Пусть $v+U$ — произвольный класс из $V/U$.
    Так как $E_V = \{e_1, \dots, e_n\}$ — базис $V$, $v = \sum_{i=1}^n \alpha_i e_i$ для некоторых $\alpha_i \in F$.
    Тогда $v+U = (\sum_{i=1}^n \alpha_i e_i) + U = (\sum_{j=1}^k \alpha_j e_j + \sum_{i=k+1}^n \alpha_i e_i) + U$.
    Поскольку $\sum_{j=1}^k \alpha_j e_j \in U$, то класс $(\sum_{j=1}^k \alpha_j e_j) + U$ является нулевым классом $U$.
    Следовательно, $v+U = (\sum_{i=k+1}^n \alpha_i e_i) + U = \sum_{i=k+1}^n \alpha_i (e_i+U)$.
    Таким образом, любой класс из $V/U$ может быть выражен как линейная комбинация классов из $B_{V/U}$.

Поскольку $B_{V/U}$ является линейно независимой системой, порождающей $V/U$, она является базисом. Число элементов в $B_{V/U}$ равно $n-k$. Следовательно, $\dim(V/U) = n-k = \dim(V) - \dim(U)$.

#### 6.3. Теорема о размерности ядра и образа

**Формулировка (Теорема о ранге и дефекте):**
Пусть $A: U \to V$ — гомоморфизм конечномерных векторных пространств. Тогда сумма размерностей ядра гомоморфизма и его образа равна размерности области определения $U$:
$\dim(\text{Ker } A) + \dim(\text{Im } A) = \dim(U)$.

**Доказательство:**
Эта теорема является прямым следствием **Основной теоремы о гомоморфизме векторных пространств**, которая утверждает, что факторпространство $U/\text{Ker}(A)$ изоморфно образу гомоморфизма $\text{Im}(A)$:
$U/\text{Ker}(A) \cong \text{Im}(A)$.

Поскольку изоморфные векторные пространства имеют одинаковую размерность, мы можем приравнять их размерности:
$\dim(U/\text{Ker}(A)) = \dim(\text{Im}(A))$.

Используя теорему о размерности факторпространства (Билет 6.2), мы знаем, что $\dim(U/\text{Ker}(A)) = \dim(U) - \dim(\text{Ker}(A))$.
Подставляя это в предыдущее равенство, получаем:
$\dim(U) - \dim(\text{Ker}(A)) = \dim(\text{Im}(A))$.

Перенося $\dim(\text{Ker}(A))$ в правую часть, получаем искомую формулу:
$\dim(U) = \dim(\text{Ker}(A)) + \dim(\text{Im}(A))$.

**Интерпретация:**
*   $\dim(U)$ — размерность "входного" пространства.
*   $\dim(\text{Ker } A)$ — размерность "потерянной" части пространства $U$, которая отображается в ноль. Это называется **дефектом** гомоморфизма.
*   $\dim(\text{Im } A)$ — размерность "выходной" части пространства $V$, которая является образом $U$. Это называется **рангом** гомоморфизма.
Теорема утверждает, что "входная" размерность распределяется между "потерянной" и "сохраненной" частями.

---

### Билет 7. Матрица гомоморфизма. Её преобразование при изменении базиса. Векторное пространство гомоморфизмов/прямоугольных матриц. Алгебра эндоморфизмов/квадратных матриц.

#### 7.1. Матрица гомоморфизма

Пусть $A: U \to V$ — гомоморфизм векторных пространств над полем $F$.
Пусть $E = \{e_1, \dots, e_n\}$ — базис пространства $U$ ($\dim(U)=n$).
Пусть $F = \{f_1, \dots, f_m\}$ — базис пространства $V$ ($\dim(V)=m$).

Образ каждого базисного вектора $e_j \in U$ под действием гомоморфизма $A$ является вектором в $V$, и, следовательно, может быть единственным образом разложен по базису $F$:
$A(e_j) = a_{1j} f_1 + a_{2j} f_2 + \dots + a_{mj} f_m = \sum_{i=1}^m a_{ij} f_i$.

**Определение матрицы гомоморфизма:**
**Матрица гомоморфизма** $A_{F, E}$ (или просто $A$) в паре базисов $E$ и $F$ — это матрица размера $m \times n$, столбцами которой являются координаты образов базисных векторов $A(e_j)$ в базисе $F$.
$$A_{F, E} = \begin{pmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix}$$

**Действие гомоморфизма в координатах:**
Если $X_E$ — столбец координат вектора $u \in U$ в базисе $E$, и $Y_F$ — столбец координат вектора $A(u) \in V$ в базисе $F$, то:
$Y_F = A_{F, E} \cdot X_E$.
Это означает, что действие линейного отображения сводится к умножению матрицы на столбец координат.

#### 7.2. Преобразование матрицы гомоморфизма при изменении базиса

Пусть $A: U \to V$ — гомоморфизм.
Пусть $E$ и $\tilde{E}$ — два базиса в $U$, с матрицей перехода $C_{E \to \tilde{E}}$.
Пусть $F$ и $\tilde{F}$ — два базиса в $V$, с матрицей перехода $C_{F \to \tilde{F}}$.

Пусть $A_{F, E}$ — матрица гомоморфизма $A$ в старых базисах $E$ и $F$.
Пусть $A_{\tilde{F}, \tilde{E}}$ — матрица гомоморфизма $A$ в новых базисах $\tilde{E}$ и $\tilde{F}$.

**Формула преобразования:**
Матрицы гомоморфизма связаны следующим соотношением:
$A_{\tilde{F}, \tilde{E}} = (C_{F \to \tilde{F}})^{-1} \cdot A_{F, E} \cdot C_{E \to \tilde{E}}$.

**Доказательство:**
Пусть $u \in U$ — произвольный вектор.
Пусть $X_E$ и $X_{\tilde{E}}$ — его координаты в базисах $E$ и $\tilde{E}$ соответственно.
Пусть $Y_F$ и $Y_{\tilde{F}}$ — координаты образа $A(u)$ в базисах $F$ и $\tilde{F}$ соответственно.

Мы знаем формулы преобразования координат (Билет 5.2):
$X_E = C_{E \to \tilde{E}} \cdot X_{\tilde{E}}$
$Y_F = C_{F \to \tilde{F}} \cdot Y_{\tilde{F}} \implies Y_{\tilde{F}} = (C_{F \to \tilde{F}})^{-1} \cdot Y_F$.

Также мы знаем, как действует гомоморфизм в координатах:
$Y_F = A_{F, E} \cdot X_E$.
$Y_{\tilde{F}} = A_{\tilde{F}, \tilde{E}} \cdot X_{\tilde{E}}$.

Подставим $Y_F$ и $X_E$ в формулу для $Y_{\tilde{F}}$:
$Y_{\tilde{F}} = (C_{F \to \tilde{F}})^{-1} \cdot (A_{F, E} \cdot X_E)$
$Y_{\tilde{F}} = (C_{F \to \tilde{F}})^{-1} \cdot A_{F, E} \cdot (C_{E \to \tilde{E}} \cdot X_{\tilde{E}})$.

Поскольку это равенство должно выполняться для любого $X_{\tilde{E}}$, то матрицы, умножаемые на $X_{\tilde{E}}$, должны быть равны:
$A_{\tilde{F}, \tilde{E}} = (C_{F \to \tilde{F}})^{-1} \cdot A_{F, E} \cdot C_{E \to \tilde{E}}$.

**Частный случай (эндоморфизм):**
Если $A: V \to V$ — эндоморфизм (гомоморфизм из пространства в себя), и мы используем один и тот же базис $E$ (и $\tilde{E}$) для области определения и области значений, то $C_{E \to \tilde{E}}$ — это матрица перехода $C$. Тогда формула упрощается:
$A_{\tilde{E}, \tilde{E}} = C^{-1} \cdot A_{E, E} \cdot C$.
Такое преобразование матрицы называется **преобразованием подобия**.

#### 7.3. Векторное пространство гомоморфизмов/прямоугольных матриц

**Пространство гомоморфизмов $Hom(U, V)$:**
Множество всех гомоморфизмов из векторного пространства $U$ в векторное пространство $V$ над одним полем $F$, обозначаемое $Hom(U, V)$, само образует векторное пространство над $F$.
Операции определяются поточечно:
*   **Сумма гомоморфизмов $(A+B)$:** $(A+B)(u) = A(u) + B(u)$ для всех $u \in U$.
*   **Умножение гомоморфизма на скаляр $(\lambda A)$:** $(\lambda A)(u) = \lambda (A(u))$ для всех $u \in U, \lambda \in F$.
Все аксиомы векторного пространства выполняются, так как они выполняются для векторов в $V$.

**Изоморфизм $Hom(U, V) \cong M_{m \times n}(F)$:**
Пространство гомоморфизмов $Hom(U, V)$ изоморфно пространству прямоугольных матриц $M_{m \times n}(F)$, где $n = \dim(U)$ и $m = \dim(V)$.
*   **Доказательство:** Изоморфизм устанавливается путем сопоставления каждому гомоморфизму $A \in Hom(U, V)$ его матрицы $A_{F, E}$ в фиксированной паре базисов $E$ и $F$.
    *   Это отображение является линейным: $M(A+B) = M(A)+M(B)$ и $M(\lambda A) = \lambda M(A)$.
    *   Это отображение является биективным:
        *   Инъективность: Если $M(A)$ — нулевая матрица, то $A(e_j) = \mathbf{0}_V$ для всех базисных векторов $e_j$. Поскольку $A$ линейно, это означает $A(u) = \mathbf{0}_V$ для всех $u \in U$, т.е. $A$ — нулевой гомоморфизм.
        *   Сюръективность: Для любой матрицы $M \in M_{m \times n}(F)$ можно построить гомоморфизм $A$, который переводит базисные векторы $e_j$ в векторы, координаты которых в базисе $F$ являются столбцами $M$.
*   **Следствие (Размерность):** $\dim(Hom(U, V)) = \dim(M_{m \times n}(F)) = m \cdot n = \dim(U) \cdot \dim(V)$.

#### 7.4. Алгебра эндоморфизмов/квадратных матриц

**Эндоморфизм:**
Гомоморфизм $A: V \to V$ из векторного пространства в себя называется **эндоморфизмом**.
Матрица эндоморфизма в любом базисе $E$ является квадратной матрицей размера $n \times n$, где $n = \dim(V)$.

**Алгебра эндоморфизмов $End(V)$:**
Множество всех эндоморфизмов $End(V) = Hom(V, V)$ обладает структурой **алгебры** над полем $F$.
Алгебра — это векторное пространство, на котором дополнительно определена ассоциативная операция умножения (композиция гомоморфизмов), связанная с умножением на скаляр.
1.  $End(V)$ является векторным пространством (как частный случай $Hom(U, V)$).
2.  Определена операция **композиции (умножения)** гомоморфизмов: $(A \circ B)(v) = A(B(v))$. Композиция двух эндоморфизмов также является эндоморфизмом.
3.  Композиция является ассоциативной: $(A \circ B) \circ C = A \circ (B \circ C)$.
4.  Композиция дистрибутивна относительно сложения: $A \circ (B+C) = A \circ B + A \circ C$ и $(A+B) \circ C = A \circ C + B \circ C$.
5.  Композиция совместима с умножением на скаляр: $\lambda(A \circ B) = (\lambda A) \circ B = A \circ (\lambda B)$.
6.  Существует единичный элемент по умножению: тождественный эндоморфизм $I(v)=v$.

**Алгебра квадратных матриц $M_{n \times n}(F)$:**
Пространство квадратных матриц $M_{n \times n}(F)$ также образует алгебру над $F$.
1.  $M_{n \times n}(F)$ является векторным пространством.
2.  Определено стандартное умножение матриц, которое является ассоциативным.
3.  Умножение матриц дистрибутивно относительно сложения.
4.  Умножение на скаляр совместимо с умножением матриц.
5.  Единичная матрица $I$ является единичным элементом по умножению.

**Изоморфизм алгебр:**
Алгебра эндоморфизмов $End(V)$ изоморфна алгебре квадратных матриц $M_{n \times n}(F)$ (при фиксированном базисе $E$ в $V$).
*   Изоморфизм устанавливается сопоставлением эндоморфизма его матрице.
*   При этом изоморфизме композиции гомоморфизмов соответствует умножение матриц: $M(A \circ B) = M(A) \cdot M(B)$.
    *   **Доказательство:** Пусть $A: V \to V$ и $B: V \to V$ — эндоморфизмы. Пусть $E$ — базис $V$.
        Пусть $X_E$ — координаты вектора $v \in V$.
        Координаты $B(v)$ в базисе $E$ это $M(B) X_E$.
        Координаты $A(B(v))$ в базисе $E$ это $M(A) (M(B) X_E) = (M(A) M(B)) X_E$.
        С другой стороны, координаты $(A \circ B)(v)$ это $M(A \circ B) X_E$.
        Следовательно, $M(A \circ B) = M(A) M(B)$.

---

### Билет 8. Сумма и пересечение подпространств. Нахождение базиса суммы и пересечения подпространств. Теорема Нётер об изоморфизме. Формула Грассмана.

#### 8.1. Сумма и пересечение подпространств

Пусть $U$ и $W$ — два подпространства векторного пространства $V$.

**Определение пересечения подпространств ($U \cap W$):**
**Пересечением** подпространств $U$ и $W$ называется множество векторов, которые принадлежат как $U$, так и $W$:
$U \cap W = \{v \in V \mid v \in U \text{ и } v \in W\}$.

**Свойства пересечения:**
*   Пересечение $U \cap W$ **всегда является подпространством** $V$.
    *   **Доказательство:**
        1.  $\mathbf{0} \in U$ и $\mathbf{0} \in W$, так как $U$ и $W$ — подпространства. Следовательно, $\mathbf{0} \in U \cap W$, и пересечение непусто.
        2.  Если $v_1, v_2 \in U \cap W$, то $v_1, v_2 \in U$ и $v_1, v_2 \in W$. Поскольку $U$ и $W$ замкнуты относительно сложения, $v_1+v_2 \in U$ и $v_1+v_2 \in W$. Следовательно, $v_1+v_2 \in U \cap W$.
        3.  Если $v \in U \cap W$ и $\lambda \in F$, то $v \in U$ и $v \in W$. Поскольку $U$ и $W$ замкнуты относительно умножения на скаляр, $\lambda v \in U$ и $\lambda v \in W$. Следовательно, $\lambda v \in U \cap W$.
*   $U \cap W$ является наибольшим подпространством, содержащимся как в $U$, так и в $W$.

**Определение суммы подпространств ($U + W$):**
**Суммой** подпространств $U$ и $W$ называется множество всех векторов, которые могут быть представлены как сумма вектора из $U$ и вектора из $W$:
$U + W = \{u+w \mid u \in U, w \in W\}$.

**Свойства суммы:**
*   Сумма $U + W$ **всегда является подпространством** $V$.
    *   **Доказательство:**
        1.  $\mathbf{0} = \mathbf{0}_U + \mathbf{0}_W \in U+W$, так как $\mathbf{0}_U \in U$ и $\mathbf{0}_W \in W$. Сумма непуста.
        2.  Если $v_1, v_2 \in U+W$, то $v_1 = u_1+w_1$ и $v_2 = u_2+w_2$. Тогда $v_1+v_2 = (u_1+w_1)+(u_2+w_2) = (u_1+u_2)+(w_1+w_2)$. Поскольку $u_1+u_2 \in U$ и $w_1+w_2 \in W$, то $v_1+v_2 \in U+W$.
        3.  Если $v \in U+W$ и $\lambda \in F$, то $v = u+w$. Тогда $\lambda v = \lambda(u+w) = \lambda u + \lambda w$. Поскольку $\lambda u \in U$ и $\lambda w \in W$, то $\lambda v \in U+W$.
*   $U+W$ является наименьшим подпространством, содержащим как $U$, так и $W$.
*   $U+W$ также является линейной оболочкой объединения $U \cup W$: $U+W = \text{Span}(U \cup W)$.

#### 8.2. Нахождение базиса суммы и пересечения подпространств

Пусть $U$ и $W$ — подпространства, заданные своими базисами.
Пусть $B_U = \{u_1, \dots, u_k\}$ — базис $U$.
Пусть $B_W = \{w_1, \dots, w_m\}$ — базис $W$.

**Нахождение базиса суммы $U+W$:**
1.  Объединим базисы: $B_{U \cup W} = \{u_1, \dots, u_k, w_1, \dots, w_m\}$.
2.  Эта объединенная система порождает $U+W$.
3.  Чтобы получить базис $U+W$, нужно из $B_{U \cup W}$ выделить максимальную линейно независимую подсистему. Это можно сделать, записав все векторы $B_{U \cup W}$ как столбцы матрицы и приведя её к ступенчатому виду. Ненулевые строки (или соответствующие им исходные столбцы) будут образовывать базис $U+W$.
    *   **Пример:** Если $U = \text{Span}((1,0,0), (0,1,0))$ и $W = \text{Span}((1,1,0), (0,0,1))$, то $B_{U \cup W} = \{(1,0,0), (0,1,0), (1,1,0), (0,0,1)\}$. Матрица из этих векторов:
        $\begin{pmatrix}
        1 & 0 & 1 & 0 \\
        0 & 1 & 1 & 0 \\
        0 & 0 & 0 & 1
        \end{pmatrix}$
        Вектор $(1,1,0)$ является линейной комбинацией первых двух. Базис $U+W$ будет $\{(1,0,0), (0,1,0), (0,0,1)\}$.

**Нахождение базиса пересечения $U \cap W$:**
Вектор $v \in U \cap W$ тогда и только тогда, когда $v \in U$ и $v \in W$.
Это означает, что $v$ может быть выражен как линейная комбинация базисных векторов $U$ и как линейная комбинация базисных векторов $W$:
$v = \alpha_1 u_1 + \dots + \alpha_k u_k = \beta_1 w_1 + \dots + \beta_m w_m$.
Перенесем все в одну сторону:
$\alpha_1 u_1 + \dots + \alpha_k u_k - \beta_1 w_1 - \dots - \beta_m w_m = \mathbf{0}$.
Это однородная система линейных уравнений относительно неизвестных $\alpha_i$ и $\beta_j$.
1.  Запишем векторы $u_i$ и $w_j$ в координатной форме (например, в стандартном базисе объемлющего пространства $V$).
2.  Составим матрицу, столбцами которой являются координаты $u_1, \dots, u_k, -w_1, \dots, -w_m$.
3.  Найдем фундаментальную систему решений (ФСР) для этой однородной СЛАУ. Решения будут представлять собой наборы $(\alpha_1, \dots, \alpha_k, \beta_1, \dots, \beta_m)$.
4.  Для каждого вектора ФСР подставим найденные $\alpha_i$ (или $\beta_j$) обратно в выражение для $v$. Полученные векторы будут образовывать базис $U \cap W$.

#### 8.3. Теорема Нётер об изоморфизме

**Формулировка (Вторая теорема об изоморфизме):**
Пусть $U$ и $W$ — подпространства векторного пространства $V$. Тогда факторпространство $(U+W)/U$ изоморфно факторпространству $W/(U \cap W)$:
$$(U+W)/U \cong W/(U \cap W)$$

**Доказательство:**
Построим отображение $\phi: W \to (U+W)/U$ следующим образом:
Для любого $w \in W$, определим $\phi(w) = w+U$.

1.  **$\phi$ является гомоморфизмом:**
    *   Аддитивность: $\phi(w_1+w_2) = (w_1+w_2)+U = (w_1+U)+(w_2+U) = \phi(w_1)+\phi(w_2)$.
    *   Однородность: $\phi(\lambda w) = (\lambda w)+U = \lambda(w+U) = \lambda \phi(w)$.

2.  **$\phi$ является сюръективным:**
    Пусть $x+U$ — произвольный элемент из $(U+W)/U$. По определению суммы, $x = u+w$ для некоторых $u \in U, w \in W$.
    Тогда $x+U = (u+w)+U = w+U$ (поскольку $u \in U$).
    Следовательно, для любого класса $x+U$ существует $w \in W$ (а именно, $w$) такой, что $\phi(w) = w+U = x+U$. Значит, $\phi$ сюръективно.

3.  **Ядро $\text{Ker}(\phi)$:**
    $\text{Ker}(\phi) = \{w \in W \mid \phi(w) = U\}$ (нулевой класс в $(U+W)/U$).
    $\phi(w) = U \iff w+U = U \iff w \in U$.
    Таким образом, $\text{Ker}(\phi) = \{w \in W \mid w \in U\} = U \cap W$.

По Основной теореме о гомоморфизме векторных пространств (Билет 2.4), факторпространство по ядру изоморфно образу. В данном случае, образ $\text{Im}(\phi) = (U+W)/U$.
Следовательно, $W/\text{Ker}(\phi) \cong \text{Im}(\phi)$, что дает:
$W/(U \cap W) \cong (U+W)/U$.

#### 8.4. Формула Грассмана

**Формулировка:**
Пусть $U$ и $W$ — конечномерные подпространства векторного пространства $V$. Тогда размерность их суммы связана с размерностями $U$, $W$ и их пересечения следующей формулой:
$\dim(U+W) = \dim(U) + \dim(W) - \dim(U \cap W)$.

**Доказательство:**
Формула Грассмана является прямым следствием Теоремы Нётер об изоморфизме и теоремы о размерности факторпространства.
Из Теоремы Нётер: $(U+W)/U \cong W/(U \cap W)$.
Поскольку изоморфные пространства имеют одинаковую размерность:
$\dim((U+W)/U) = \dim(W/(U \cap W))$.

Используя теорему о размерности факторпространства (Билет 6.2), мы знаем, что:
$\dim((U+W)/U) = \dim(U+W) - \dim(U)$
$\dim(W/(U \cap W)) = \dim(W) - \dim(U \cap W)$.

Приравнивая эти выражения:
$\dim(U+W) - \dim(U) = \dim(W) - \dim(U \cap W)$.

Перенося $\dim(U)$ в правую часть, получаем формулу Грассмана:
$\dim(U+W) = \dim(U) + \dim(W) - \dim(U \cap W)$.

**Интерпретация:**
Формула Грассмана похожа на принцип включения-исключения для множеств. Когда мы складываем размерности $U$ и $W$, мы "дважды считаем" размерность их пересечения. Вычитание $\dim(U \cap W)$ компенсирует это двойное считывание.

---

### Билет 9. Линейно независимые подпространства. Эквивалентные условия линейной независимости подпространств. Внутренняя прямая сумма и проекция на прямое слагаемое. Канонический изоморфизм внешней и внутренней прямых сумм.

#### 9.1. Линейно независимые подпространства

**Определение:**
Система подпространств $U_1, U_2, \dots, U_k$ векторного пространства $V$ называется **линейно независимой**, если из равенства $\sum_{i=1}^k u_i = \mathbf{0}$, где $u_i \in U_i$ для каждого $i$, следует, что все векторы $u_i$ являются нулевыми: $u_1 = u_2 = \dots = u_k = \mathbf{0}$.

**Лемма о линейной независимости двух подпространств:**
Два подпространства $U$ и $W$ линейно независимы тогда и только тогда, когда их пересечение тривиально: $U \cap W = \{\mathbf{0}\}$.

**Доказательство:**
*   **($\implies$)** Пусть $U$ и $W$ линейно независимы. Предположим, $v \in U \cap W$. Тогда $v \in U$ и $v \in W$. Мы можем записать $v + (-v) = \mathbf{0}$, где $v \in U$ и $-v \in W$. Поскольку $U$ и $W$ линейно независимы, это означает, что $v = \mathbf{0}$ и $-v = \mathbf{0}$. Следовательно, $U \cap W = \{\mathbf{0}\}$.
*   **($\impliedby$)** Пусть $U \cap W = \{\mathbf{0}\}$. Предположим, $u+w = \mathbf{0}$ для некоторых $u \in U, w \in W$. Тогда $u = -w$. Поскольку $u \in U$, а $-w \in W$ (так как $W$ — подпространство), то $u \in U \cap W$. Следовательно, $u = \mathbf{0}$. А так как $u = -w$, то и $w = \mathbf{0}$. Таким образом, $u=w=\mathbf{0}$, что означает линейную независимость $U$ и $W$.

#### 9.2. Эквивалентные условия линейной независимости подпространств

Для набора подпространств $U_1, \dots, U_k$ в конечномерном пространстве $V$ следующие условия эквивалентны:
1.  Подпространства $U_1, \dots, U_k$ линейно независимы.
2.  Для каждого $j \in \{1, \dots, k\}$, пересечение $U_j$ с суммой остальных подпространств является тривиальным: $U_j \cap (\sum_{i \neq j} U_i) = \{\mathbf{0}\}$.
3.  Если $B_1, \dots, B_k$ — базисы подпространств $U_1, \dots, U_k$ соответственно, то их объединение $B = B_1 \cup \dots \cup B_k$ является линейно независимой системой векторов.
4.  Размерность суммы этих подпространств равна сумме их размерностей: $\dim(U_1 + \dots + U_k) = \sum_{i=1}^k \dim(U_i)$.

**Доказательство (ключевые связи):**
*   **($1 \iff 2$):**
    *   **($1 \implies 2$):** Пусть $U_1, \dots, U_k$ линейно независимы. Предположим, $v \in U_j \cap (\sum_{i \neq j} U_i)$. Тогда $v \in U_j$ и $v = \sum_{i \neq j} u_i$ для некоторых $u_i \in U_i$. Мы можем записать $v - \sum_{i \neq j} u_i = \mathbf{0}$. Это линейная комбинация векторов из $U_1, \dots, U_k$, равная нулю. По определению линейной независимости подпространств, все слагаемые должны быть нулевыми. В частности, $v = \mathbf{0}$.
    *   **($2 \implies 1$):** Пусть $U_j \cap (\sum_{i \neq j} U_i) = \{\mathbf{0}\}$ для всех $j$. Предположим, $\sum_{i=1}^k u_i = \mathbf{0}$ для $u_i \in U_i$. Тогда $u_j = -\sum_{i \neq j} u_i$. Левая часть $u_j \in U_j$, правая часть $-\sum_{i \neq j} u_i \in \sum_{i \neq j} U_i$. Следовательно, $u_j \in U_j \cap (\sum_{i \neq j} U_i)$. По условию, $u_j = \mathbf{0}$. Это верно для всех $j$. Значит, подпространства линейно независимы.
*   **($3 \iff 4$):**
    *   **($3 \implies 4$):** Если объединение базисов $B$ линейно независимо, то оно является базисом суммы $U_1 + \dots + U_k$. Число векторов в $B$ равно $\sum \dim(U_i)$. Следовательно, $\dim(\sum U_i) = \sum \dim(U_i)$.
    *   **($4 \implies 3$):** Если $\dim(\sum U_i) = \sum \dim(U_i)$, то объединение базисов $B$ содержит $\sum \dim(U_i)$ векторов. Поскольку оно порождает $\sum U_i$ и имеет то же количество векторов, что и размерность, оно должно быть линейно независимым.

#### 9.3. Внутренняя прямая сумма и проекция на прямое слагаемое

**Определение внутренней прямой суммы:**
Векторное пространство $V$ называется **внутренней прямой суммой** своих подпространств $U_1, \dots, U_k$ (обозначается $V = U_1 \oplus \dots \oplus U_k$), если:
1.  $V$ является суммой этих подпространств: $V = U_1 + \dots + U_k$.
2.  Эти подпространства $U_1, \dots, U_k$ линейно независимы.

**Теорема (свойство внутренней прямой суммы):**
Пространство $V$ является внутренней прямой суммой подпространств $U_1, \dots, U_k$ тогда и только тогда, когда каждый вектор $v \in V$ может быть **единственным образом** представлен в виде суммы $v = u_1 + \dots + u_k$, где $u_i \in U_i$.

**Доказательство:**
*   **($\implies$)** Пусть $V = U_1 \oplus \dots \oplus U_k$. По определению, $V = \sum U_i$, поэтому каждый $v \in V$ может быть представлен как $v = u_1 + \dots + u_k$. Докажем единственность. Предположим, $v$ имеет два таких разложения:
    $v = u_1 + \dots + u_k$
    $v = u_1' + \dots + u_k'$
    Вычтем одно из другого: $\mathbf{0} = (u_1-u_1') + \dots + (u_k-u_k')$.
    Поскольку $u_i-u_i' \in U_i$, и подпространства линейно независимы, то $u_i-u_i' = \mathbf{0}$ для всех $i$.
    Следовательно, $u_i = u_i'$ для всех $i$, и разложение единственно.
*   **($\impliedby$)** Предположим, каждый $v \in V$ единственным образом раскладывается.
    1.  $V = \sum U_i$ очевидно из существования разложения.
    2.  Докажем линейную независимость подпространств. Пусть $\sum u_i = \mathbf{0}$. Мы также знаем, что $\mathbf{0} = \mathbf{0}_1 + \dots + \mathbf{0}_k$, где $\mathbf{0}_i \in U_i$. Из единственности разложения нулевого вектора следует, что $u_i = \mathbf{0}_i = \mathbf{0}$ для всех $i$. Значит, подпространства линейно независимы.

**Проекция на прямое слагаемое:**
Если $V = U_1 \oplus \dots \oplus U_k$, то для каждого вектора $v \in V$ существует единственное разложение $v = u_1 + \dots + u_k$.
Отображение $P_j: V \to U_j$, которое ставит в соответствие вектору $v$ его $j$-ю компоненту $u_j$ в этом разложении, называется **проекцией на подпространство $U_j$ параллельно остальным слагаемым**.
*   Каждая проекция $P_j$ является гомоморфизмом.
*   $P_j^2 = P_j$ (проекция на себя).
*   $\sum_{j=1}^k P_j = I$ (тождественное отображение).
*   $P_j \circ P_i = 0$ при $i \neq j$.

#### 9.4. Канонический изоморфизм внешней и внутренней прямых сумм

**Теорема:**
Пусть $U_1, \dots, U_k$ — подпространства векторного пространства $V$. Если $V$ является внутренней прямой суммой этих подпространств ($V = U_1 \oplus \dots \oplus U_k$), то $V$ канонически изоморфно внешней прямой сумме $U_1 \oplus \dots \oplus U_k$.

**Доказательство:**
Пусть $W = U_1 \oplus \dots \oplus U_k$ — внешняя прямая сумма. Элементы $W$ — это кортежи $(u_1, \dots, u_k)$, где $u_i \in U_i$.
Построим отображение $\Phi: W \to V$ следующим образом:
$\Phi((u_1, \dots, u_k)) = u_1 + \dots + u_k$.

1.  **$\Phi$ является гомоморфизмом:**
    *   Аддитивность: $\Phi((u_1, \dots, u_k) + (u_1', \dots, u_k')) = \Phi((u_1+u_1', \dots, u_k+u_k')) = (u_1+u_1') + \dots + (u_k+u_k') = (u_1+\dots+u_k) + (u_1'+\dots+u_k') = \Phi((u_1, \dots, u_k)) + \Phi((u_1', \dots, u_k'))$.
    *   Однородность: $\Phi(\lambda(u_1, \dots, u_k)) = \Phi((\lambda u_1, \dots, \lambda u_k)) = \lambda u_1 + \dots + \lambda u_k = \lambda(u_1+\dots+u_k) = \lambda \Phi((u_1, \dots, u_k))$.

2.  **$\Phi$ является сюръективным:**
    Поскольку $V = U_1 + \dots + U_k$ (по определению внутренней прямой суммы), любой вектор $v \in V$ может быть представлен как $v = u_1 + \dots + u_k$ для некоторых $u_i \in U_i$. Тогда $\Phi((u_1, \dots, u_k)) = v$. Значит, $\Phi$ сюръективно.

3.  **$\Phi$ является инъективным:**
    Пусть $\Phi((u_1, \dots, u_k)) = \mathbf{0}_V$. Это означает $u_1 + \dots + u_k = \mathbf{0}_V$.
    Поскольку подпространства $U_1, \dots, U_k$ линейно независимы (по определению внутренней прямой суммы), из этого равенства следует, что $u_1 = \dots = u_k = \mathbf{0}$.
    Следовательно, $(u_1, \dots, u_k) = (\mathbf{0}, \dots, \mathbf{0})$, что является нулевым элементом внешней прямой суммы $W$.
    Значит, $\text{Ker}(\Phi) = \{(\mathbf{0}, \dots, \mathbf{0})\}$, и $\Phi$ инъективно.

Поскольку $\Phi$ является биективным гомоморфизмом, оно является изоморфизмом. Этот изоморфизм называется каноническим, так как его построение не зависит от выбора базисов.

---

### Билет 10. Сопряжённое (двойственное) к V пространство V* . Сопряжённый базис, смысл его элементов. Канонический изоморфизм между V и V**.

#### 10.1. Сопряжённое (двойственное) к V пространство V*

**Определение:**
Пусть $V$ — векторное пространство над полем $F$. **Сопряжённым (или двойственным) пространством** к $V$, обозначаемым $V^*$, называется множество всех линейных функционалов (линейных отображений) из $V$ в поле $F$.
$V^* = Hom(V, F) = \{ \omega: V \to F \mid \omega \text{ — линейное отображение} \}$.

**Свойства $V^*$:**
*   $V^*$ само является векторным пространством над $F$. Операции сложения функционалов и умножения функционала на скаляр определяются поточечно:
    *   $(\omega_1 + \omega_2)(v) = \omega_1(v) + \omega_2(v)$
    *   $(\lambda \omega)(v) = \lambda \omega(v)$
*   Если $V$ конечномерно, то $\dim(V^*) = \dim(V)$.
    *   **Доказательство:** Это следует из того, что $\dim(Hom(U, V)) = \dim(U) \cdot \dim(V)$. В данном случае $U=V$ и $V=F$. $\dim(F)=1$. Значит, $\dim(V^*) = \dim(V) \cdot \dim(F) = \dim(V) \cdot 1 = \dim(V)$.

#### 10.2. Сопряжённый базис, смысл его элементов

**Определение:**
Пусть $E = \{e_1, \dots, e_n\}$ — базис векторного пространства $V$. **Сопряжённым базисом** (или двойственным базисом) к $E$ в пространстве $V^*$ называется система линейных функционалов $E^* = \{e_1^*, \dots, e_n^*\}$, определенная следующим образом:
$e_i^*(e_j) = \delta_{ij}$, где $\delta_{ij}$ — символ Кронекера (равен 1, если $i=j$, и 0, если $i \neq j$).

**Свойства сопряжённого базиса:**
*   Сопряжённый базис $E^*$ всегда является базисом пространства $V^*$.
    *   **Доказательство:**
        1.  **Линейная независимость:** Предположим, $\sum_{i=1}^n \lambda_i e_i^* = \mathbf{0}_{V^*}$ (нулевой функционал). Применим этот функционал к каждому базисному вектору $e_j \in V$:
            $(\sum_{i=1}^n \lambda_i e_i^*)(e_j) = \sum_{i=1}^n \lambda_i e_i^*(e_j) = \sum_{i=1}^n \lambda_i \delta_{ij} = \lambda_j$.
            Поскольку это нулевой функционал, он должен давать 0 для любого вектора, в том числе для $e_j$. Значит, $\lambda_j = 0$ для всех $j=1, \dots, n$. Таким образом, $E^*$ линейно независим.
        2.  **Порождает $V^*$:** Пусть $\omega \in V^*$ — произвольный линейный функционал. Мы хотим показать, что $\omega = \sum_{i=1}^n \omega(e_i) e_i^*$.
            Применим функционал $\sum_{i=1}^n \omega(e_i) e_i^*$ к произвольному базисному вектору $e_j$:
            $(\sum_{i=1}^n \omega(e_i) e_i^*)(e_j) = \sum_{i=1}^n \omega(e_i) e_i^*(e_j) = \sum_{i=1}^n \omega(e_i) \delta_{ij} = \omega(e_j)$.
            Поскольку два линейных функционала совпадают на базисных векторах, они совпадают на всем пространстве $V$. Следовательно, $\omega = \sum_{i=1}^n \omega(e_i) e_i^*$.
            Таким образом, $E^*$ порождает $V^*$.

**Смысл элементов сопряжённого базиса:**
Каждый функционал $e_i^*$ "измеряет" $i$-ю координату вектора в базисе $E$.
Если $v = \sum_{j=1}^n x_j e_j$ — разложение вектора $v$ по базису $E$, то:
$e_i^*(v) = e_i^*(\sum_{j=1}^n x_j e_j) = \sum_{j=1}^n x_j e_i^*(e_j) = \sum_{j=1}^n x_j \delta_{ij} = x_i$.
Таким образом, $e_i^*(v)$ возвращает $i$-ю координату вектора $v$ в базисе $E$. Функционалы сопряжённого базиса являются **координатными функциями**.

#### 10.3. Канонический изоморфизм между V и V**

**Определение второго сопряжённого пространства V**:**
**Второе сопряжённое пространство** $V^{**}$ — это сопряжённое пространство к $V^*$, то есть $V^{**} = (V^*)^* = Hom(V^*, F)$.
Если $V$ конечномерно, то $\dim(V^{**}) = \dim(V^*) = \dim(V)$.

**Канонический изоморфизм:**
Существует естественный (канонический) изоморфизм $\Phi: V \to V^{**}$, который не зависит от выбора базиса.
**Определение $\Phi$:**
Для каждого вектора $v \in V$, определим $\Phi(v)$ как линейный функционал на $V^*$ (т.е. элемент $V^{**}$), который действует на любой функционал $\omega \in V^*$ следующим образом:
$\Phi(v)(\omega) = \omega(v)$.
То есть, $\Phi(v)$ — это "функция оценки", которая берет функционал $\omega$ и возвращает его значение на векторе $v$.

**Доказательство, что $\Phi$ является изоморфизмом (для конечномерных $V$):**
1.  **$\Phi(v)$ является линейным функционалом на $V^*$:**
    Для любых $\omega_1, \omega_2 \in V^*$ и $\lambda \in F$:
    *   $\Phi(v)(\omega_1+\omega_2) = (\omega_1+\omega_2)(v) = \omega_1(v)+\omega_2(v) = \Phi(v)(\omega_1)+\Phi(v)(\omega_2)$.
    *   $\Phi(v)(\lambda \omega) = (\lambda \omega)(v) = \lambda \omega(v) = \lambda \Phi(v)(\omega)$.
    Значит, $\Phi(v) \in V^{**}$.

2.  **$\Phi$ является гомоморфизмом из $V$ в $V^{**}$:**
    Для любых $v_1, v_2 \in V$ и $\lambda \in F$:
    *   $\Phi(v_1+v_2)(\omega) = \omega(v_1+v_2) = \omega(v_1)+\omega(v_2) = \Phi(v_1)(\omega)+\Phi(v_2)(\omega) = (\Phi(v_1)+\Phi(v_2))(\omega)$.
        Следовательно, $\Phi(v_1+v_2) = \Phi(v_1)+\Phi(v_2)$.
    *   $\Phi(\lambda v)(\omega) = \omega(\lambda v) = \lambda \omega(v) = \lambda \Phi(v)(\omega) = (\lambda \Phi(v))(\omega)$.
        Следовательно, $\Phi(\lambda v) = \lambda \Phi(v)$.
    Значит, $\Phi$ — гомоморфизм.

3.  **$\Phi$ является инъективным:**
    Предположим, $\Phi(v) = \mathbf{0}_{V^{**}}$ (нулевой функционал на $V^*$).
    Это означает, что $\Phi(v)(\omega) = 0$ для всех $\omega \in V^*$.
    По определению, $\omega(v) = 0$ для всех $\omega \in V^*$.
    Если $v \neq \mathbf{0}_V$, то по теореме о дополнении линейно независимой системы до базиса, $v$ можно включить в базис $V$. Тогда существует функционал $e_1^*$ (из сопряжённого базиса), такой что $e_1^*(v) = 1$. Это противоречит тому, что $\omega(v)=0$ для всех $\omega$.
    Следовательно, $v$ должно быть $\mathbf{0}_V$.
    Значит, $\text{Ker}(\Phi) = \{\mathbf{0}_V\}$, и $\Phi$ инъективно.

4.  **$\Phi$ является сюръективным (для конечномерных $V$):**
    Поскольку $\Phi$ инъективно и $\dim(V) = \dim(V^{**})$ (как показано выше), то для конечномерных пространств инъективный гомоморфизм между пространствами одинаковой размерности является также сюръективным.

Таким образом, $\Phi$ является биективным гомоморфизмом, то есть изоморфизмом. Этот изоморфизм называется каноническим, потому что его определение не требует выбора базиса.

---

### Билет 11. Аннулятор подпространства. Теоремы о размерности аннулятора и о втором аннуляторе. Теорема о соответствии U <-> 0. Критерий базисности системы линейных функций.

#### 11.1. Аннулятор подпространства

**Определение:**
Пусть $V$ — векторное пространство над полем $F$, и $U$ — его подпространство. **Аннулятором** подпространства $U$, обозначаемым $U^\circ$ (или $U^\perp$), называется множество всех линейных функционалов из $V^*$, которые обращаются в нуль на всех векторах из $U$:
$U^\circ = \{\omega \in V^* \mid \omega(u) = 0 \text{ для всех } u \in U\}$.

**Свойства аннулятора:**
*   Аннулятор $U^\circ$ всегда является подпространством $V^*$.
    *   **Доказательство:**
        1.  Нулевой функционал $\mathbf{0}_{V^*}$ обращается в нуль на всех векторах, в том числе на всех $u \in U$. Значит, $\mathbf{0}_{V^*} \in U^\circ$, и $U^\circ$ непусто.
        2.  Если $\omega_1, \omega_2 \in U^\circ$, то $\omega_1(u)=0$ и $\omega_2(u)=0$ для всех $u \in U$. Тогда $(\omega_1+\omega_2)(u) = \omega_1(u)+\omega_2(u) = 0+0 = 0$. Значит, $\omega_1+\omega_2 \in U^\circ$.
        3.  Если $\omega \in U^\circ$ и $\lambda \in F$, то $\omega(u)=0$ для всех $u \in U$. Тогда $(\lambda \omega)(u) = \lambda \omega(u) = \lambda \cdot 0 = 0$. Значит, $\lambda \omega \in U^\circ$.
    По критерию подпространства, $U^\circ$ является подпространством $V^*$.
*   Аннулятор всего пространства $V^\circ = \{\mathbf{0}_{V^*}\}$.
*   Аннулятор нулевого подпространства $\{\mathbf{0}\}^\circ = V^*$.

#### 11.2. Теоремы о размерности аннулятора и о втором аннуляторе

**Теорема о размерности аннулятора:**
Пусть $V$ — конечномерное векторное пространство, и $U$ — его подпространство. Тогда размерность аннулятора $U^\circ$ равна коразмерности $U$ в $V$:
$\dim(U^\circ) = \dim(V) - \dim(U)$.

**Доказательство:**
Эта теорема является следствием изоморфизма факторпространства сопряжённого пространства по аннулятору $U$ и сопряжённого пространства $U$:
$V^*/U^\circ \cong U^*$.
Поскольку изоморфные пространства имеют одинаковую размерность, $\dim(V^*/U^\circ) = \dim(U^*)$.
Используя теорему о размерности факторпространства (Билет 6.2) и тот факт, что $\dim(V^*) = \dim(V)$ и $\dim(U^*) = \dim(U)$ для конечномерных пространств:
$\dim(V^*) - \dim(U^\circ) = \dim(U^*)$
$\dim(V) - \dim(U^\circ) = \dim(U)$.
Отсюда: $\dim(U^\circ) = \dim(V) - \dim(U)$.

**Теорема о втором аннуляторе:**
Пусть $V$ — конечномерное векторное пространство, и $U$ — его подпространство. Тогда второй аннулятор подпространства $U$ совпадает с самим подпространством $U$:
$U^{\circ\circ} = U$.
Здесь $U^{\circ\circ} = (U^\circ)^\circ$ — это аннулятор подпространства $U^\circ \subseteq V^*$, который является подпространством $V^{**}$. При этом $U$ отождествляется с его образом при каноническом изоморфизме $\Phi: V \to V^{**}$.

**Доказательство:**
1.  **Покажем, что $U \subseteq U^{\circ\circ}$:**
    Пусть $u \in U$. Рассмотрим $\Phi(u) \in V^{**}$. По определению, $\Phi(u)(\omega) = \omega(u)$ для любого $\omega \in V^*$.
    Если $\omega \in U^\circ$, то по определению аннулятора $\omega(u) = 0$.
    Следовательно, $\Phi(u)(\omega) = 0$ для всех $\omega \in U^\circ$.
    Это означает, что $\Phi(u)$ обращается в нуль на всех функционалах из $U^\circ$, то есть $\Phi(u) \in (U^\circ)^\circ = U^{\circ\circ}$.
    Таким образом, $U \subseteq U^{\circ\circ}$ (с учетом отождествления $V$ и $V^{**}$).

2.  **Покажем, что $\dim(U) = \dim(U^{\circ\circ})$:**
    Используем теорему о размерности аннулятора дважды:
    $\dim(U^\circ) = \dim(V) - \dim(U)$.
    $\dim(U^{\circ\circ}) = \dim(V^*) - \dim(U^\circ)$.
    Поскольку $\dim(V^*) = \dim(V)$ для конечномерных пространств:
    $\dim(U^{\circ\circ}) = \dim(V) - (\dim(V) - \dim(U)) = \dim(U)$.

Поскольку $U$ является подпространством $U^{\circ\circ}$ и их размерности совпадают, они должны быть равны: $U = U^{\circ\circ}$.

#### 11.3. Теорема о соответствии U <-> 0

**Формулировка:**
Для конечномерного векторного пространства $V$, операция аннуляции устанавливает взаимно однозначное соответствие (биекцию) между множеством всех подпространств $V$ и множеством всех подпространств $V^*$. Это соответствие обращает включение.

**Свойства этого соответствия:**
1.  **Биекция:** Для каждого подпространства $U \subseteq V$ существует единственное подпространство $U^\circ \subseteq V^*$, и для каждого подпространства $W \subseteq V^*$ существует единственное подпространство $W^\circ \subseteq V^{**}$ (которое можно отождествить с подпространством $V$).
2.  **Обращение включения:** Если $U_1 \subseteq U_2$ — подпространства $V$, то $U_2^\circ \subseteq U_1^\circ$ — подпространства $V^*$.
    *   **Доказательство:** Пусть $\omega \in U_2^\circ$. Тогда $\omega(u)=0$ для всех $u \in U_2$. Поскольку $U_1 \subseteq U_2$, то $\omega(u)=0$ для всех $u \in U_1$. Следовательно, $\omega \in U_1^\circ$.
3.  **Аннулятор суммы — пересечение аннуляторов:** $(U_1+U_2)^\circ = U_1^\circ \cap U_2^\circ$.
    *   **Доказательство:** $\omega \in (U_1+U_2)^\circ \iff \omega(u_1+u_2)=0$ для всех $u_1 \in U_1, u_2 \in U_2$. Это эквивалентно тому, что $\omega(u_1)=0$ для всех $u_1 \in U_1$ и $\omega(u_2)=0$ для всех $u_2 \in U_2$. А это эквивалентно $\omega \in U_1^\circ$ и $\omega \in U_2^\circ$, то есть $\omega \in U_1^\circ \cap U_2^\circ$.
4.  **Аннулятор пересечения — сумма аннуляторов:** $(U_1 \cap U_2)^\circ = U_1^\circ + U_2^\circ$.
    *   **Доказательство:** Используем свойство 3 и теорему о втором аннуляторе:
        $(U_1^\circ + U_2^\circ)^\circ = (U_1^\circ)^\circ \cap (U_2^\circ)^\circ = U_1 \cap U_2$.
        Применяя аннулятор к обеим частям: $((U_1^\circ + U_2^\circ)^\circ)^\circ = (U_1 \cap U_2)^\circ$.
        По теореме о втором аннуляторе, левая часть равна $U_1^\circ + U_2^\circ$.
        Следовательно, $U_1^\circ + U_2^\circ = (U_1 \cap U_2)^\circ$.

#### 11.4. Критерий базисности системы линейных функций

**Формулировка:**
Пусть $V$ — конечномерное векторное пространство размерности $n$. Система из $n$ линейных функционалов $\omega_1, \dots, \omega_n \in V^*$ является базисом пространства $V^*$ тогда и только тогда, когда пересечение ядер этих функционалов является нулевым подпространством:
$\bigcap_{i=1}^n \text{Ker}(\omega_i) = \{\mathbf{0}\}$.

**Доказательство:**
1.  **($\implies$)** Пусть $\{\omega_1, \dots, \omega_n\}$ — базис $V^*$. Предположим, $v \in \bigcap_{i=1}^n \text{Ker}(\omega_i)$. Это означает, что $\omega_i(v) = 0$ для всех $i=1, \dots, n$.
    Пусть $v \neq \mathbf{0}$. Тогда по теореме о дополнении линейно независимой системы до базиса, $v$ можно дополнить до базиса $V$. Пусть $e_1=v$. Тогда существует функционал $e_1^* \in V^*$ из сопряжённого базиса, такой что $e_1^*(v)=1$.
    Поскольку $\{\omega_1, \dots, \omega_n\}$ — базис $V^*$, $e_1^*$ может быть выражен как линейная комбинация: $e_1^* = \sum_{j=1}^n \lambda_j \omega_j$.
    Тогда $e_1^*(v) = (\sum_{j=1}^n \lambda_j \omega_j)(v) = \sum_{j=1}^n \lambda_j \omega_j(v) = \sum_{j=1}^n \lambda_j \cdot 0 = 0$.
    Но мы знаем, что $e_1^*(v)=1$. Получаем $1=0$, что является противоречием.
    Следовательно, наше предположение $v \neq \mathbf{0}$ неверно, и $v$ должно быть $\mathbf{0}$.
    Таким образом, $\bigcap_{i=1}^n \text{Ker}(\omega_i) = \{\mathbf{0}\}$.

2.  **($\impliedby$)** Пусть $\bigcap_{i=1}^n \text{Ker}(\omega_i) = \{\mathbf{0}\}$.
    Рассмотрим линейную оболочку функционалов $W = \text{Span}(\omega_1, \dots, \omega_n)$.
    Аннулятор этой линейной оболочки $W^\circ = \{\Phi(v) \in V^{**} \mid \Phi(v)(\omega_i)=0 \text{ для всех } i\}$.
    Это эквивалентно $\{\Phi(v) \in V^{**} \mid \omega_i(v)=0 \text{ для всех } i\}$.
    По условию, $\bigcap_{i=1}^n \text{Ker}(\omega_i) = \{\mathbf{0}\}$.
    Следовательно, $W^\circ = \{\Phi(\mathbf{0})\} = \{\mathbf{0}_{V^{**}}\}$.
    По теореме о размерности аннулятора, $\dim(W^\circ) = \dim(V^*) - \dim(W)$.
    $0 = \dim(V^*) - \dim(W)$.
    $\dim(W) = \dim(V^*)$.
    Поскольку $W$ является подпространством $V^*$ и имеет ту же размерность, что и $V^*$, то $W = V^*$.
    Так как $W = \text{Span}(\omega_1, \dots, \omega_n)$, это означает, что $\{\omega_1, \dots, \omega_n\}$ порождает $V^*$.
    Поскольку это система из $n$ векторов, порождающая $n$-мерное пространство, она также линейно независима и, следовательно, является базисом $V^*$.

---

### Билет 12. Сопряжённый гомоморфизм и его свойства. Смысл транспонирования матрицы. Связь между ядрами и образами сопряжённых гомоморфизмов. Ранг матрицы и его свойства. Теорема Кронекера-Капелли.

#### 12.1. Сопряжённый гомоморфизм и его свойства

Пусть $A: U \to V$ — гомоморфизм векторных пространств над полем $F$.
Пусть $U^*$ и $V^*$ — их сопряжённые пространства.

**Определение сопряжённого гомоморфизма:**
**Сопряжённый гомоморфизм** (или двойственный гомоморфизм) $A^*: V^* \to U^*$ — это отображение, которое каждому линейному функционалу $\omega \in V^*$ ставит в соответствие линейный функционал $A^*(\omega) \in U^*$, определяемый композицией:
$(A^*(\omega))(u) = \omega(A(u))$ для всех $u \in U$.
Это определение часто записывается с использованием канонического спаривания: $\langle A^*\omega, u \rangle = \langle \omega, Au \rangle$.

**Свойства сопряжённого гомоморфизма:**
1.  **Линейность:** Отображение $A^*$ само является гомоморфизмом.
    *   **Доказательство:**
        *   Аддитивность: $(A^*(\omega_1+\omega_2))(u) = (\omega_1+\omega_2)(A(u)) = \omega_1(A(u))+\omega_2(A(u)) = (A^*(\omega_1))(u)+(A^*(\omega_2))(u) = (A^*(\omega_1)+A^*(\omega_2))(u)$.
            Следовательно, $A^*(\omega_1+\omega_2) = A^*(\omega_1)+A^*(\omega_2)$.
        *   Однородность: $(A^*(\lambda \omega))(u) = (\lambda \omega)(A(u)) = \lambda \omega(A(u)) = \lambda (A^*(\omega))(u) = (\lambda A^*(\omega))(u)$.
            Следовательно, $A^*(\lambda \omega) = \lambda A^*(\omega)$.
2.  **Сумма и умножение на скаляр:**
    *   $(A+B)^* = A^*+B^*$
    *   $(\lambda A)^* = \lambda A^*$
3.  **Композиция (произведение):** Если $A: V \to W$ и $B: U \to V$ — гомоморфизмы, то сопряжённый к композиции равен композиции сопряжённых в обратном порядке:
    $(A \circ B)^* = B^* \circ A^*$.
    *   **Доказательство:** Для $\omega \in W^*$ и $u \in U$:
        $((A \circ B)^*(\omega))(u) = \omega((A \circ B)(u)) = \omega(A(B(u)))$.
        $(B^* \circ A^*)(\omega)(u) = B^*(A^*(\omega))(u) = (A^*(\omega))(B(u)) = \omega(A(B(u)))$.
        Поскольку они совпадают для всех $\omega$ и $u$, то $(A \circ B)^* = B^* \circ A^*$.
4.  **Второй сопряжённый гомоморфизм:** Для конечномерных пространств, второй сопряжённый гомоморфизм совпадает с исходным (с учётом канонического изоморфизма $V \cong V^{**}$):
    $A^{**} = A$.

#### 12.2. Смысл транспонирования матрицы

**Теорема:**
Пусть $A: U \to V$ — гомоморфизм, $E$ и $F$ — базисы в $U$ и $V$ соответственно, и $A_{F,E}$ — матрица гомоморфизма $A$ в этих базисах.
Пусть $E^*$ и $F^*$ — сопряжённые базисы в $U^*$ и $V^*$ соответственно.
Тогда матрица сопряжённого гомоморфизма $A^*: V^* \to U^*$ в сопряжённых базисах $F^*$ и $E^*$ является транспонированной матрицей исходного гомоморфизма:
$M(A^*)_{E^*, F^*} = (M(A)_{F,E})^T$.

**Доказательство (и смысл):**
Пусть $M(A)_{F,E} = (a_{ij})$. Это означает $A(e_j) = \sum_{i=1}^m a_{ij} f_i$.
Пусть $M(A^*)_{E^*, F^*} = (b_{ji})$. Это означает $A^*(f_i^*) = \sum_{j=1}^n b_{ji} e_j^*$.
Мы хотим показать, что $b_{ji} = a_{ij}$.

По определению сопряжённого базиса, $e_k^*(e_l) = \delta_{kl}$ и $f_k^*(f_l) = \delta_{kl}$.
Рассмотрим действие $A^*(f_i^*)$ на $e_j$:
$(A^*(f_i^*))(e_j) = f_i^*(A(e_j))$ (по определению $A^*$).
Подставим разложение $A(e_j)$:
$f_i^*(A(e_j)) = f_i^*(\sum_{k=1}^m a_{kj} f_k) = \sum_{k=1}^m a_{kj} f_i^*(f_k) = \sum_{k=1}^m a_{kj} \delta_{ik} = a_{ij}$.

С другой стороны, $A^*(f_i^*)$ — это функционал в $U^*$. Его разложение по базису $E^*$ есть:
$A^*(f_i^*) = \sum_{k=1}^n b_{ki} e_k^*$.
Применим этот функционал к $e_j$:
$(\sum_{k=1}^n b_{ki} e_k^*)(e_j) = \sum_{k=1}^n b_{ki} e_k^*(e_j) = \sum_{k=1}^n b_{ki} \delta_{kj} = b_{ji}$.

Сравнивая два выражения для $(A^*(f_i^*))(e_j)$, получаем $b_{ji} = a_{ij}$.
Это означает, что элемент в $j$-й строке и $i$-м столбце матрицы $M(A^*)_{E^*, F^*}$ равен элементу в $i$-й строке и $j$-м столбце матрицы $M(A)_{F,E}$. То есть, $M(A^*)_{E^*, F^*} = (M(A)_{F,E})^T$.

**Смысл транспонирования:** Транспонирование матрицы соответствует переходу к сопряжённому гомоморфизму и смене базисов на сопряжённые. Оно "меняет местами" роли строк и столбцов, что отражает дуальность между векторами и функционалами.

#### 12.3. Связь между ядрами и образами сопряжённых гомоморфизмов

Пусть $A: U \to V$ — гомоморфизм конечномерных векторных пространств.
1.  **Ядро сопряжённого — аннулятор образа:**
    $\text{Ker}(A^*) = (\text{Im}(A))^\circ$.
    *   **Доказательство:**
        *   **($\subseteq$)** Пусть $\omega \in \text{Ker}(A^*)$. Тогда $A^*(\omega) = \mathbf{0}_{U^*}$. Это означает $(A^*(\omega))(u) = 0$ для всех $u \in U$. По определению $A^*$, $\omega(A(u)) = 0$ для всех $u \in U$. Это означает, что $\omega$ обращается в нуль на всех векторах из образа $\text{Im}(A)$. Следовательно, $\omega \in (\text{Im}(A))^\circ$.
        *   **($\supseteq$)** Пусть $\omega \in (\text{Im}(A))^\circ$. Тогда $\omega(v) = 0$ для всех $v \in \text{Im}(A)$. В частности, для любого $u \in U$, $A(u) \in \text{Im}(A)$, поэтому $\omega(A(u)) = 0$. По определению $A^*$, $(A^*(\omega))(u) = 0$ для всех $u \in U$. Следовательно, $A^*(\omega) = \mathbf{0}_{U^*}$, что означает $\omega \in \text{Ker}(A^*)$.

2.  **Образ сопряжённого — аннулятор ядра:**
    $\text{Im}(A^*) = (\text{Ker}(A))^\circ$.
    *   **Доказательство:**
        *   **($\subseteq$)** Пусть $\phi \in \text{Im}(A^*)$. Тогда $\phi = A^*(\omega)$ для некоторого $\omega \in V^*$. Мы хотим показать, что $\phi(u) = 0$ для всех $u \in \text{Ker}(A)$.
            Если $u \in \text{Ker}(A)$, то $A(u) = \mathbf{0}_V$.
            Тогда $\phi(u) = (A^*(\omega))(u) = \omega(A(u)) = \omega(\mathbf{0}_V) = 0$.
            Следовательно, $\phi \in (\text{Ker}(A))^\circ$.
        *   **($\supseteq$)** Используем равенство размерностей.
            $\dim(\text{Im}(A^*)) = \dim(U^*) - \dim(\text{Ker}(A^*))$ (по теореме о ранге и дефекте для $A^*$).
            $\dim(\text{Ker}(A^*)) = \dim(V) - \dim(\text{Im}(A))$ (по свойству 1 и теореме о размерности аннулятора).
            $\dim(\text{Im}(A^*)) = \dim(U) - (\dim(V) - \dim(\text{Im}(A)))$.
            По теореме о ранге и дефекте для $A$: $\dim(\text{Im}(A)) = \dim(U) - \dim(\text{Ker}(A))$.
            $\dim(\text{Im}(A^*)) = \dim(U) - \dim(V) + (\dim(U) - \dim(\text{Ker}(A)))$.
            Это неверно. Давайте используем более прямой путь.
            $\dim(\text{Im}(A^*)) = \dim(U) - \dim(\text{Ker}(A^*))$ (по теореме о ранге и дефекте для $A^*$).
            $\dim(\text{Ker}(A^*)) = \dim((\text{Im}(A))^\circ) = \dim(V) - \dim(\text{Im}(A))$ (по свойству 1 и теореме о размерности аннулятора).
            $\dim(\text{Im}(A^*)) = \dim(U) - (\dim(V) - \dim(\text{Im}(A)))$.
            По теореме о ранге и дефекте для $A$: $\dim(\text{Im}(A)) = \dim(U) - \dim(\text{Ker}(A))$.
            $\dim(\text{Im}(A^*)) = \dim(U) - \dim(V) + \dim(U) - \dim(\text{Ker}(A))$.
            Это не упрощается до $\dim((\text{Ker}(A))^\circ)$.
            Давайте используем $\dim((\text{Ker}(A))^\circ) = \dim(U) - \dim(\text{Ker}(A))$.
            Мы знаем, что $\dim(\text{Im}(A^*)) = \text{rank}(A^*)$ и $\text{rank}(A^*) = \text{rank}(A)$.
            $\text{rank}(A) = \dim(\text{Im}(A)) = \dim(U) - \dim(\text{Ker}(A))$.
            Таким образом, $\dim(\text{Im}(A^*)) = \dim(U) - \dim(\text{Ker}(A)) = \dim((\text{Ker}(A))^\circ)$.
            Поскольку $\text{Im}(A^*)$ является подпространством $(\text{Ker}(A))^\circ$ (доказано выше) и их размерности совпадают, они должны быть равны.

#### 12.4. Ранг матрицы и его свойства

**Определение ранга матрицы:**
**Рангом матрицы** $A$ (обозначается $\text{rank}(A)$) называется максимальное число линейно независимых строк (строчный ранг) или столбцов (столбцовый ранг) этой матрицы.

**Теорема о совпадении рангов:**
Размерность линейной оболочки системы столбцов некоторой матрицы совпадает с размерностью линейной оболочки системы строк этой же матрицы. Эта общая размерность и называется рангом матрицы.

**Доказательство (с использованием гомоморфизмов и дуальности):**
1.  Рассмотрим матрицу $A$ как матрицу гомоморфизма $A: U \to V$. Столбцовый ранг $A$ равен $\dim(\text{Im}(A))$.
2.  Рассмотрим сопряжённый гомоморфизм $A^*: V^* \to U^*$. Его матрица в сопряжённых базисах является транспонированной матрицей $A^T$.
3.  Строчный ранг $A$ равен столбцовому рангу $A^T$, который равен $\dim(\text{Im}(A^*))$.
4.  По теореме о ранге и дефекте для $A^*$: $\dim(\text{Im}(A^*)) = \dim(V^*) - \dim(\text{Ker}(A^*))$.
5.  По свойству 1 (ядро сопряжённого — аннулятор образа): $\text{Ker}(A^*) = (\text{Im}(A))^\circ$.
6.  По теореме о размерности аннулятора: $\dim((\text{Im}(A))^\circ) = \dim(V) - \dim(\text{Im}(A))$.
7.  Подставляем: $\dim(\text{Im}(A^*)) = \dim(V^*) - (\dim(V) - \dim(\text{Im}(A)))$.
8.  Так как $\dim(V^*) = \dim(V)$ для конечномерных пространств: $\dim(\text{Im}(A^*)) = \dim(V) - \dim(V) + \dim(\text{Im}(A)) = \dim(\text{Im}(A))$.
Таким образом, столбцовый ранг $A$ равен строчному рангу $A$.

**Свойства ранга:**
*   $\text{rank}(A) = \text{rank}(A^T)$.
*   $\text{rank}(A) \le \min(m, n)$ для матрицы $m \times n$.
*   $\text{rank}(A) = 0 \iff A = O$ (нулевая матрица).
*   $\text{rank}(A) = n$ для квадратной матрицы $n \times n \iff A$ невырождена.
*   $\text{rank}(AB) \le \min(\text{rank}(A), \text{rank}(B))$.
*   $\text{rank}(A+B) \le \text{rank}(A) + \text{rank}(B)$.

#### 12.5. Теорема Кронекера-Капелли

**Формулировка:**
Система линейных алгебраических уравнений (СЛАУ) $A\mathbf{x} = \mathbf{b}$ совместно (т.е. имеет хотя бы одно решение) тогда и только тогда, когда ранг матрицы коэффициентов $A$ равен рангу расширенной матрицы $(A|\mathbf{b})$.
$\text{rank}(A) = \text{rank}(A|\mathbf{b})$.

**Доказательство:**
Пусть $\mathbf{a}_1, \dots, \mathbf{a}_n$ — столбцы матрицы $A$.
1.  **($\implies$) Если СЛАУ совместно:**
    Существует решение $\mathbf{x} = (x_1, \dots, x_n)^T$ такое, что $x_1 \mathbf{a}_1 + \dots + x_n \mathbf{a}_n = \mathbf{b}$.
    Это означает, что вектор $\mathbf{b}$ является линейной комбинацией столбцов матрицы $A$.
    Следовательно, $\mathbf{b}$ лежит в линейной оболочке столбцов $A$, т.е. $\mathbf{b} \in \text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n)$.
    Добавление вектора, который уже лежит в линейной оболочке, не меняет размерность этой оболочки.
    Поэтому $\text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n) = \text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n, \mathbf{b})$.
    Размерность линейной оболочки столбцов — это столбцовый ранг.
    Следовательно, $\text{rank}(A) = \text{rank}(A|\mathbf{b})$.

2.  **($\impliedby$) Если ранги совпадают:**
    Пусть $\text{rank}(A) = \text{rank}(A|\mathbf{b})$.
    Это означает, что $\dim(\text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n)) = \dim(\text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n, \mathbf{b}))$.
    Поскольку $\text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n)$ является подпространством $\text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n, \mathbf{b})$, и их размерности равны, эти подпространства должны совпадать:
    $\text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n) = \text{Span}(\mathbf{a}_1, \dots, \mathbf{a}_n, \mathbf{b})$.
    Это означает, что вектор $\mathbf{b}$ лежит в линейной оболочке столбцов матрицы $A$.
    Следовательно, $\mathbf{b}$ может быть выражен как линейная комбинация столбцов $A$:
    $\mathbf{b} = x_1 \mathbf{a}_1 + \dots + x_n \mathbf{a}_n$.
    Это в точности означает, что существует решение $\mathbf{x} = (x_1, \dots, x_n)^T$ для СЛАУ $A\mathbf{x} = \mathbf{b}$.
    Таким образом, СЛАУ совместна.

---

### Билет 13. Точная последовательность гомоморфизмов векторных пространств: определение и примеры. Теорема о сохранении точности при переходе к сопряжённым объектам.

#### 13.1. Точная последовательность гомоморфизмов векторных пространств: определение и примеры

**Определение точной последовательности:**
Последовательность гомоморфизмов векторных пространств
$\dots \xrightarrow{A_{i-1}} V_{i-1} \xrightarrow{A_i} V_i \xrightarrow{A_{i+1}} V_{i+1} \xrightarrow{A_{i+2}} \dots$
называется **точной в члене $V_i$**, если образ гомоморфизма, входящего в $V_i$, совпадает с ядром гомоморфизма, выходящего из $V_i$:
$\text{Im}(A_i) = \text{Ker}(A_{i+1})$.
Последовательность называется **точной**, если она точна в каждом своём члене.

**Примеры коротких точных последовательностей (из трёх членов):**
1.  **Эпиморфизм (отображение "на"):**
    $U \xrightarrow{A} V \to \mathbf{0}$
    Эта последовательность точна в $V$, если $\text{Im}(A) = \text{Ker}(V \to \mathbf{0})$. Ядро гомоморфизма из $V$ в нулевое пространство $\mathbf{0}$ — это всё пространство $V$. Следовательно, $\text{Im}(A) = V$, что означает, что $A$ является сюръективным гомоморфизмом (эпиморфизмом).

2.  **Мономорфизм (инъективное отображение):**
    $\mathbf{0} \to U \xrightarrow{B} V$
    Эта последовательность точна в $U$, если $\text{Im}(\mathbf{0} \to U) = \text{Ker}(B)$. Образ гомоморфизма из нулевого пространства в $U$ — это только нулевой вектор $\{\mathbf{0}_U\}$. Следовательно, $\text{Ker}(B) = \{\mathbf{0}_U\}$, что означает, что $B$ является инъективным гомоморфизмом (мономорфизмом).

3.  **Короткая точная последовательность:**
    $\mathbf{0} \to U \xrightarrow{A} V \xrightarrow{B} W \to \mathbf{0}$
    Эта последовательность точна, если:
    *   Точна в $U$: $\text{Ker}(A) = \{\mathbf{0}_U\}$ (A — мономорфизм).
    *   Точна в $V$: $\text{Im}(A) = \text{Ker}(B)$.
    *   Точна в $W$: $\text{Im}(B) = W$ (B — эпиморфизм).
    Такая последовательность означает, что $U$ изоморфно подпространству $\text{Im}(A)$ в $V$, и $W$ изоморфно факторпространству $V/\text{Im}(A) = V/\text{Ker}(B)$.

**Конструкция точной последовательности (пятичленная):**
Для любого гомоморфизма $A: U \to V$ можно построить следующую точную последовательность:
$\mathbf{0} \to \text{Ker}(A) \xrightarrow{i} U \xrightarrow{A} V \xrightarrow{p} \text{Coker}(A) \to \mathbf{0}$
где:
*   $i: \text{Ker}(A) \to U$ — это вложение (инъекция) ядра в $U$.
*   $\text{Coker}(A) = V/\text{Im}(A)$ — коядро гомоморфизма $A$.
*   $p: V \to \text{Coker}(A)$ — это каноническая проекция на факторпространство.

**Проверка точности:**
*   В $\text{Ker}(A)$: $\text{Im}(\mathbf{0} \to \text{Ker}(A)) = \{\mathbf{0}\} = \text{Ker}(i)$ (так как $i$ — вложение).
*   В $U$: $\text{Im}(i) = \text{Ker}(A)$.
*   В $V$: $\text{Im}(A) = \text{Ker}(p)$ (по определению проекции на коядро).
*   В $\text{Coker}(A)$: $\text{Im}(p) = \text{Coker}(A) = \text{Ker}(\text{Coker}(A) \to \mathbf{0})$.

#### 13.2. Теорема о сохранении точности при переходе к сопряжённым объектам

**Формулировка:**
При переходе к сопряжённым пространствам и сопряжённым гомоморфизмам (дуальным объектам) точность последовательности сохраняется, но направление стрелок меняется на обратное.
Если последовательность гомоморфизмов конечномерных векторных пространств $U \xrightarrow{A} V \xrightarrow{B} W$ точна в $V$ (т.е. $\text{Im}(A) = \text{Ker}(B)$), то сопряжённая последовательность $W^* \xrightarrow{B^*} V^* \xrightarrow{A^*} U^*$ также точна в $V^*$ (т.е. $\text{Im}(B^*) = \text{Ker}(A^*)$).

**Доказательство сохранения точности в $V^*$:**
Нам необходимо доказать, что $\text{Im}(B^*) = \text{Ker}(A^*)$.

1.  **Покажем, что $\text{Im}(B^*) \subseteq \text{Ker}(A^*)$:**
    Пусть $\eta \in \text{Im}(B^*)$. Это означает, что $\eta = B^*(\omega)$ для некоторого $\omega \in W^*$.
    Чтобы показать, что $\eta \in \text{Ker}(A^*)$, нужно доказать, что $A^*(\eta) = \mathbf{0}_{U^*}$ (нулевой функционал на $U$).
    Применим $A^*(\eta)$ к произвольному вектору $u \in U$:
    $\langle A^*(\eta), u \rangle = \langle \eta, A(u) \rangle$ (по определению $A^*$).
    Подставим $\eta = B^*(\omega)$:
    $\langle B^*(\omega), A(u) \rangle = \langle \omega, B(A(u)) \rangle$ (по определению $B^*$).
    Поскольку исходная последовательность точна в $V$, мы имеем $\text{Im}(A) = \text{Ker}(B)$.
    Вектор $A(u)$ лежит в образе $A$, следовательно, он лежит в ядре $B$.
    Поэтому $B(A(u)) = \mathbf{0}_W$.
    Тогда $\langle \omega, B(A(u)) \rangle = \langle \omega, \mathbf{0}_W \rangle = 0$.
    Так как $A^*(\eta)$ переводит произвольный вектор $u$ в ноль, $A^*(\eta)$ — нулевой функционал.
    Следовательно, $\eta \in \text{Ker}(A^*)$. Включение доказано.

2.  **Покажем, что $\dim(\text{Im}(B^*)) = \dim(\text{Ker}(A^*))$ (для конечномерных пространств):**
    *   Мы знаем, что $\text{Ker}(A^*) = (\text{Im}(A))^\circ$ (Билет 12.3, свойство 1).
    *   По теореме о размерности аннулятора (Билет 11.2): $\dim(\text{Ker}(A^*)) = \dim((\text{Im}(A))^\circ) = \dim(V) - \dim(\text{Im}(A))$.
    *   Так как исходная последовательность точна, $\text{Im}(A) = \text{Ker}(B)$.
    *   Следовательно, $\dim(\text{Ker}(A^*)) = \dim(V) - \dim(\text{Ker}(B))$.
    *   По теореме о ранге и дефекте для $B$ (Билет 6.3): $\dim(V) - \dim(\text{Ker}(B)) = \dim(\text{Im}(B))$.
    *   Таким образом, $\dim(\text{Ker}(A^*)) = \dim(\text{Im}(B))$.
    *   Мы также знаем, что $\text{rank}(B^*) = \text{rank}(B)$ (Билет 12.4, свойство ранга).
    *   Следовательно, $\dim(\text{Im}(B^*)) = \dim(\text{Im}(B))$.
    *   Из этого следует, что $\dim(\text{Im}(B^*)) = \dim(\text{Ker}(A^*))$.

Поскольку $\text{Im}(B^*)$ является подпространством $\text{Ker}(A^*)$ и их размерности совпадают, они должны быть равны: $\text{Im}(B^*) = \text{Ker}(A^*)$.
Таким образом, точность последовательности сохраняется при дуализации.

---

### Билет 14. Аффинные пространства: определение и примеры. Аффинные плоскости. Аффинизация векторного пространства и векторизация аффинного. Взаимное расположение аффинных плоскостей: пересечение, скрещивание, параллельность. Репер в аффинном пространстве, координаты точки. Изменение координат точки при замене репера.

#### 14.1. Аффинные пространства: определение и примеры

**Определение:**
**Аффинное пространство** $S$ — это непустое множество элементов, называемых **точками**, ассоциированное с некоторым векторным пространством $V$ над полем $F$. Для аффинного пространства определена операция **сложения точки с вектором** (или параллельного переноса/сдвига): $S \times V \to S$, обозначаемая $A+v=B$, где $A, B \in S, v \in V$.

Эта операция должна удовлетворять следующим аксиомам:
1.  **Ассоциативность для векторов:** $(A + v_1) + v_2 = A + (v_1 + v_2)$ для всех $A \in S$ и $v_1, v_2 \in V$.
2.  **Действие нулевого вектора:** $A + \mathbf{0} = A$ для всех $A \in S$, где $\mathbf{0}$ — нулевой вектор из $V$.
3.  **Единственность соединяющего вектора:** Для любых двух точек $A, B \in S$ существует единственный вектор $v \in V$ такой, что $A + v = B$. Этот вектор обозначается $\vec{AB}$.

**Следствия из аксиом:**
*   **Правило треугольника:** Для любых трёх точек $A, B, C \in S$, $\vec{AB} + \vec{BC} = \vec{AC}$.
*   **Противоположный вектор:** $\vec{AB} = -\vec{BA}$.

**Размерность аффинного пространства:**
Размерностью аффинного пространства $S$ называется размерность ассоциированного с ним векторного пространства $V$: $\dim(S) = \dim(V)$.

**Примеры аффинных пространств:**
*   **Геометрическое пространство:** Привычное евклидово пространство, где точки — это точки, а векторы — это направленные отрезки.
*   **Аффинное координатное пространство $F^N$:** Множество $F^N$ можно рассматривать как аффинное пространство, ассоциированное с векторным пространством $F^N$. Операция сложения точки с вектором — это обычное сложение векторов.
*   **Любое векторное пространство $V$:** Само векторное пространство $V$ может быть рассмотрено как аффинное пространство, ассоциированное с $V$. Точки — это векторы из $V$, а операция сложения точки с вектором — это обычное сложение векторов.
*   **Линейное многообразие:** Множество решений неоднородной системы линейных алгебраических уравнений $A\mathbf{x} = \mathbf{b}$ является аффинным пространством. Оно имеет вид $\mathbf{x}_0 + \text{Ker}(A)$, где $\mathbf{x}_0$ — частное решение, а $\text{Ker}(A)$ — ассоциированное векторное пространство.
*   **Множество нормированных многочленов:** Например, многочлены степени не выше 2 со старшим коэффициентом 1 ($x^2+px+q$). Это не векторное пространство, но аффинное пространство, ассоциированное с пространством многочленов степени не выше 1 ($px+q$).

#### 14.2. Аффинные плоскости

**Определение:**
**Аффинная плоскость** (или аффинное подпространство) в аффинном пространстве $S$ — это непустое подмножество $P \subseteq S$, которое может быть записано в виде:
$P = A_0 + U = \{A_0 + u \mid u \in U\}$,
где $A_0 \in S$ — фиксированная **опорная точка** (любая точка, лежащая на плоскости), а $U$ — **направляющее подпространство** (подпространство ассоциированного векторного пространства $V$).

**Размерность аффинной плоскости:**
Размерность аффинной плоскости $P$ определяется как размерность её направляющего подпространства $U$: $\dim(P) = \dim(U)$.

**Частные случаи аффинных плоскостей:**
*   **Точка:** Аффинная плоскость размерности 0 (направляющее подпространство $\{\mathbf{0}\}$).
*   **Прямая:** Аффинная плоскость размерности 1 (направляющее подпространство одномерно, порождено одним ненулевым направляющим вектором $u$). Параметрическое задание: $X = A_0 + t \cdot u$.
*   **Плоскость:** Аффинная плоскость размерности 2.
*   **Гиперплоскость:** Аффинная плоскость размерности $\dim(S)-1$ (коразмерности 1). В координатах задается одним линейным неоднородным уравнением: $a_1 x_1 + \dots + a_n x_n = b$.

#### 14.3. Аффинизация векторного пространства и векторизация аффинного

**Аффинизация векторного пространства:**
Любое векторное пространство $V$ может быть естественным образом превращено в аффинное пространство. Для этого:
*   Множество точек $S$ отождествляется с самим $V$.
*   Ассоциированное векторное пространство также берется как $V$.
*   Операция сложения точки с вектором определяется как обычное сложение векторов в $V$.
Все аксиомы аффинного пространства при этом выполняются.

**Векторизация аффинного пространства:**
Аффинное пространство $S$ можно превратить в векторное пространство, выбрав в нём произвольную точку $O \in S$ в качестве **начала координат**.
Тогда каждому вектору $v \in V$ можно сопоставить точку $A = O+v$.
И наоборот, каждой точке $A \in S$ можно сопоставить вектор $\vec{OA} \in V$.
Это устанавливает биекцию между $S$ и $V$. При этом $S$ становится векторным пространством с началом в $O$, где сложение точек $A, B$ определяется как $O + (\vec{OA} + \vec{OB})$, а умножение на скаляр $\lambda A$ как $O + \lambda \vec{OA}$.
Однако, это векторизованное пространство зависит от выбора начала координат $O$. Канонического способа векторизации нет.

#### 14.4. Взаимное расположение аффинных плоскостей: пересечение, скрещивание, параллельность

Пусть даны две аффинные плоскости $P_1 = A_1 + U_1$ и $P_2 = A_2 + U_2$, где $U_1, U_2$ — их направляющие подпространства.

1.  **Пересекающиеся:**
    Плоскости $P_1$ и $P_2$ **пересекаются**, если их пересечение непусто: $P_1 \cap P_2 \neq \emptyset$.
    Если они пересекаются, то их пересечение само является аффинной плоскостью: $P_1 \cap P_2 = P_0 + (U_1 \cap U_2)$, где $P_0$ — любая точка пересечения, а $U_1 \cap U_2$ — пересечение их направляющих подпространств.
    Размерность пересечения: $\dim(P_1 \cap P_2) = \dim(U_1 \cap U_2)$.

2.  **Параллельные:**
    Плоскости $P_1$ и $P_2$ **параллельны**, если одно из их направляющих подпространств содержится в другом: $U_1 \subseteq U_2$ или $U_2 \subseteq U_1$.
    *   Если $U_1 = U_2$, то плоскости называются **строго параллельными**.
    *   Если $P_1 \cap P_2 \neq \emptyset$ и они параллельны, то одна плоскость содержится в другой (например, если $U_1 \subseteq U_2$, то $P_1 \subseteq P_2$).
    *   Если $P_1 \cap P_2 = \emptyset$ и они параллельны, то они не имеют общих точек.

3.  **Скрещивающиеся:**
    Плоскости $P_1$ и $P_2$ **скрещиваются**, если их пересечение пусто ($P_1 \cap P_2 = \emptyset$) и они не параллельны (т.е. $U_1 \not\subseteq U_2$ и $U_2 \not\subseteq U_1$).
    Это определение отличается от школьного для прямых в 3D, поскольку оно применимо к плоскостям любой размерности.

#### 14.5. Репер в аффинном пространстве, координаты точки

**Определение репера:**
Для введения координат в $n$-мерном аффинном пространстве $S$ необходим **репер** (или аффинная система координат). Репер состоит из:
1.  Выделенной точки $O \in S$, называемой **началом координат**.
2.  Базиса $E = \{e_1, \dots, e_n\}$ ассоциированного векторного пространства $V$.
Репер обозначается $(O; E)$.

**Координаты точки:**
Координатами точки $A \in S$ в репере $(O; E)$ называются координаты вектора $\vec{OA}$ в базисе $E$.
Если $\vec{OA} = x_1 e_1 + \dots + x_n e_n$, то столбец $X = (x_1, \dots, x_n)^T$ является столбцом координат точки $A$.
Начало координат $O$ имеет нулевые координаты.

#### 14.6. Изменение координат точки при замене репера

Пусть в аффинном пространстве $S$ заданы два репера:
*   "Старый" репер: $(O; E)$, где $E = \{e_1, \dots, e_n\}$.
*   "Новый" репер: $(\tilde{O}; \tilde{E})$, где $\tilde{E} = \{\tilde{e}_1, \dots, \tilde{e}_n\}$.

Пусть $X$ — столбец старых координат точки $A$ в репере $(O; E)$.
Пусть $\tilde{X}$ — столбец новых координат точки $A$ в репере $(\tilde{O}; \tilde{E})$.

Пусть $X_0$ — столбец старых координат нового начала координат $\tilde{O}$ в репере $(O; E)$. То есть $X_0$ — это координаты вектора $\vec{O\tilde{O}}$ в базисе $E$.
Пусть $C = C_{E \to \tilde{E}}$ — матрица перехода от старого базиса $E$ к новому базису $\tilde{E}$.

Вектор $\vec{OA}$ (который дает координаты $X$) можно представить, используя правило треугольника:
$\vec{OA} = \vec{O\tilde{O}} + \vec{\tilde{O}A}$.

Переведем это векторное равенство в координатную форму:
*   Координаты $\vec{OA}$ в базисе $E$ — это $X$.
*   Координаты $\vec{O\tilde{O}}$ в базисе $E$ — это $X_0$.
*   Координаты $\vec{\tilde{O}A}$ в базисе $\tilde{E}$ — это $\tilde{X}$.
*   Чтобы получить координаты $\vec{\tilde{O}A}$ в базисе $E$, нужно умножить $\tilde{X}$ на матрицу перехода $C_{E \to \tilde{E}}$ (Билет 5.2): $C \tilde{X}$.

Таким образом, получаем формулу, связывающую старые координаты с новыми:
$X = X_0 + C \tilde{X}$.

Чтобы выразить новые координаты через старые, перенесем $X_0$ и умножим на $C^{-1}$:
$C \tilde{X} = X - X_0$
$\tilde{X} = C^{-1} (X - X_0)$.

Эта формула описывает общее преобразование координат точки при замене репера, включающее как сдвиг начала координат, так и поворот/масштабирование базисных векторов.

---

### Билет 15. Барицентрическая комбинация точек. Материальные точки, их центр масс и материальный центр. Теорема о группировании масс.

#### 15.1. Барицентрическая комбинация точек

В аффинном пространстве точки нельзя просто складывать или умножать на скаляры, как векторы. Однако можно определить специальные линейные комбинации, называемые барицентрическими.

**Определение:**
Пусть $A_1, \dots, A_k$ — точки аффинного пространства $S$, и $\lambda_1, \dots, \lambda_k$ — скаляры из поля $F$. **Барицентрической комбинацией** этих точек с коэффициентами $\lambda_i$ называется точка $A$, определяемая вектором $\vec{OA}$ из произвольного начала отсчета $O \in S$ по формуле:
$\vec{OA} = \lambda_1 \vec{OA_1} + \dots + \lambda_k \vec{OA_k}$,
при условии, что **сумма коэффициентов равна единице**: $\sum_{i=1}^k \lambda_i = 1$.

**Ключевое свойство (независимость от выбора начала отсчета):**
Результат барицентрической комбинации (точка $A$) не зависит от выбора произвольной точки $O$.

**Доказательство:**
Пусть $O'$ — другое начало отсчета. Тогда $\vec{O'A} = \lambda_1 \vec{O'A_1} + \dots + \lambda_k \vec{O'A_k}$.
Мы знаем, что $\vec{O'A_i} = \vec{O'O} + \vec{OA_i}$.
Подставим это:
$\vec{O'A} = \sum_{i=1}^k \lambda_i (\vec{O'O} + \vec{OA_i}) = \sum_{i=1}^k \lambda_i \vec{O'O} + \sum_{i=1}^k \lambda_i \vec{OA_i}$.
$\vec{O'A} = (\sum_{i=1}^k \lambda_i) \vec{O'O} + \vec{OA}$.
Поскольку $\sum_{i=1}^k \lambda_i = 1$, получаем $\vec{O'A} = \vec{O'O} + \vec{OA}$.
Это равенство означает, что $\vec{OA} = \vec{O'A} - \vec{O'O} = \vec{OO'}+\vec{O'A} = \vec{OA}$, что является правилом треугольника для векторов $\vec{O'O}, \vec{OA}, \vec{O'A}$.
Таким образом, точка $A$ определяется однозначно, независимо от выбора $O$.

#### 15.2. Материальные точки, их центр масс и материальный центр

**Определение материальной точки:**
**Материальная точка** в аффинном пространстве — это пара $(A, \lambda)$, состоящая из точки $A \in S$ и приписанной ей массы (или веса) $\lambda \in F$.

**Определение центра масс:**
**Центром масс** $C$ системы материальных точек $(A_1, \lambda_1), \dots, (A_k, \lambda_k)$ называется точка $C \in S$, если сумма всех масс не равна нулю ($\sum_{i=1}^k \lambda_i \neq 0$), и для которой выполняется условие:
$\sum_{i=1}^k \lambda_i \vec{CA_i} = \mathbf{0}$.

**Выражение центра масс через произвольное начало отсчета:**
Центр масс $C$ можно выразить через произвольное начало отсчета $O \in S$:
$\vec{OC} = \frac{\sum_{i=1}^k \lambda_i \vec{OA_i}}{\sum_{i=1}^k \lambda_i}$.

**Доказательство:**
Начнем с определения центра масс: $\sum_{i=1}^k \lambda_i \vec{CA_i} = \mathbf{0}$.
Используем правило треугольника: $\vec{CA_i} = \vec{CO} + \vec{OA_i} = -\vec{OC} + \vec{OA_i}$.
Подставим это в уравнение:
$\sum_{i=1}^k \lambda_i (-\vec{OC} + \vec{OA_i}) = \mathbf{0}$
$-\sum_{i=1}^k \lambda_i \vec{OC} + \sum_{i=1}^k \lambda_i \vec{OA_i} = \mathbf{0}$
$(\sum_{i=1}^k \lambda_i) \vec{OC} = \sum_{i=1}^k \lambda_i \vec{OA_i}$.
Поскольку $\sum \lambda_i \neq 0$, мы можем разделить:
$\vec{OC} = \frac{\sum_{i=1}^k \lambda_i \vec{OA_i}}{\sum_{i=1}^k \lambda_i}$.

**Связь с барицентрической комбинацией:**
Если обозначить $\mu_i = \frac{\lambda_i}{\sum \lambda_j}$, то $\sum \mu_i = 1$. Тогда $\vec{OC} = \sum \mu_i \vec{OA_i}$.
Это означает, что центр масс всегда является барицентрической комбинацией данных точек.

**Материальный центр:**
**Материальный центр** подсистемы материальных точек — это их центр масс, которому приписывается суммарная масса этой подсистемы. Например, для подсистемы $(A_1, \lambda_1), \dots, (A_k, \lambda_k)$, её материальный центр — это точка $Z$ (центр масс этой подсистемы) с массой $\Lambda = \sum_{i=1}^k \lambda_i$.

#### 15.3. Теорема о группировании масс

**Формулировка:**
Центр масс системы материальных точек не изменится, если некоторую её подсистему заменить на один материальный центр (точку в центре масс подсистемы с массой, равной сумме масс этой подсистемы).

**Доказательство:**
Пусть дана система из $n$ материальных точек $(A_1, \lambda_1), \dots, (A_n, \lambda_n)$. Пусть $C$ — её центр масс.
Пусть $\sum_{i=1}^n \lambda_i \neq 0$.
По определению, $\sum_{i=1}^n \lambda_i \vec{CA_i} = \mathbf{0}$.

Выделим подсистему из первых $k$ точек: $(A_1, \lambda_1), \dots, (A_k, \lambda_k)$.
Пусть $Z$ — центр масс этой подсистемы, и $\Lambda_k = \sum_{i=1}^k \lambda_i$ — её суммарная масса.
Если $\Lambda_k \neq 0$, то по определению центра масс для этой подсистемы:
$\sum_{i=1}^k \lambda_i \vec{ZA_i} = \mathbf{0}$.

Используя формулу для вектора центра масс относительно произвольного начала отсчета $C$:
$\vec{CZ} = \frac{\sum_{i=1}^k \lambda_i \vec{CA_i}}{\sum_{i=1}^k \lambda_i}$.
Отсюда выразим сумму векторов подсистемы:
$\sum_{i=1}^k \lambda_i \vec{CA_i} = (\sum_{i=1}^k \lambda_i) \vec{CZ} = \Lambda_k \vec{CZ}$.

Теперь подставим это выражение обратно в исходное уравнение для центра масс всей системы:
$\Lambda_k \vec{CZ} + \sum_{i=k+1}^n \lambda_i \vec{CA_i} = \mathbf{0}$.

Это уравнение в точности совпадает с определением центра масс для новой системы, состоящей из:
*   Одного материального центра $(Z, \Lambda_k)$ (вместо первых $k$ точек).
*   Оставшихся $n-k$ точек $(A_{k+1}, \lambda_{k+1}), \dots, (A_n, \lambda_n)$.
Суммарная масса новой системы также равна $\Lambda_k + \sum_{i=k+1}^n \lambda_i = \sum_{i=1}^n \lambda_i$, что не равно нулю.
Таким образом, центр масс всей системы не изменился. Теорема доказана.

**Геометрические приложения (Пример: медианы треугольника):**
Теорема о группировании масс позволяет легко доказать, что медианы треугольника пересекаются в одной точке и делятся в отношении 2:1.
1.  Поместим в вершины треугольника $A, B, C$ равные массы (например, по 1).
2.  Найдем центр масс всей системы. Сгруппируем точки $A$ и $B$. Их центр масс $C_1$ лежит посередине стороны $AB$ (так как массы равны) и имеет суммарную массу $1+1=2$.
3.  Теперь система состоит из точки $C$ (масса 1) и материального центра $C_1$ (масса 2). Общий центр масс $M$ должен лежать на отрезке $CC_1$ (медиане).
4.  По правилу рычага (или формуле центра масс), чтобы уравновесить массу 2 в точке $C_1$ и массу 1 в точке $C$, точка $M$ должна делить отрезок $CC_1$ в отношении 2:1, считая от вершины $C$.
5.  Поскольку центр масс всей системы единственен, он будет лежать на всех трех медианах и делить каждую в том же отношении 2:1.

---

### Билет 16. Нормальные подгруппы: определение и примеры. Сопряжённость элементов в группе. Описание сопряжённости в группе перестановок S . Критерий нормальности подгруппы. Образ и ядро гомоморфизма групп. Факторгруппы. Основная теорема о гомоморфизме групп.

**Примечание:** Этот билет относится к теории групп, а не к линейной алгебре. Я предоставлю ответы, используя общие математические знания.

#### 16.1. Нормальные подгруппы: определение и примеры

**Определение:**
Подгруппа $H$ группы $G$ называется **нормальной подгруппой** (обозначается $H \triangleleft G$), если её левые смежные классы совпадают с правыми смежными классами для любого элемента $g \in G$. То есть, $gH = Hg$ для всех $g \in G$.

**Примеры нормальных подгрупп:**
*   **Тривиальные подгруппы:** Любая группа $G$ имеет две тривиальные нормальные подгруппы: саму себя $G$ и подгруппу, состоящую только из нейтрального элемента $\{e\}$.
*   **Центр группы:** Центр группы $Z(G) = \{z \in G \mid zg = gz \text{ для всех } g \in G\}$ всегда является нормальной подгруппой.
*   **Коммутант группы:** Коммутант группы $G'$ (или $[G,G]$), порожденный всеми коммутаторами $aba^{-1}b^{-1}$, всегда является нормальной подгруппой.
*   **Специальная линейная группа $SL_n(F)$ в общей линейной группе $GL_n(F)$:** $SL_n(F) \triangleleft GL_n(F)$. Для любой матрицы $A \in SL_n(F)$ ($\det A = 1$) и любой матрицы $C \in GL_n(F)$, определитель сопряженной матрицы $\det(CAC^{-1}) = \det C \cdot \det A \cdot \det C^{-1} = \det A = 1$. Значит, $CAC^{-1} \in SL_n(F)$.
*   **Знакопеременная группа $A_n$ в симметрической группе $S_n$:** $A_n \triangleleft S_n$. Знак перестановки при сопряжении не меняется: $sgn(\pi \sigma \pi^{-1}) = sgn(\pi) sgn(\sigma) (sgn(\pi))^{-1} = sgn(\sigma)$. Если $\sigma$ четная, то и результат четный.
*   **Подгруппа индекса 2:** Если подгруппа $H$ имеет индекс 2 в группе $G$ (т.е. $|G:H|=2$), то она всегда нормальна. Это означает, что существует только два левых смежных класса ($H$ и $G \setminus H$) и два правых смежных класса ($H$ и $G \setminus H$), которые должны совпадать.

#### 16.2. Сопряжённость элементов в группе

**Определение:**
Элемент $a \in G$ называется **сопряжённым** элементу $b \in G$, если существует элемент $g \in G$ такой, что $a = gbg^{-1}$.
Отношение сопряжённости является отношением эквивалентности, которое разбивает группу на **классы сопряжённости**.

**Свойства:**
*   $e$ (нейтральный элемент) сопряжён только самому себе.
*   В абелевой группе каждый элемент сопряжён только самому себе ($gbg^{-1} = g g^{-1} b = b$).

#### 16.3. Описание сопряжённости в группе перестановок $S_n$

**Теорема:**
Две перестановки в симметрической группе $S_n$ сопряжены тогда и только тогда, когда они имеют одинаковую **цикловую структуру** (т.е. одинаковое количество циклов каждой длины в их разложении на непересекающиеся циклы).

**Пример:**
В $S_4$, перестановка $\sigma = (1 2)(3 4)$ имеет цикловую структуру (два цикла длины 2).
Перестановка $\tau = (1 3)(2 4)$ также имеет цикловую структуру (два цикла длины 2).
Следовательно, $\sigma$ и $\tau$ сопряжены. Действительно, если взять $g = (2 3)$, то $g \sigma g^{-1} = (2 3)(1 2)(3 4)(2 3) = (1 3)(2 4) = \tau$.

#### 16.4. Критерий нормальности подгруппы

**Критерий нормальности (Лемма о сопряжении):**
Подгруппа $H$ группы $G$ нормальна тогда и только тогда, когда она замкнута относительно операции сопряжения. То есть, для любого $g \in G$ и любого $h \in H$, элемент $ghg^{-1}$ также принадлежит $H$.
$H \triangleleft G \iff \forall g \in G, \forall h \in H \implies ghg^{-1} \in H$.

**Доказательство:**
*   **($\implies$)** Пусть $H \triangleleft G$. По определению, $gH = Hg$ для всех $g \in G$.
    Это означает, что для любого $h \in H$, элемент $gh$ (который лежит в $gH$) также должен лежать в $Hg$.
    Следовательно, $gh = h'g$ для некоторого $h' \in H$.
    Умножим справа на $g^{-1}$: $ghg^{-1} = h'$. Так как $h' \in H$, то $ghg^{-1} \in H$.
*   **($\impliedby$)** Пусть для всех $g \in G, h \in H$ выполняется $ghg^{-1} \in H$.
    1.  Покажем, что $gH \subseteq Hg$: Для любого $gh \in gH$, мы знаем, что $ghg^{-1} = h'$ для некоторого $h' \in H$. Тогда $gh = h'g$. Следовательно, $gh \in Hg$.
    2.  Покажем, что $Hg \subseteq gH$: Для любого $hg \in Hg$, мы можем записать $hg = g(g^{-1}hg)$.
        По условию, $g^{-1}hg \in H$ (так как $g^{-1} \in G$ и $h \in H$). Пусть $g^{-1}hg = h'' \in H$.
        Тогда $hg = gh''$. Следовательно, $hg \in gH$.
    Из $gH \subseteq Hg$ и $Hg \subseteq gH$ следует $gH = Hg$. Таким образом, $H$ нормальна.

#### 16.5. Образ и ядро гомоморфизма групп

Пусть $\phi: G \to H$ — гомоморфизм групп.

**Определение образа (Im $\phi$):**
**Образ гомоморфизма** $\phi$ — это множество всех элементов $H$, которые являются образами элементов $G$:
$\text{Im}(\phi) = \{\phi(g) \mid g \in G\}$.
*   $\text{Im}(\phi)$ является подгруппой $H$.

**Определение ядра (Ker $\phi$):**
**Ядро гомоморфизма** $\phi$ — это множество всех элементов $G$, которые отображаются в нейтральный элемент $e_H$ группы $H$:
$\text{Ker}(\phi) = \{g \in G \mid \phi(g) = e_H\}$.
*   $\text{Ker}(\phi)$ является подгруппой $G$.
*   **$\text{Ker}(\phi)$ всегда является нормальной подгруппой $G$.**
    *   **Доказательство:** Пусть $k \in \text{Ker}(\phi)$ и $g \in G$. Мы хотим показать, что $gkg^{-1} \in \text{Ker}(\phi)$.
        $\phi(gkg^{-1}) = \phi(g)\phi(k)\phi(g^{-1}) = \phi(g)e_H\phi(g)^{-1} = \phi(g)\phi(g)^{-1} = e_H$.
        Следовательно, $gkg^{-1} \in \text{Ker}(\phi)$. По критерию нормальности, $\text{Ker}(\phi)$ нормальна.

#### 16.6. Факторгруппы

**Определение:**
Пусть $G$ — группа, и $H$ — её **нормальная подгруппа**. Отношение эквивалентности на $G$ определяется как $a \sim b \iff ab^{-1} \in H$. Классы эквивалентности — это смежные классы $gH = \{gh \mid h \in H\}$.
**Факторгруппа** $G/H$ — это множество всех смежных классов $gH$, на котором определена операция умножения:
$(aH)(bH) = (ab)H$.

**Корректность операции:**
Операция умножения классов определена корректно, если результат не зависит от выбора представителей.
Пусть $aH = a'H$ и $bH = b'H$. Тогда $a' = ah_1$ и $b' = bh_2$ для некоторых $h_1, h_2 \in H$.
$(a'H)(b'H) = (a'b')H = (ah_1bh_2)H$.
Поскольку $H$ нормальна, $h_1b = bh_3$ для некоторого $h_3 \in H$.
Тогда $(ah_1bh_2)H = (abh_3h_2)H$. Поскольку $h_3h_2 \in H$, то $(abh_3h_2)H = (ab)H$.
Таким образом, $(a'H)(b'H) = (ab)H$, и операция корректна.

**Свойства факторгруппы:**
*   Факторгруппа $G/H$ сама является группой.
    *   Нейтральный элемент: $eH = H$.
    *   Обратный элемент: $(gH)^{-1} = g^{-1}H$.
*   Порядок факторгруппы $|G/H|$ равен индексу подгруппы $H$ в $G$: $|G/H| = |G:H|$.

#### 16.7. Основная теорема о гомоморфизме групп

**Формулировка:**
Пусть $\phi: G \to H$ — гомоморфизм групп. Тогда факторгруппа $G/\text{Ker}(\phi)$ изоморфна образу гомоморфизма $\text{Im}(\phi)$:
$G/\text{Ker}(\phi) \cong \text{Im}(\phi)$.

**Доказательство:**
Пусть $K = \text{Ker}(\phi)$. Мы знаем, что $K$ является нормальной подгруппой $G$.
Построим отображение $\bar{\phi}: G/K \to \text{Im}(\phi)$ следующим образом:
Для любого смежного класса $gK \in G/K$, определим $\bar{\phi}(gK) = \phi(g)$.

1.  **Корректность определения $\bar{\phi}$:**
    Пусть $g_1K = g_2K$. Это означает $g_1g_2^{-1} \in K$.
    По определению ядра, $\phi(g_1g_2^{-1}) = e_H$.
    Так как $\phi$ — гомоморфизм, $\phi(g_1)\phi(g_2^{-1}) = e_H \implies \phi(g_1)\phi(g_2)^{-1} = e_H \implies \phi(g_1) = \phi(g_2)$.
    Следовательно, $\bar{\phi}(g_1K) = \bar{\phi}(g_2K)$, и отображение $\bar{\phi}$ определено корректно.

2.  **$\bar{\phi}$ является гомоморфизмом:**
    $\bar{\phi}((g_1K)(g_2K)) = \bar{\phi}((g_1g_2)K) = \phi(g_1g_2) = \phi(g_1)\phi(g_2) = \bar{\phi}(g_1K)\bar{\phi}(g_2K)$.

3.  **$\bar{\phi}$ является сюръективным:**
    Для любого $h \in \text{Im}(\phi)$, существует $g \in G$ такой, что $\phi(g)=h$.
    Тогда $\bar{\phi}(gK) = \phi(g) = h$. Значит, $\bar{\phi}$ сюръективно на $\text{Im}(\phi)$.

4.  **$\bar{\phi}$ является инъективным:**
    Пусть $\bar{\phi}(gK) = e_H$. Это означает $\phi(g) = e_H$.
    По определению ядра, $g \in K$.
    Следовательно, класс $gK$ является нулевым классом $K$ в $G/K$.
    Значит, $\text{Ker}(\bar{\phi}) = \{K\}$, что является нейтральным элементом факторгруппы. Таким образом, $\bar{\phi}$ инъективно.

Поскольку $\bar{\phi}$ является биективным гомоморфизмом, оно является изоморфизмом.

---
[Timestamp: 2026/01/21 05:25:01]