
## 1. Обзор лекции

Лекция посвящена центральной идее многомерного анализа — понятию **дифференцируемости функции нескольких переменных**. Отправной точкой служит естественный вопрос: как выделить «линейную часть» приращения функции $f: \mathbb{R}^n \to \mathbb{R}^m$ при малом изменении аргумента $h \in \mathbb{R}^n$? Оказывается, что приращение $\Delta f$ раскладывается в сумму линейного слагаемого $Ah$ (где $A$ — матрица, а $Ah$ — матричное произведение) и остатка, являющегося $o(\|h\|)$ при $h \to 0$. Строгое доказательство того, что квадратичные по компонентам $h$ слагаемые действительно малы более высокого порядка, чем $\|h\|$, составляет техническое ядро начала лекции. Далее вводятся два важнейших понятия: **производная по направлению** (обобщение производной на произвольное направление в $\mathbb{R}^n$) и **частная производная** (производная по координатному направлению). Затем вводится **матрица Якоби** — матрица оператора дифференцирования, составленная из частных производных. Доказывается теорема: если функция дифференцируема, то существуют все производные по всем направлениям, и они вычисляются умножением матрицы Якоби на направляющий вектор. Лекция завершается формулировкой арифметических свойств дифференцирования: линейности (якобиан суммы есть сумма якобианов) и правила Лейбница для произведения.

---

## 2. Предварительные сведения

| Понятие | Краткое содержание |
|---|---|
| Норма вектора $\|h\|$ | Длина вектора $h = (\Delta x_1, \ldots, \Delta x_n)^T$; $\|h\| = \sqrt{\Delta x_1^2 + \cdots + \Delta x_n^2}$ |
| Символ $o(\|h\|)$ | Функция $\alpha(h)$ есть $o(\|h\|)$, если $\dfrac{\alpha(h)}{\|h\|} \to 0$ при $h \to 0$ |
| Линейный оператор $A: \mathbb{R}^n \to \mathbb{R}^m$ | Отображение, задаваемое матрицей $m \times n$; $A(h) = Ah$ (матричное умножение) |
| Частная производная $\dfrac{\partial f}{\partial x_i}$ | Производная $f$ по переменной $x_i$ при фиксированных остальных |
| Предел $\lim_{h \to 0}$ в $\mathbb{R}^n$ | Предел рассматривается по всем путям приближения, что принципиально сложнее одномерного случая |
| Стандартный базис $e_i \in \mathbb{R}^n$ | Вектор с единицей на $i$-м месте и нулями на остальных |

---

## 3. Основное содержание

---

### 3.1. Определение дифференцируемости функции многих переменных

#### Мотивационный пример

Рассмотрим функцию $f: \mathbb{R}^2 \to \mathbb{R}$, например $f(x, y) = x^2 - xy$. Вычислим приращение в точке $(x_0, y_0)$:

$$
\Delta f = f(x_0 + \Delta x,\; y_0 + \Delta y) - f(x_0, y_0).
$$

Раскроем явно (для примера $f(x,y) = x^2 - xy$):

$$
\Delta f = (x_0 + \Delta x)^2 - (x_0 + \Delta x)(y_0 + \Delta y) - (x_0^2 - x_0 y_0).
$$

$$
= 2x_0 \Delta x - y_0 \Delta x - x_0 \Delta y + (\Delta x)^2 - \Delta x \cdot \Delta y.
$$

Итого:

$$
\Delta f = \underbrace{(2x_0 - y_0)\,\Delta x + (-x_0)\,\Delta y}_{\text{линейная часть}} + \underbrace{(\Delta x)^2 - \Delta x \cdot \Delta y}_{\text{нелинейный остаток}}.
$$

Линейную часть можно записать в матричном виде как строку, умноженную на столбец:

$$
\text{линейная часть} = \begin{pmatrix} 2x_0 - y_0 & -x_0 \end{pmatrix} \begin{pmatrix} \Delta x \\ \Delta y \end{pmatrix} = A \cdot h,
$$

где $h = (\Delta x, \Delta y)^T$ — вектор приращения аргумента.

МОТИВАЦИЯ: Мы хотим понять, «как лучше всего приблизить» функцию линейным отображением вблизи точки. Это многомерный аналог понятия производной из одномерного анализа.

ИНТУИЦИЯ: В одномерном случае $\Delta f \approx f'(x_0)\,\Delta x$ — приращение функции пропорционально приращению аргумента. В многомерном случае «коэффициент пропорциональности» превращается в матрицу $A$, а «приращение аргумента» — в вектор $h$.

---

#### Лемма 3.1 (Нелинейный остаток есть $o(\|h\|)$)

**Утверждение.** Пусть $R(h) = (\Delta x)^2 - \Delta x \cdot \Delta y$ (нелинейный остаток в примере выше). Тогда $R(h) = o(\|h\|)$ при $h \to 0$, то есть

$$
\frac{R(h)}{\|h\|} \to 0 \quad \text{при} \quad \|h\| \to 0.
$$

**Доказательство.** Вычислим

$$
\left|\frac{R(h)}{\|h\|}\right| = \frac{|(\Delta x)^2 - \Delta x \cdot \Delta y|}{\sqrt{(\Delta x)^2 + (\Delta y)^2}}.
$$

По неравенству треугольника:

$$
|(\Delta x)^2 - \Delta x \cdot \Delta y| \leq (\Delta x)^2 + |\Delta x| \cdot |\Delta y|.
$$

Воспользуемся неравенством между средним арифметическим и средним геометрическим:

$$
|\Delta x| \cdot |\Delta y| \leq \frac{(\Delta x)^2 + (\Delta y)^2}{2} = \frac{\|h\|^2}{2}.
$$

Также $(\Delta x)^2 \leq \|h\|^2$. Следовательно:

$$
|(\Delta x)^2 - \Delta x \cdot \Delta y| \leq (\Delta x)^2 + |\Delta x||\Delta y| \leq \|h\|^2 + \frac{\|h\|^2}{2} = \frac{3}{2}\|h\|^2.
$$

Тогда:

$$
\left|\frac{R(h)}{\|h\|}\right| \leq \frac{\frac{3}{2}\|h\|^2}{\|h\|} = \frac{3}{2}\|h\| \to 0 \quad \text{при} \quad \|h\| \to 0. \quad \square
$$

[доказательство добавлено] Общий принцип: любое слагаемое, содержащее произведение двух или более компонент вектора $h$ (т.е. степени не ниже второй по компонентам), является $o(\|h\|)$, поскольку $|\Delta x_i \Delta x_j| \leq \|h\|^2$.

ТИПИЧНЫЕ ОШИБКИ: Нельзя забывать, что предел берётся по всем направлениям одновременно, а не только вдоль осей координат. Именно поэтому оценку нужно делать через норму $\|h\|$, а не через отдельные $|\Delta x|$ или $|\Delta y|$.

---

#### Определение 3.1 (Дифференцируемость функции)

Пусть $f: U \subset \mathbb{R}^n \to \mathbb{R}^m$, где $U$ — открытое множество, и $x_0 \in U$. Функция $f$ называется **дифференцируемой** в точке $x_0$, если существует линейный оператор $A: \mathbb{R}^n \to \mathbb{R}^m$ (задаваемый матрицей размера $m \times n$) такой, что

$$
\boxed{f(x_0 + h) - f(x_0) = Ah + o(\|h\|) \quad \text{при} \quad h \to 0,}
$$

то есть

$$
\frac{\|f(x_0 + h) - f(x_0) - Ah\|}{\|h\|} \to 0 \quad \text{при} \quad \|h\| \to 0.
$$

Оператор $A$ называется **дифференциалом** (или производной) функции $f$ в точке $x_0$ и обозначается $df(x_0)$ или $A = f'(x_0)$.

**Эквивалентная запись через приращение:**

$$
\Delta f = Ah + o(\|h\|).
$$

**Размерность матрицы $A$:** оператор $A$ отображает $\mathbb{R}^n$ в $\mathbb{R}^m$, поэтому матрица $A$ имеет размер $m \times n$ — $m$ строк (по числу компонент результата) и $n$ столбцов (по числу переменных).

МОТИВАЦИЯ: Это определение обобщает понятие производной на многомерный случай. В одномерном случае $n = m = 1$, и условие принимает вид $f(x_0 + h) - f(x_0) = f'(x_0) h + o(h)$, что совпадает с классическим определением.

ИНТУИЦИЯ: Функция дифференцируема, если её «локально можно приблизить аффинным (линейным + сдвиг) отображением» с точностью, лучшей, чем линейная по $\|h\|$.

СВЯЗИ: Это определение является многомерным аналогом дифференцируемости по Фреше. Из дифференцируемости следует непрерывность (так как $Ah \to 0$ при $h \to 0$, а значит и $\Delta f \to 0$).

---

#### Следствие 3.1 (Дифференцируемость влечёт непрерывность)

Если $f$ дифференцируема в точке $x_0$, то $f$ непрерывна в $x_0$.

[доказательство добавлено] **Доказательство.** По определению дифференцируемости $\Delta f = Ah + o(\|h\|)$. При $h \to 0$ имеем $Ah \to 0$ (линейный оператор непрерывен) и $o(\|h\|) \to 0$. Следовательно, $\Delta f \to 0$, то есть $\lim_{h\to 0} f(x_0 + h) = f(x_0)$. $\square$

---

### 3.2. Производная по направлению

МОТИВАЦИЯ: В одномерном анализе производная описывает скорость изменения функции вдоль единственного возможного направления (числовой оси). В многомерном пространстве $\mathbb{R}^n$ при $n \geq 2$ направлений бесконечно много — по одному для каждого единичного вектора $e \in \mathbb{R}^n$. Возникает естественный вопрос: как изменяется $f$ при движении из точки $x_0$ в направлении $e$?

ИНТУИЦИЯ: Представьте рельеф местности (функция высоты на плоскости). Производная по направлению — это крутизна склона, если идти в заданном направлении. Она зависит от направления: подъём может быть крутым к северу и пологим к востоку.

---

#### Определение 3.2 (Производная по направлению)

Пусть $f: U \subset \mathbb{R}^n \to \mathbb{R}$, $x_0 \in U$, и $e \in \mathbb{R}^n$ — единичный вектор ($\|e\| = 1$). **Производной функции $f$ по направлению $e$ в точке $x_0$** называется предел

$$
\boxed{\frac{\partial f}{\partial e}(x_0) \;=\; f'_e(x_0) \;=\; \lim_{t \to +0} \frac{f(x_0 + te) - f(x_0)}{t},}
$$

если этот предел существует.

**Замечания к определению:**

1. Предел берётся только при $t \to +0$ (то есть с одной стороны — справа). Это принципиально отличается от частных производных, где предел двусторонний.

2. Нормировка $\|e\| = 1$ необходима, чтобы знаменатель $t$ действительно измерял расстояние от $x_0$ до $x_0 + te$. Если $e$ не единичный, то $\|te\| = |t|\|e\| \neq |t|$, и в знаменателе надо писать $t\|e\|$ или нормировать $e$.

3. Если $\|e\| = 1$, то $\dfrac{\partial f}{\partial (-e)}(x_0) = -\dfrac{\partial f}{\partial e}(x_0)$ при условии существования обоих пределов [восстановлено: это следует непосредственно из замены $t \mapsto -t$ в определении, что при $e \to -e$ даёт предел с противоположным знаком].[/восстановлено]

ТИПИЧНЫЕ ОШИБКИ: Путать производную по направлению (односторонний предел) с частной производной (двусторонний предел). Если функция не дифференцируема, эти понятия могут давать разные результаты.

---

#### Определение 3.3 (Частная производная)

Пусть $f: U \subset \mathbb{R}^n \to \mathbb{R}$, $x_0 \in U$, и $e_k = (0, \ldots, 0, 1, 0, \ldots, 0)^T$ — $k$-й базисный вектор стандартного базиса $\mathbb{R}^n$. **Частной производной** функции $f$ по переменной $x_k$ в точке $x_0$ называется

$$
\boxed{\frac{\partial f}{\partial x_k}(x_0) \;=\; \lim_{t \to 0} \frac{f(x_0 + t e_k) - f(x_0)}{t},}
$$

если этот предел существует.

**Ключевое отличие от производной по направлению:** здесь $t \to 0$ (двусторонний предел), а не $t \to +0$.

**Практический смысл:** при вычислении $\dfrac{\partial f}{\partial x_k}$ рассматриваем $f$ как функцию одной переменной $x_k$, считая все остальные переменные константами, и берём обычную производную.

СВЯЗИ: Частная производная $\dfrac{\partial f}{\partial x_k}$ соответствует производной по направлению $e_k$ только тогда, когда предел по $t$ существует с обеих сторон и совпадает. Если функция дифференцируема, это автоматически выполняется.

---

#### Сравнение: производная по направлению и частная производная

| Свойство | $\dfrac{\partial f}{\partial e}$ | $\dfrac{\partial f}{\partial x_k}$ |
|---|---|---|
| Предел по $t$ | $t \to +0$ (односторонний) | $t \to 0$ (двусторонний) |
| Направление | Произвольный ед. вектор $e$ | Базисный вектор $e_k$ |
| Существование | Слабее: может быть только с одной стороны | Сильнее: требует с обеих сторон |
| Связь | $\dfrac{\partial f}{\partial e_k} \neq \dfrac{\partial f}{\partial x_k}$ в общем случае | — |

[восстановлено] Пример, когда они различаются: рассмотрим $f: \mathbb{R}^2 \to \mathbb{R}$, $f(x, y) = |x|$. Тогда $\dfrac{\partial f}{\partial e_1}(0,0) = \lim_{t\to+0} \dfrac{|t|}{t} = 1$, но $\dfrac{\partial f}{\partial x}(0,0)$ не существует (двусторонний предел не существует).[/восстановлено]

---

### 3.3. Теорема о существовании производных по направлению для дифференцируемой функции

#### Теорема 3.1

Пусть $f: U \subset \mathbb{R}^n \to \mathbb{R}$ дифференцируема в точке $x_0 \in U$, то есть $\Delta f = Ah + o(\|h\|)$. Тогда:

1. Для любого единичного вектора $e \in \mathbb{R}^n$ существует производная по направлению $\dfrac{\partial f}{\partial e}(x_0)$, и

$$
\boxed{\frac{\partial f}{\partial e}(x_0) = Ae.}
$$

2. Для каждого $k = 1, \ldots, n$ существует частная производная $\dfrac{\partial f}{\partial x_k}(x_0)$, и

$$
\frac{\partial f}{\partial x_k}(x_0) = A e_k.
$$

**Доказательство.**

[Идея: подставить $h = te$ в определение дифференцируемости и перейти к пределу при $t \to +0$.]

**Шаг 1.** По определению дифференцируемости при $h = te$ (где $t > 0$, $\|e\| = 1$):

$$
f(x_0 + te) - f(x_0) = A(te) + o(\|te\|) = t \cdot Ae + o(t).
$$

**Шаг 2.** Разделим на $t > 0$:

$$
\frac{f(x_0 + te) - f(x_0)}{t} = Ae + \frac{o(t)}{t}.
$$

**Шаг 3.** По определению символа $o(t)$: $\dfrac{o(t)}{t} \to 0$ при $t \to +0$. Следовательно,

$$
\lim_{t \to +0} \frac{f(x_0 + te) - f(x_0)}{t} = Ae.
$$

**Шаг 4.** Это в точности определение $\dfrac{\partial f}{\partial e}(x_0)$. При $e = e_k$ получаем $Ae_k = \dfrac{\partial f}{\partial x_k}(x_0)$.

$\square$

ЗАМЕЧАНИЕ: Обратное неверно! Существование всех частных производных (и даже всех производных по направлению) не гарантирует дифференцируемость функции. Это принципиальное отличие многомерного анализа от одномерного.

[восстановлено] Классический контрпример: $f(x, y) = \dfrac{xy}{x^2 + y^2}$ при $(x,y) \neq (0,0)$ и $f(0,0) = 0$. Оба частных производных в нуле равны нулю, но функция не непрерывна в нуле (а значит и не дифференцируема).[/восстановлено]

---

### 3.4. Матрица Якоби

МОТИВАЦИЯ: Теорема 3.1 говорит, что оператор $A$ (матрица) полностью определяется частными производными. Возникает вопрос: как именно? Ответ даёт матрица Якоби — она является матрицей оператора $A$ в стандартных базисах.

ИНТУИЦИЯ: Якобиан обобщает понятие градиента на векторнозначные функции. Для скалярной функции ($m = 1$) якобиан — это строка, называемая **градиентом** $\nabla f$.

---

#### Определение 3.4 (Матрица Якоби)

Пусть $f = (f_1, \ldots, f_m): U \subset \mathbb{R}^n \to \mathbb{R}^m$, где каждая $f_i: U \to \mathbb{R}$. **Матрицей Якоби** (или **якобианом**) функции $f$ в точке $x_0$ называется матрица размера $m \times n$:

$$
\boxed{J_f(x_0) = \frac{\partial f}{\partial x}(x_0) =
\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1}(x_0) & \dfrac{\partial f_1}{\partial x_2}(x_0) & \cdots & \dfrac{\partial f_1}{\partial x_n}(x_0) \\[8pt]
\dfrac{\partial f_2}{\partial x_1}(x_0) & \dfrac{\partial f_2}{\partial x_2}(x_0) & \cdots & \dfrac{\partial f_2}{\partial x_n}(x_0) \\[4pt]
\vdots & \vdots & \ddots & \vdots \\[4pt]
\dfrac{\partial f_m}{\partial x_1}(x_0) & \dfrac{\partial f_m}{\partial x_2}(x_0) & \cdots & \dfrac{\partial f_m}{\partial x_n}(x_0)
\end{pmatrix}.}
$$

**Структура:** строка $i$ содержит все частные производные функции $f_i$ по всем переменным; столбец $j$ содержит частные производные всех функций по переменной $x_j$.

**Связь с оператором $A$:** если $f$ дифференцируема в точке $x_0$, то матрица Якоби $J_f(x_0)$ является матрицей оператора $A = df(x_0)$ в стандартных базисах $\mathbb{R}^n$ и $\mathbb{R}^m$.

---

#### Пример 3.1 (Вычисление матрицы Якоби)

Рассмотрим $f: \mathbb{R}^2 \to \mathbb{R}$, $f(x, y) = x^2 - xy$. Вычислим якобиан:

$$
\frac{\partial f}{\partial x} = 2x - y, \qquad \frac{\partial f}{\partial y} = -x.
$$

Матрица Якоби (строка $1 \times 2$):

$$
J_f(x, y) = \begin{pmatrix} 2x - y & -x \end{pmatrix}.
$$

В конкретной точке, например $(x_0, y_0) = (1, 0)$:

$$
J_f(1, 0) = \begin{pmatrix} 2 & -1 \end{pmatrix}.
$$

**Проверка через определение дифференцируемости:**

$$
\Delta f = (2x_0 - y_0)\Delta x + (-x_0)\Delta y + \underbrace{(\Delta x)^2 - \Delta x \Delta y}_{o(\|h\|)} = J_f(x_0, y_0) \cdot h + o(\|h\|). \quad \checkmark
$$

---

#### Пример 3.2 (Якобиан для $f: \mathbb{R}^2 \to \mathbb{R}^3$)

[восстановлено] Пусть $f(x, y) = (x^2 + y, \; xy, \; e^x \sin y)^T$. Тогда:

$$
J_f(x, y) = \begin{pmatrix}
\dfrac{\partial f_1}{\partial x} & \dfrac{\partial f_1}{\partial y} \\[8pt]
\dfrac{\partial f_2}{\partial x} & \dfrac{\partial f_2}{\partial y} \\[8pt]
\dfrac{\partial f_3}{\partial x} & \dfrac{\partial f_3}{\partial y}
\end{pmatrix}
= \begin{pmatrix}
2x & 1 \\
y & x \\
e^x \sin y & e^x \cos y
\end{pmatrix}.
$$

Это матрица $3 \times 2$: 3 строки (3 компоненты результата) и 2 столбца (2 переменные).[/восстановлено]

---

#### Замечание 3.1 (Координатное представление)

Функцию $f: \mathbb{R}^n \to \mathbb{R}^m$ можно записать через координатные функции:

$$
f(x) = \begin{pmatrix} f_1(x) \\ f_2(x) \\ \vdots \\ f_m(x) \end{pmatrix},
$$

где каждая $f_i: \mathbb{R}^n \to \mathbb{R}$. Дифференцируемость $f$ в точке $x_0$ равносильна дифференцируемости каждой $f_i$ в той же точке [восстановлено: это следует из того, что норма в $\mathbb{R}^m$ эквивалентна максимуму норм координат].[/восстановлено]

---

### 3.5. Формула для производной по направлению через якобиан

#### Следствие 3.2

Если $f: U \subset \mathbb{R}^n \to \mathbb{R}$ дифференцируема в $x_0$ и $e \in \mathbb{R}^n$ — единичный вектор, то

$$
\boxed{\frac{\partial f}{\partial e}(x_0) = J_f(x_0) \cdot e = \sum_{k=1}^n \frac{\partial f}{\partial x_k}(x_0)\, e_k = \nabla f(x_0) \cdot e,}
$$

где $\nabla f(x_0) = \left(\dfrac{\partial f}{\partial x_1}(x_0), \ldots, \dfrac{\partial f}{\partial x_n}(x_0)\right)$ — **градиент** функции $f$.

ИНТУИЦИЯ: Производная по произвольному направлению — это проекция градиента на это направление (скалярное произведение). Максимальное значение производной по направлению достигается при $e = \dfrac{\nabla f}{\|\nabla f\|}$, то есть в направлении самого градиента.

---

### 3.6. Арифметические свойства дифференцирования

#### Теорема 3.2 (Линейность якобиана)

Пусть $f, g: U \subset \mathbb{R}^n \to \mathbb{R}^m$ — дифференцируемые в точке $x_0$ функции, и $\alpha, \beta \in \mathbb{R}$. Тогда функция $\alpha f + \beta g$ также дифференцируема в $x_0$, и

$$
\boxed{J_{\alpha f + \beta g}(x_0) = \alpha\, J_f(x_0) + \beta\, J_g(x_0).}
$$

**Доказательство.**

[Идея: использовать линейность частных производных.]

По определению, каждый элемент матрицы Якоби:

$$
\left[J_{\alpha f + \beta g}\right]_{ij} = \frac{\partial(\alpha f_i + \beta g_i)}{\partial x_j}(x_0) = \alpha \frac{\partial f_i}{\partial x_j}(x_0) + \beta \frac{\partial g_i}{\partial x_j}(x_0) = \alpha [J_f]_{ij} + \beta [J_g]_{ij}.
$$

Это верно для каждой пары индексов $(i, j)$, следовательно матрицы совпадают. $\square$

---

#### Теорема 3.3 (Правило Лейбница, произведение скалярной и векторной функций)

Пусть $\lambda: U \subset \mathbb{R}^n \to \mathbb{R}$ и $f: U \subset \mathbb{R}^n \to \mathbb{R}^m$ — дифференцируемы в точке $x_0$. Тогда функция $g = \lambda \cdot f: U \to \mathbb{R}^m$ (поточечное произведение скаляра на вектор) также дифференцируема, и

$$
\boxed{J_{\lambda f}(x_0) = \lambda(x_0)\, J_f(x_0) + f(x_0)^T \cdot J_\lambda(x_0).}
$$

Здесь $f(x_0)^T$ — строка из значений $f$, а $J_\lambda(x_0)$ — строка-якобиан скалярной функции.

**Замечание о некоммутативности:** поскольку в общем случае фигурируют произведения матриц, порядок множителей менять нельзя. Для скалярно-векторного произведения формула записана выше; для произведения двух скалярных функций она совпадает с классическим правилом Лейбница.

[восстановлено] **Частный случай (скалярный):** если $m = 1$, то обе функции скалярные:

$$
\nabla(\lambda f)(x_0) = \lambda(x_0) \nabla f(x_0) + f(x_0) \nabla \lambda(x_0).
$$

Это прямое многомерное обобщение одномерного правила $(\lambda f)' = \lambda' f + \lambda f'$.[/восстановлено]

---

## 4. Справочный лист (ключевые формулы)

| Объект | Формула |
|---|---|
| Определение дифференцируемости | $f(x_0 + h) - f(x_0) = Ah + o(\|h\|)$ |
| Производная по направлению | $\dfrac{\partial f}{\partial e}(x_0) = \lim_{t \to +0} \dfrac{f(x_0 + te) - f(x_0)}{t}$ |
| Частная производная | $\dfrac{\partial f}{\partial x_k}(x_0) = \lim_{t \to 0} \dfrac{f(x_0 + te_k) - f(x_0)}{t}$ |
| Матрица Якоби | $[J_f]_{ij} = \dfrac{\partial f_i}{\partial x_j}$, размер $m \times n$ |
| Производная по направлению через якобиан | $\dfrac{\partial f}{\partial e}(x_0) = J_f(x_0) \cdot e$ |
| Линейность | $J_{\alpha f + \beta g} = \alpha J_f + \beta J_g$ |
| Правило Лейбница (скаляр $\times$ вектор) | $J_{\lambda f} = \lambda J_f + f^T J_\lambda$ |
| Оценка остатка | $|(\Delta x_i)(\Delta x_j)| \leq \|h\|^2 = o(\|h\|)$ |

---

## 5. Концептуальное резюме

Дифференцируемость функции многих переменных — это требование существования наилучшего линейного приближения к приращению функции, записываемого через матрицу (оператор) $A$ так, что невязка $\Delta f - Ah$ оказывается бесконечно малой более высокого порядка, чем $\|h\|$. Матрица $A$ однозначно определяется частными производными и называется матрицей Якоби. Производные по произвольным направлениям вычисляются умножением якобиана на единичный вектор направления — тем самым единственный объект (матрица) кодирует поведение функции вдоль всех направлений сразу. Ключевые арифметические свойства (линейность, правило Лейбница) переносятся из одномерного анализа, но в матричной записи требуют аккуратного учёта порядка множителей и размерности объектов.

---

## 6. Связи с более широкой математикой

**Дифференцируемость по Фреше и Гато.** Определение дифференцируемости в данной лекции совпадает с определением производной по Фреше. Производная по направлению (при предельном переходе $t \to 0$, двусторонний) совпадает с производной по Гато. Дифференцируемость по Фреше влечёт дифференцируемость по Гато, но не наоборот.

**Теорема о достаточном условии дифференцируемости.** Если все частные производные $\dfrac{\partial f}{\partial x_k}$ существуют и непрерывны в окрестности точки $x_0$, то $f$ дифференцируема в $x_0$ (теорема о достаточном условии дифференцируемости). Это даёт практический критерий проверки дифференцируемости.

**Правило дифференцирования сложной функции (цепное правило).** Если $f$ дифференцируема в $x_0$ и $g$ дифференцируема в $f(x_0)$, то $g \circ f$ дифференцируема в $x_0$, и $J_{g \circ f}(x_0) = J_g(f(x_0)) \cdot J_f(x_0)$ — произведение матриц Якоби.

**Теорема об обратной функции.** Если $J_f(x_0)$ — невырожденная матрица (квадратная и $\det J_f(x_0) \neq 0$), то существует локальная обратная функция, тоже дифференцируемая.

**Интегрирование и формула Грина.** Матрица Якоби появляется в формуле замены переменных в кратных интегралах: $|\det J_f|$ — это «коэффициент растяжения объёма» при замене переменных.
