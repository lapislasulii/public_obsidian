Данная лекция является логическим продолжением изучения основ теории вероятностей. Она связывает ранее изученные конструктивные определения (дискретную и геометрическую модели) с фундаментальными аксиомами и вводит ключевые инструменты современной статистики и машинного обучения: **условную вероятность**, **закон полной вероятности** и **теорему Байеса**. Лекция решает проблему оценки вероятности событий в условиях притока новой информации и демонстрирует, почему интуитивные догадки часто проигрывают строгому математическому расчету.

---

## Основное содержание

### 1. Повторение и свойства вероятности

Прежде чем переходить к новому, лектор напоминает свойства вероятности, вытекающие из дискретной и геометрической моделей:

- **Неотрицательность:** $P(A) \ge 0$.
- **Нормировка:** $P(\Omega) = 1$.
- **Аддитивность для несовместных событий:** Если события не могут произойти одновременно ($A \cap B = \emptyset$), то вероятность их суммы равна сумме их вероятностей: $P(A + B) = P(A) + P(B)$.
- **Вероятность противоположного события:** $P(A) = 1 - P(\bar{A})$.
- **Монотонность:** Если $A \subseteq B$, то $P(A) \le P(B)$. Отсюда следует, что любая вероятность не превосходит 1.
- **Формула включений-исключений:** Для двух событий: $P(A+B) = P(A) + P(B) - P(AB)$. Для $n$ событий используется более общая формула, где поочередно прибавляются вероятности отдельных событий, вычитаются вероятности двойных пересечений, прибавляются тройные и т.д..

### 2. Условная вероятность

**Мотивация и интуиция:** Как меняется вероятность события $A$, если мы точно знаем, что событие $B$ уже произошло? В классической модели это означает, что наше пространство исходов сужается с $\Omega$ до $B$.

**Формальное определение:** Пусть $P(B) > 0$. **Условной вероятностью** $A$ при условии $B$ называется: $$P(A|B) = \frac{P(AB)}{P(B)}$$.

**Принцип произведения вероятностей:** Из определения следует: $P(AB) = P(B) \cdot P(A|B)$. Для $n$ событий формула обобщается по индукции: $$P(A_1 \dots A_n) = P(A_1) \cdot P(A_2|A_1) \cdot P(A_3|A_1 A_2) \dots P(A_n|A_1 \dots A_{n-1})$$. _Интуиция:_ Чтобы $n$ событий произошли одновременно, нужно, чтобы произошло первое, затем второе (с учетом первого), третье (с учетом двух первых) и т.д..

### 3. Независимость событий

**Мотивация и интуиция:** События независимы, если наступление одного не меняет шансов другого. В терминах формул: условная вероятность равна безусловной.

**Формальные определения:**

1. **Попарная независимость:** $A$ и $B$ независимы, если $P(AB) = P(A) \cdot P(B)$.
2. **Независимость в совокупности:** Набор $A_1 \dots A_n$ независим, если для любого поднабора индексов вероятность их пересечения равна произведению их вероятностей.

**Важный контрпример (Тетраэдр Бернштейна):** Рассмотрим правильный тетраэдр, грани которого покрашены: 1-я — Красный (К), 2-я — Зеленый (З), 3-я — Синий (С), 4-я — все три цвета сразу.

- Вероятность каждого цвета: $P(K) = P(З) = P(С) = 2/4 = 1/2$.
- Вероятность пары (например, К и З одновременно): $1/4$. Так как $1/4 = 1/2 \cdot 1/2$, события **попарно независимы**.
- Вероятность трех цветов сразу: $1/4$. Но $P(K)P(З)P(С) = 1/8$.
- **Вывод:** Попарная независимость НЕ гарантирует независимость в совокупности.

### 4. Формула полной вероятности и теорема Байеса

**Формула полной вероятности (ФПВ):** Если событие $A$ может произойти только с одной из несовместных гипотез $B_k$, то: $$P(A) = \sum P(A|B_k) \cdot P(B_k)$$. _Идея доказательства:_ $A$ представляется как объединение непересекающихся кусков $(A \cap B_k)$, и к каждому куску применяется принцип произведения.

**Теорема Байеса:** Позволяет "переставить" условие: $$P(B_k|A) = \frac{P(A|B_k) \cdot P(B_k)}{P(A)}$$ Где $P(A)$ в знаменателе расписывается по ФПВ.

**Терминология:**

- $P(B_k)$ — **априорная** вероятность (до опыта).
- $P(B_k|A)$ — **апостериорная** вероятность (после получения данных $A$).
- $P(A|B_k)$ — **правдоподобие** (likelihood).
- $P(A)$ — **свидетельство** (evidence).

---

## Примеры и парадоксы

### 1. Парадокс Монти Холла (ящики с призом)

**Условие:** 3 ящика, в одном приз. Игрок выбирает ящик №1. Ведущий (зная, где приз) открывает пустой ящик (например, №3) и предлагает сменить выбор. **Разбор:**

- Априорная вероятность приза в любом ящике — $1/3$.
- Если приз в №2 или №3 (вероятность $2/3$), ведущий своим действием "подсвечивает" оставшийся ящик, так как у него нет выбора, что открывать.
- Расчет через принцип произведения показывает: если сменить выбор, вероятность победы — **2/3**, если остаться — **1/3**. Интуиция "50 на 50" ложна, так как исходы стали неравновероятными.

### 2. Медицинская диагностика

**Условие:** Болезнь встречается у 1 из 1000. Тест дает верный результат в 99% случаев для больных и ошибается в 1% случаев для здоровых. Анализ положителен. Какова вероятность, что человек здоров?. **Решение:**

- $P(H) = 0.999$ (здоров), $P(I) = 0.001$ (болен).
- $P(+|H) = 0.01$ (ложноположительный), $P(+|I) = 0.99$ (верноположительный).
- По теореме Байеса $P(H|+) \approx 0.91$.
- **Вывод:** Даже с положительным тестом человек, скорее всего, здоров, так как сама болезнь очень редкая ("базовая ставка"). При повторном независимом положительном тесте вероятность здоровья падает (примерно до 0.1).

### 3. Парадокс трех заключенных

**Условие:** Из заключенных А, B, C помилуют одного. А просит охранника назвать имя того (кроме него), кто точно будет казнен. Охранник говорит: "B будет казнен". А считает, что его шансы выжить стали 1/2. **Разбор:** Аналогично Монти Холлу. Вероятность того, что помилуют А, остается **1/3**. Информация охранника не меняет априорную вероятность для А, так как охранник в любом случае назвал бы кого-то из пары B/C.

---

## Важные детали

- **Замечание преподавателя:** Не путайте независимость и несовместность! Это разные понятия, хотя звучат похоже.
- **Технический нюанс:** В ФПВ требование, чтобы сумма гипотез $B_k$ составляла всё пространство $\Omega$, формально излишне, если $A$ уже содержится в их сумме, но обычно это подразумевается для удобства.
- **Вопрос студента:** "А если ящиков 10?" **Ответ:** Принцип тот же, нужно аккуратно расписывать конфигурации, интуиция может подвести.
- **Анонс:** В следующем семестре аксиоматика вероятности будет введена более строго.

## Ключевые инсайты

1. **Математика против интуиции:** В задачах с условием (Монти Холл, тесты) нельзя просто делить 1 на количество оставшихся вариантов. Нужно учитывать, как информация "собиралась".
2. **Важность априорных данных:** Редкое событие (болезнь) остается маловероятным даже при наличии неплохого подтверждающего теста.
3. **Независимость — это жестко:** Чтобы набор событий был независим, "независимость пар" — лишь необходимое, но недостаточное условие.

### Доказательства 

## 1. Свойства вероятности (Аксиоматический вывод)

Преподаватель отметил, что свойства вероятности можно вывести из трех базовых условий (аксиом): $P(A) \ge 0$, $P(\Omega) = 1$ и аддитивности для несовместных событий.

### А) Вероятность противоположного события: $P(\bar{A}) = 1 - P(A)$

**Доказательство:**

1. По определению противоположного события, $A$ и $\bar{A}$ несовместны ($A \cap \bar{A} = \emptyset$) и в сумме дают всё пространство исходов ($A \cup \bar{A} = \Omega$).
2. Используя аксиому аддитивности: $P(A \cup \bar{A}) = P(A) + P(\bar{A})$.
3. Так как $P(\Omega) = 1$, получаем: $1 = P(A) + P(\bar{A})$.
4. Следовательно, $P(A) = 1 - P(\bar{A})$.

### Б) Монотонность: Если $A \subseteq B$, то $P(A) \le P(B)$

**Доказательство:**

1. Событие $B$ можно представить как объединение двух несовместных событий: самого $A$ и той части $B$, которая не входит в $A$ (обозначается $B \setminus A$). То есть $B = A \cup (B \setminus A)$.
2. По аксиоме аддитивности: $P(B) = P(A) + P(B \setminus A)$.
3. По аксиоме неотрицательности $P(B \setminus A) \ge 0$.
4. Следовательно, $P(B) \ge P(A)$.

### В) Формула включений-исключений для двух событий: $P(A \cup B) = P(A) + P(B) - P(AB)$

**Доказательство (через несовместные объединения):**

1. Представим объединение $A \cup B$ как сумму трех несовместных частей: $A \setminus B$ (только А), $B \setminus A$ (только B) и $AB$ (их пересечение). $P(A \cup B) = P(A \setminus B) + P(B \setminus A) + P(AB)$.
2. Заметим, что $P(A) = P(A \setminus B) + P(AB) \implies P(A \setminus B) = P(A) - P(AB)$.
3. Аналогично $P(B) = P(B \setminus A) + P(AB) \implies P(B \setminus A) = P(B) - P(AB)$.
4. Подставим эти выражения в первую формулу: $P(A \cup B) = (P(A) - P(AB)) + (P(B) - P(AB)) + P(AB) = P(A) + P(B) - P(AB)$.

---

## 2. Условная вероятность и принцип произведения

### Вывод формулы условной вероятности $P(A|B)$

**Доказательство (через классическую модель):**

1. Пусть в пространстве $\Omega$ всего $N$ равновероятных исходов. Событию $B$ благоприятствуют $M$ исходов, а пересечению $AB$ — $K$ исходов.
2. Если мы знаем, что $B$ произошло, то наше пространство исходов сужается до $M$. Теперь нас интересуют только те исходы из $A$, которые попали в этот новый набор (то есть исходы пересечения $AB$).
3. Тогда условная вероятность: $P(A|B) = \frac{K}{M}$.
4. Разделим числитель и знаменатель на общее число исходов $N$: $P(A|B) = \frac{K/N}{M/N} = \frac{P(AB)}{P(B)}$.

### Принцип произведения (обобщенная формула)

**Доказательство (методом индукции):**

1. **База ($n=2$):** Из определения $P(A_2|A_1) = \frac{P(A_1 A_2)}{P(A_1)} \implies P(A_1 A_2) = P(A_1)P(A_2|A_1)$.
2. **Шаг:** Пусть формула верна для $n-1$ событий. Для $n$ событий запишем: $P(A_1 \dots A_n) = P((A_1 \dots A_{n-1}) \cap A_n)$.
3. По определению условной вероятности: $P(A_1 \dots A_n) = P(A_1 \dots A_{n-1}) \cdot P(A_n | A_1 \dots A_{n-1})$.
4. Применяя предположение индукции к $P(A_1 \dots A_{n-1})$, получаем полную цепочку произведений.

---

## 3. Формула полной вероятности (ФПВ)

**Теорема:** Если $A \subseteq \bigcup_{k} B_k$ и $B_k$ несовместны, то $P(A) = \sum P(A|B_k)P(B_k)$.

**Доказательство:**

1. Представим событие $A$ как пересечение самого себя со всем пространством гипотез: $A = A \cap (\bigcup_{k} B_k)$.
2. Используя закон дистрибутивности: $A = \bigcup_{k} (A \cap B_k)$.
3. Так как $B_k$ несовместны, то и события $(A \cap B_k)$ тоже несовместны (это части $A$, лежащие в разных $B_k$).
4. По аксиоме аддитивности: $P(A) = \sum_{k} P(A \cap B_k)$.
5. По определению условной вероятности $P(A \cap B_k) = P(A|B_k)P(B_k)$.
6. Итого: $P(A) = \sum_{k} P(A|B_k)P(B_k)$.

---

## 4. Теорема Байеса

**Теорема:** $P(B_k|A) = \frac{P(A|B_k)P(B_k)}{P(A)}$.

**Доказательство:**

1. По определению условной вероятности: $P(B_k|A) = \frac{P(B_k \cap A)}{P(A)}$.
2. Применим принцип произведения к числителю (с другой стороны): $P(B_k \cap A) = P(A|B_k)P(B_k)$.
3. Подставим это в формулу: $P(B_k|A) = \frac{P(A|B_k)P(B_k)}{P(A)}$.
4. Чтобы получить полную форму, знаменатель $P(A)$ раскрывается по формуле полной вероятности (см. выше).

---

## 5. Независимость и условная вероятность

**Утверждение:** Если $A$ и $B$ независимы, то $P(A|B) = P(A)$.

**Доказательство:**

1. По определению независимости: $P(AB) = P(A)P(B)$.
2. По определению условной вероятности: $P(A|B) = \frac{P(AB)}{P(B)}$.
3. Подставляем (1) в (2): $P(A|B) = \frac{P(A)P(B)}{P(B)} = P(A)$.
4. _Интуиция:_ Информация о том, что $B$ произошло, никак не меняет оценку вероятности $A$.

---

## 6. Разбор парадокса Монти Холла (математический расчет)

В лекции расчет проводился через перебор конфигураций, что по сути является применением ФПВ.

**Доказательство преимущества смены выбора:**

1. Пусть $C_i$ — приз в $i$-м ящике ($P(C_i) = 1/3$). Пусть игрок выбрал ящик 1 ($S_1$).
2. Ведущий открывает ящик 3 ($O_3$). Нам нужно сравнить $P(C_1 | O_3)$ и $P(C_2 | O_3)$.
3. **Для ящика 1 (не менять):** $P(O_3 | C_1) = 1/2$ (ведущий может открыть 2 или 3 равновероятно). $P(O_3 | C_2) = 1$ (у ведущего нет выбора, он не может открыть 1 и ящик с призом 2). $P(O_3 | C_3) = 0$ (ведущий не открывает ящик с призом).
4. По формуле полной вероятности $P(O_3) = \frac{1}{2} \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} = \frac{1}{6} + \frac{2}{6} = \frac{1}{2}$.
5. По теореме Байеса для ящика 1: $P(C_1 | O_3) = \frac{P(O_3 | C_1)P(C_1)}{P(O_3)} = \frac{1/2 \cdot 1/3}{1/2} = \mathbf{1/3}$.
6. По теореме Байеса для ящика 2: $P(C_2 | O_3) = \frac{P(O_3 | C_2)P(C_2)}{P(O_3)} = \frac{1 \cdot 1/3}{1/2} = \mathbf{2/3}$. **Вывод:** Смена выбора удваивает шансы.