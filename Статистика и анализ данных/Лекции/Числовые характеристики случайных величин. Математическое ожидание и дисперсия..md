
## 1. Обзор лекции

На данной лекции был завершён обзор ряда дискретных распределений (геометрическое, отрицательное биномиальное, гипергеометрическое), после чего совершён переход к числовым характеристикам случайных величин. Лектор мотивировал необходимость числовых характеристик тем, что функция распределения содержит исчерпывающую информацию, однако непосредственное сравнение двух распределений через их PMF затруднено. Числа дают краткое, интерпретируемое описание. Были введены два центральных понятия: **математическое ожидание** (среднее взвешенное значение) и **дисперсия** (мера разброса вокруг среднего). На мотивирующем примере с предприятием, где один сотрудник из ста получает 10 000 единиц, а остальные 99 — по 10, было наглядно показано, что математическое ожидание само по себе недостаточно описывает ситуацию. В качестве аппроксимационного результата анонсирована локальная теорема Муавра–Лапласа: при больших $n$ вероятности биномиального распределения приближаются гауссовой функцией, что мотивирует введение нормального распределения и переход к непрерывным случайным величинам.

---

## 2. Предварительные сведения

- **Дискретная случайная величина:** функция $X\colon \Omega \to \mathbb{R}$, принимающая не более чем счётное множество значений.
- **Функция вероятности (PMF):** $p(x_k) = P(X = x_k)$, удовлетворяет $\sum_k p(x_k) = 1$.
- **Биномиальное распределение** $B(n,p)$: $P(X=k) = \binom{n}{k} p^k q^{n-k}$, $q = 1-p$.
- **Независимые испытания Бернулли:** на каждом испытании успех с вероятностью $p$, неудача с вероятностью $q = 1-p$.
- **Формула полной вероятности:** $P(B) = \sum_i P(B \mid A_i) \cdot P(A_i)$.
- **Формула Стирлинга:** $n! \approx \sqrt{2\pi n}\cdot (n/e)^n$ при $n \to \infty$.

---

## 3. Основное содержание

### 3.1. Геометрическое распределение

Проводятся независимые испытания Бернулли с вероятностью успеха $p$ и вероятностью неудачи $q = 1-p$. Случайная величина $X$ — **число неудач до первого успеха**.

**Определение 3.1 (Геометрическое распределение).**
Случайная величина $X$ имеет *геометрическое распределение* с параметром $p \in (0,1)$, если

$$P(X = k) = q^k \cdot p, \quad k = 0, 1, 2, \ldots$$

Обозначение: $X \sim \mathrm{Geom}(p)$.

**МОТИВАЦИЯ.** Модель описывает время ожидания первого успеха в серии независимых одинаковых испытаний: сколько раз «не получилось» прежде, чем «получилось».

**ИНТУИЦИЯ.** Чем больше $k$, тем меньше вероятность $q^k p$, поскольку нужно $k$ подряд неудач. Геометрическая убыль — отсюда и название.

> **Замечание.** В литературе встречается альтернативное определение: $X$ — *номер* испытания, на котором впервые наступает успех. Тогда $P(X=k) = q^{k-1} p$, $k = 1, 2, \ldots$ Это тот же закон со сдвигом на 1. При чтении источников необходимо уточнять, какое из двух определений используется.

---

### 3.2. Отрицательное биномиальное распределение

Обобщение геометрического: вместо первого успеха ожидается $r$-й успех. Случайная величина $X$ — **число неудач до наступления $r$-го успеха**.

**Определение 3.2 (Отрицательное биномиальное распределение).**
$X \sim \mathrm{NegBin}(r, p)$, если

$$P(X = k) = \binom{k+r-1}{k} \cdot q^k \cdot p^r, \quad k = 0, 1, 2, \ldots$$

**Вывод формулы.** Событие $\{X = k\}$ означает:
- в первых $k+r-1$ испытаниях ровно $k$ неудач и $r-1$ успехов;
- на $(k+r)$-м испытании — успех (фиксировано).

По независимости:

$$P(X = k) = \binom{k+r-1}{r-1} \cdot q^k \cdot p^{r-1} \cdot p = \binom{k+r-1}{k} \cdot q^k \cdot p^r.$$

**МОТИВАЦИЯ.** Геометрическое описывает ожидание одного успеха; отрицательное биномиальное — $r$ успехов подряд. Применяется, например, в задачах контроля качества, страховании, биологии.

> **Замечание (связь с геометрическим).** При $r=1$: $P(X=k) = \binom{k}{0} q^k p = q^k p$. Это в точности геометрическое распределение. Иными словами, $\mathrm{Geom}(p) = \mathrm{NegBin}(1, p)$.

---

### 3.3. Гипергеометрическое распределение

**Модель.** В корзине $M$ деталей, из которых $N$ — хорошие ($N \leq M$). Без возвращения извлекают $k$ деталей. Случайная величина $X$ — **число хороших деталей среди извлечённых**.

**Определение 3.3 (Гипергеометрическое распределение).**
$X \sim \mathrm{HGeom}(M, N, k)$, если

$$P(X = g) = \frac{\binom{N}{g}\binom{M-N}{k-g}}{\binom{M}{k}},$$

где $\max(0,\, k-(M-N)) \leq g \leq \min(k, N)$.

**Пояснение ограничений.**
- $g \leq N$: нельзя взять больше хороших, чем их есть.
- $k - g \leq M - N$: нельзя взять больше плохих, чем их есть, т.е. $g \geq k - (M-N)$.
- Объединяя: $g \geq \max(0,\, k-(M-N))$.

**МОТИВАЦИЯ.** Биномиальное распределение предполагает независимость испытаний (извлечение с возвращением). Гипергеометрическое — более реалистичная модель выборки из конечной совокупности без возвращения.

> **Замечание (тождество Вандермонда).** Из условия нормировки $\sum_g P(X=g) = 1$ следует комбинаторное тождество
> $$\sum_g \binom{N}{g}\binom{M-N}{k-g} = \binom{M}{k}.$$
> Это — вероятностное доказательство формулы свёртки биномиальных коэффициентов.

**Предельный переход к биномиальному.** Пусть $M \to \infty$, $N \to \infty$ так, что $N/M \to p \in (0,1)$. Тогда

$$P(X = g) \;\longrightarrow\; \binom{k}{g} p^g (1-p)^{k-g}.$$

*Интуиция:* при очень большой корзине извлечение без возвращения практически не отличается от извлечения с возвращением, так как после изъятия одного элемента доля хороших меняется незначительно.

---

### 3.4. Мотивация числовых характеристик

Функция вероятности (PMF) содержит полную информацию о распределении, однако непосредственное сравнение двух распределений через PMF затруднено: требуется анализировать (потенциально) бесконечную последовательность чисел. Числовые характеристики сжимают информацию до одного-двух чисел, допускающих наглядную интерпретацию и удобное сравнение.

---

### 3.5. Математическое ожидание

**Определение 3.4 (Математическое ожидание дискретной СВ).**
Пусть $X$ — дискретная случайная величина с функцией вероятности $p_k = P(X = x_k)$. *Математическим ожиданием* (средним значением) $X$ называется

$$\mathbb{E}[X] = \sum_k x_k \cdot p_k,$$

если данный ряд сходится абсолютно: $\sum_k |x_k| \cdot p_k < \infty$. В русскоязычной литературе также используется обозначение $\mathbf{M}[X]$ или $MX$.

**МОТИВАЦИЯ.** Математическое ожидание — обобщение среднего арифметического на случай неравновероятных исходов. Оно указывает, вокруг какого значения «концентрируется» случайная величина.

**ИНТУИЦИЯ.** Представьте, что эксперимент повторяется $N$ раз. Исход $x_k$ наблюдается приблизительно $N \cdot p_k$ раз, и выборочное среднее равно

$$\frac{1}{N}\sum_k x_k \cdot (N p_k) = \sum_k x_k p_k = \mathbb{E}[X].$$

Это согласуется с законом больших чисел (будет изучен позднее).

#### 3.5.1. Частный случай: равномерное дискретное распределение

Пусть $X$ принимает значения $x_1, \ldots, x_n$ с равными вероятностями $p_k = 1/n$. Тогда

$$\mathbb{E}[X] = \sum_{k=1}^n x_k \cdot \frac{1}{n} = \frac{x_1 + x_2 + \cdots + x_n}{n}.$$

Математическое ожидание совпадает с обычным средним арифметическим.

#### 3.5.2. Пример 3.1 — «Ловушка среднего»

**Условие.** Предприятие: 1 сотрудник из 100 получает 10 000 единиц ресурса, остальные 99 — по 10 единиц.

**Решение.** Пусть $X$ — заработок случайно выбранного сотрудника:

$$P(X = 10000) = 0.01, \quad P(X = 10) = 0.99.$$

Математическое ожидание:

$$\mathbb{E}[X] = 10000 \cdot 0.01 + 10 \cdot 0.99 = 100 + 9.9 = 109.9.$$

**Вывод.** Среднее равно $109.9$ — «прилично». Однако в $99\%$ случаев человек получает лишь $10$ единиц. Математическое ожидание в данном примере вводит в заблуждение относительно «типичного» дохода.

> **Замечание (лектор).** Когда в отчётах фигурирует только среднее значение, это может быть результатом незнания либо сознательным введением в заблуждение. Грамотная статистика требует указывать не только среднее, но и меру разброса.

---

### 3.6. Дисперсия и среднеквадратическое отклонение

**Определение 3.5 (Дисперсия дискретной СВ).**
*Дисперсией* случайной величины $X$ называется

$$\mathrm{Var}(X) = D[X] = \sum_k (x_k - \mathbb{E}[X])^2 \cdot p_k.$$

В русскоязычной литературе также обозначается $D[X]$ или $DX$.

**Определение 3.6 (Среднеквадратическое отклонение).**
*Среднеквадратическим (стандартным) отклонением* называется

$$\sigma(X) = \sqrt{\mathrm{Var}(X)}.$$

Оно измеряется в тех же единицах, что и $X$.

**МОТИВАЦИЯ.** Дисперсия измеряет, насколько сильно значения $X$ «разбросаны» вокруг математического ожидания. Малая дисперсия — $X$ почти всегда близко к $\mathbb{E}[X]$; большая — возможны значительные отклонения от среднего.

**ИНТУИЦИЯ.** В выражении $(x_k - \mathbb{E}[X])^2 \cdot p_k$ квадрат гарантирует, что положительные и отрицательные отклонения не компенсируют друг друга; вес $p_k$ учитывает, насколько часто встречается данное отклонение.

> **Замечание (альтернативная формула).** Для вычислений удобна формула
> $$\mathrm{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2,$$
> где $\mathbb{E}[X^2] = \sum_k x_k^2 \cdot p_k$. Она будет доказана позднее с помощью свойств математического ожидания.

#### 3.6.1. Пример 3.2 — Дисперсия в «ловушке среднего»

**Условие.** Данные из Примера 3.1: $\mathbb{E}[X] = 109.9$.

**Решение.**

$$\mathrm{Var}(X) = (10000 - 109.9)^2 \cdot 0.01 + (10 - 109.9)^2 \cdot 0.99$$

$$= (9890.1)^2 \cdot 0.01 + (-99.9)^2 \cdot 0.99$$

$$= 97{,}812{,}080.01 \cdot 0.01 + 9{,}980.01 \cdot 0.99$$

$$= 978{,}120.8 + 9{,}880.2 \approx 988{,}001.$$

$$\sigma(X) = \sqrt{988{,}001} \approx 994.$$

**Вывод.** Типичное отклонение от среднего составляет $\approx 994$ единицы, хотя само среднее равно $109.9$. Это и отражает огромный разброс в заработках, который не был заметен из одного лишь $\mathbb{E}[X]$.

---

### 3.7. Локальная теорема Муавра–Лапласа (анонс)

**Теорема 3.1 (Локальная теорема Муавра–Лапласа).**
Пусть $X \sim B(n, p)$, $q = 1-p$, $npq > 0$. Обозначим стандартную нормальную плотность:

$$\varphi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}.$$

Тогда при $n \to \infty$ равномерно по $k$:

$$P(X = k) \approx \frac{1}{\sqrt{npq}} \cdot \varphi\!\left(\frac{k - np}{\sqrt{npq}}\right).$$

**Идея вывода (неформально).** С помощью формулы Стирлинга $n! \approx \sqrt{2\pi n}(n/e)^n$ и подстановки $k = np + t\sqrt{npq}$ при $n \to \infty$ биномиальный коэффициент $\binom{n}{k} p^k q^{n-k}$ приводится к виду, пропорциональному $e^{-t^2/2}/\sqrt{npq}$.

**МОТИВАЦИЯ.** Теорема даёт практический инструмент: при большом $n$ можно считать вероятности биномиального распределения не напрямую (формула громоздка), а через значения функции $\varphi$.

**Зрительный образ.** Гистограмма биномиального распределения (прямоугольники шириной 1 и высотой $P(X=k)$) при больших $n$ всё точнее аппроксимируется колоколообразной кривой Гаусса. Суммирование площадей прямоугольников заменяется вычислением площади под кривой — это мотивирует понятие непрерывной случайной величины с плотностью.

> **Замечание об области применимости.** Локальная теорема Муавра–Лапласа работает хорошо, когда $p$ не слишком близко к $0$ или $1$. При малых $p$ (редкие события) удобнее приближение Пуассона. Оба приближения будут изучены подробнее на следующих лекциях.

---

### 3.8. Переход к непрерывным случайным величинам (анонс)

Предельный переход в теореме Муавра–Лапласа переводит нас из дискретного мира в непрерывный. Нормированная случайная величина

$$Y = \frac{X - np}{\sqrt{npq}}$$

при $n \to \infty$ «стремится» к случайной величине, принимающей любое вещественное значение. Вероятность попасть в интервал $(a, b]$ задаётся уже не суммой вероятностей, а интегралом:

$$P(a < Y \leq b) = \int_a^b \varphi(x)\, dx.$$

Функция $\varphi(x)$ называется **плотностью вероятности** нормального распределения $\mathcal{N}(0,1)$. Это вводит понятие непрерывной случайной величины и её плотности распределения, которые будут формально определены в следующей теме.

---

## 4. Справочный лист

| Объект | Формула |
|---|---|
| Geom$(p)$ | $P(X=k) = q^k p,\quad k=0,1,2,\ldots$ |
| NegBin$(r,p)$ | $P(X=k) = \binom{k+r-1}{k} q^k p^r,\quad k=0,1,\ldots$ |
| HGeom$(M,N,k)$ | $P(X=g) = \dfrac{\binom{N}{g}\binom{M-N}{k-g}}{\binom{M}{k}}$ |
| Математическое ожидание | $\mathbb{E}[X] = \sum_k x_k p_k$ |
| Дисперсия | $\mathrm{Var}(X) = \sum_k (x_k - \mathbb{E}[X])^2 p_k$ |
| Альт. формула дисперсии | $\mathrm{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ |
| Среднеквадр. отклонение | $\sigma(X) = \sqrt{\mathrm{Var}(X)}$ |
| Локальная т. Муавра–Лапласа | $P(X=k) \approx \dfrac{1}{\sqrt{npq}}\,\varphi\!\left(\dfrac{k-np}{\sqrt{npq}}\right)$ |
| Стандартная норм. плотность | $\varphi(x) = \dfrac{1}{\sqrt{2\pi}} e^{-x^2/2}$ |

---

## 5. Концептуальное резюме

На данной лекции был совершён переход от языка распределений к языку числовых характеристик. Математическое ожидание $\mathbb{E}[X]$ обобщает понятие среднего арифметического на вероятностный случай и отвечает на вопрос «вокруг какого значения сосредоточена случайная величина». Дисперсия $\mathrm{Var}(X)$ дополняет эту картину, измеряя «ширину» распределения. Вместе они дают минимально необходимый набор для описания и сравнения распределений. Ключевой пример с неравномерным доходом наглядно показал, что среднее без дисперсии может вводить в заблуждение. Предельный переход от биномиального распределения к нормальному, анонсированный в локальной теореме Муавра–Лапласа, мотивирует следующую большую тему — непрерывные случайные величины и нормальное распределение.

---

## 6. Связи с более широкой математикой

- **Закон больших чисел:** $\mathbb{E}[X]$ — предел выборочного среднего при $n \to \infty$ (теоремы Чебышёва и Колмогорова).
- **Центральная предельная теорема:** сумма независимых одинаково распределённых СВ при нормировке стремится к нормальному распределению — прямое развитие теоремы Муавра–Лапласа.
- **Моменты:** $\mathbb{E}[X]$ — первый момент, $\mathbb{E}[X^2]$ — второй момент; $\mathrm{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ — центральный второй момент.
- **Производящая функция моментов (MGF):** $M_X(t) = \mathbb{E}[e^{tX}]$ — мощный инструмент для вычисления всех моментов через дифференцирование.
- **Комбинаторика:** вероятностное доказательство тождества Вандермонда через нормировку гипергеометрического распределения.
- **Математический анализ:** формула Стирлинга связывает дискретные факториалы с непрерывными функциями и обеспечивает асимптотику в теореме Муавра–Лапласа.
- **Теория меры:** строгое определение $\mathbb{E}[X]$ как интеграла Лебега — основа для непрерывных СВ и смешанных распределений.

---
